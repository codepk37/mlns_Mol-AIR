# config/qed/molair.yaml
QED-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate (Polyak averaging)
    alpha: 0.2 # Initial entropy coefficient (can be learned)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha (if learned)
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Whether to automatically tune alpha
    target_entropy_ratio: 0.98 # Target entropy ratio (relative to max entropy)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 100000 # Size of the replay buffer
    batch_size: 256 # Batch size for sampling from buffer
    learning_starts: 1000 # Number of steps before starting training
    gradient_steps: 1 # Number of gradient steps per environment step
    # --- Common Agent Params (May have different meaning/usage in SAC) ---
    n_steps: 1 # Number of environment steps per agent interaction cycle (often 1 for SAC)

  Env: # Kept from original qed/molair.yaml
    qed_coef: 1.0

  Train: # Kept from original qed/molair.yaml (adjust lr if needed)
    num_envs: 64
    seed: 0
    total_time_steps: 120000
    summary_freq: 1000
    agent_save_freq: 5000
    num_inference_envs: 32
    n_inference_episodes: 100
    # lr: 1.0e-4
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Kept from original qed/molair.yaml
    crwd_coef: 0.01
