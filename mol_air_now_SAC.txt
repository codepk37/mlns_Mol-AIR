ðŸ“ DIRECTORY STRUCTURE (tree):
.
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ drd2+qed+sa
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair_end2end.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ppo.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ pretrained.yaml
â”‚Â Â  â”œâ”€â”€ gsk3b
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ ppo.yaml
â”‚Â Â  â”œâ”€â”€ gsk3b+jnk3
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ ppo.yaml
â”‚Â Â  â”œâ”€â”€ jnk3
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ ppo.yaml
â”‚Â Â  â”œâ”€â”€ plogp
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ ppo.yaml
â”‚Â Â  â”œâ”€â”€ qed
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lir.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molair.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ ppo.yaml
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â””â”€â”€ similarity
â”‚Â Â      â”œâ”€â”€ hir.yaml
â”‚Â Â      â”œâ”€â”€ lir.yaml
â”‚Â Â      â”œâ”€â”€ molair.yaml
â”‚Â Â      â””â”€â”€ ppo.yaml
â”œâ”€â”€ copy_all_txt.py
â”œâ”€â”€ data
â”‚Â Â  â””â”€â”€ drd2+qed+sa
â”‚Â Â      â”œâ”€â”€ pretrained.pt
â”‚Â Â      â”œâ”€â”€ smiles.txt
â”‚Â Â      â””â”€â”€ vocab.json
â”œâ”€â”€ drl
â”‚Â Â  â”œâ”€â”€ agent
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ agent.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ net.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pretrained.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ agent.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ net.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pretrained.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_ppo.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_ppo_rnd.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_sac.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_sac_inference.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ trajectory.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_ppo.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_ppo_rnd.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_sac_inference.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ recurrent_sac.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ trajectory.py
â”‚Â Â  â”œâ”€â”€ exp.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ net.py
â”‚Â Â  â”œâ”€â”€ policy_dist.py
â”‚Â Â  â”œâ”€â”€ policy.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ exp.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ net.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ policy.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ policy_dist.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ rl_loss.cpython-37.pyc
â”‚Â Â  â”œâ”€â”€ rl_loss.py
â”‚Â Â  â””â”€â”€ util
â”‚Â Â      â”œâ”€â”€ clock.py
â”‚Â Â      â”œâ”€â”€ func.py
â”‚Â Â      â”œâ”€â”€ incr_calc.py
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ __pycache__
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ clock.cpython-37.pyc
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ func.cpython-37.pyc
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ incr_calc.cpython-37.pyc
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ __init__.cpython-37.pyc
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ train_step.cpython-37.pyc
â”‚Â Â      â”‚Â Â  â””â”€â”€ truncated_sequence_generator.cpython-37.pyc
â”‚Â Â      â”œâ”€â”€ scheduler.py
â”‚Â Â      â”œâ”€â”€ train_step.py
â”‚Â Â      â””â”€â”€ truncated_sequence_generator.py
â”œâ”€â”€ envs
â”‚Â Â  â”œâ”€â”€ chem_env.py
â”‚Â Â  â”œâ”€â”€ count_int_reward.py
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”‚Â Â  â””â”€â”€ fpscores.pkl.gz
â”‚Â Â  â”œâ”€â”€ env.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chem_env.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ count_int_reward.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ env.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ score_func.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ selfies_tokenizer.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ selfies_util.cpython-37.pyc
â”‚Â Â  â”œâ”€â”€ score_func.py
â”‚Â Â  â”œâ”€â”€ selfies_tokenizer.py
â”‚Â Â  â””â”€â”€ selfies_util.py
â”œâ”€â”€ export.py
â”œâ”€â”€ img
â”‚Â Â  â”œâ”€â”€ fig4.png
â”‚Â Â  â”œâ”€â”€ fig7.png
â”‚Â Â  â””â”€â”€ toc.png
â”œâ”€â”€ metric.py
â”œâ”€â”€ mol_air_now_SAC.txt
â”œâ”€â”€ oracle
â”‚Â Â  â”œâ”€â”€ drd2_current.pkl
â”‚Â Â  â”œâ”€â”€ fpscores.pkl
â”‚Â Â  â”œâ”€â”€ gsk3b_current.pkl
â”‚Â Â  â””â”€â”€ jnk3_current.pkl
â”œâ”€â”€ __pycache__
â”‚Â Â  â”œâ”€â”€ metric.cpython-37.pyc
â”‚Â Â  â””â”€â”€ util.cpython-37.pyc
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements_gen.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results
â”‚Â Â  â””â”€â”€ PLogP-MolAIR
â”‚Â Â      â”œâ”€â”€ agent_ckpt
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent_10000.pt
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ agent_20000.pt
â”‚Â Â      â”‚Â Â  â””â”€â”€ agent_24495.pt
â”‚Â Â      â”œâ”€â”€ agent.pt
â”‚Â Â      â”œâ”€â”€ best_agent.pt
â”‚Â Â      â”œâ”€â”€ config.yaml
â”‚Â Â      â”œâ”€â”€ episode_metric.csv
â”‚Â Â      â”œâ”€â”€ events.out.tfevents.1744816662.pop-os.32678.0
â”‚Â Â      â”œâ”€â”€ log.txt
â”‚Â Â      â””â”€â”€ plots
â”‚Â Â          â”œâ”€â”€ Environment
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Avg Reward (Trace Env).png
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Diversity.png
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Novelty.png
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Score.png
â”‚Â Â          â”‚Â Â  â””â”€â”€ Uniqueness.png
â”‚Â Â          â”œâ”€â”€ Inference
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Diversity.png
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Novelty.png
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ Score.png
â”‚Â Â          â”‚Â Â  â””â”€â”€ Uniqueness.png
â”‚Â Â          â””â”€â”€ Training
â”‚Â Â              â”œâ”€â”€ Actor Loss.png
â”‚Â Â              â”œâ”€â”€ Alpha Loss.png
â”‚Â Â              â”œâ”€â”€ Alpha.png
â”‚Â Â              â””â”€â”€ Critic Loss.png
â”œâ”€â”€ run.py
â”œâ”€â”€ test.py
â”œâ”€â”€ train
â”‚Â Â  â”œâ”€â”€ factory.py
â”‚Â Â  â”œâ”€â”€ inference.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ net.py
â”‚Â Â  â”œâ”€â”€ pretrain.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ factory.cpython-312.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ factory.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ inference.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-312.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ net.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pretrain.cpython-37.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ train.cpython-37.pyc
â”‚Â Â  â””â”€â”€ train.py
â””â”€â”€ util.py

31 directories, 147 files

================================================================================

copy_all_txt.py
;;;
import os
import subprocess

# Base directory
base_dir = "/media/pavan/STORAGE/linux_storage/Gemini/Mol-AIR"
output_file = os.path.join(base_dir, "mol_air_now_SAC.txt")

# YAML files to add (with comments)
yaml_entries = [
    ("config/plogp/molair.yaml", "pLogP"),
    ("config/qed/molair.yaml", "QED"),
    ("config/similarity/molair.yaml", "Similarity"),
    ("config/gsk3b/molair.yaml", "GSK3B"),
    ("config/jnk3/molair.yaml", "JNK3"),
    ("config/gsk3b+jnk3/molair.yaml", "GSK3B+JNK3"),
]

with open(output_file, 'w', encoding='utf-8') as out_f:
    # Step 1: Run `tree` and capture output
    try:
        tree_output = subprocess.check_output(["tree", "."], cwd=base_dir, text=True)
        out_f.write("ðŸ“ DIRECTORY STRUCTURE (tree):\n")
        out_f.write(tree_output)
        out_f.write("\n" + "="*80 + "\n\n")
    except Exception as e:
        out_f.write(f"# Failed to run 'tree': {e}\n\n")

    # Step 2: Add all .py files
    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".py"):
                abs_path = os.path.join(root, file)
                rel_path = os.path.relpath(abs_path, base_dir)

                out_f.write(f"{rel_path}\n")
                out_f.write(";;;\n")

                try:
                    with open(abs_path, 'r', encoding='utf-8') as in_f:
                        content = in_f.read()
                    out_f.write(content)
                except Exception as e:
                    out_f.write(f"# Error reading {rel_path}: {e}\n")

                out_f.write("\n;;;\n\n")

    # Step 3: Add selected YAML files with simple path + comment
    for rel_yaml_path, comment in yaml_entries:
        abs_yaml_path = os.path.join(base_dir, rel_yaml_path)

        out_f.write(f"{rel_yaml_path} # {comment}\n")
        out_f.write(";;;\n")

        try:
            with open(abs_yaml_path, 'r', encoding='utf-8') as yaml_f:
                yaml_content = yaml_f.read()
            out_f.write(yaml_content)
        except Exception as e:
            out_f.write(f"# Error reading {rel_yaml_path}: {e}\n")

        out_f.write("\n;;;\n\n")

print("âœ… All content written to mol_air_now.txt")


;;;

export.py
;;;
import argparse
import numpy as np
import pandas as pd
from matplotlib.patches import Patch
import matplotlib.pyplot as plt

from util import try_create_dir, moving_average, load_yaml

def make_plot_figure(x_vals_list, y_vals_list, labels, x_label, y_label, ylim=None):
    fig, ax = plt.subplots()
    
    for x_vals, y_vals, label in zip(x_vals_list, y_vals_list, labels):
        ax.plot(x_vals, y_vals, label=label)
    
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.legend()
    if ylim is not None:
        ax.set_ylim(top=ylim)
    
    return fig, ax

def df_groupby_episode(df_list, func):
    df_result_list = [df.groupby("episode") for df in df_list]
    df_result_list = [func(df) for df in df_result_list]
    df_result_list = [df[~df.isna()] for df in df_result_list]
    return df_result_list

def series_to_xy(series_list):
    x_vals_list = [series.index for series in series_list]
    y_vals_list = [series.values for series in series_list]
    return x_vals_list, y_vals_list

def avg_score_figure(episode_metric_df_list, title, labels, n):
    avg_score_list = df_groupby_episode(
        episode_metric_df_list,
        lambda x: x["score"].mean()
    )
    x_vals_list, y_vals_list = series_to_xy(avg_score_list)
    avg_score_fig, _ = make_plot_figure(
        x_vals_list,
        y_vals_list,
        labels,
        "Episode",
        f"Average {title}"
    )
    moving_avg_score_fig, _ = make_plot_figure(
        x_vals_list,
        [moving_average(y_vals, n=n) for y_vals in y_vals_list],
        labels,
        "Episode",
        f"Moving Average {title} (n: {n})"
    )
    return avg_score_fig, moving_avg_score_fig
    
def anoot_val(x, y, ax=None):
    text= f"y={y:.3f}"
    if not ax:
        ax=plt.gca()
    bbox_props = dict(boxstyle="square,pad=0.3", fc="w", ec="k", lw=0.72)
    kw = dict(xycoords='data',textcoords="axes fraction",
              bbox=bbox_props, ha="right", va="bottom")
    ax.annotate(text, xy=(x, y), xytext=(0.94,0.96), **kw)


def best_score_figure(episode_metrics, title, labels):
    fig, ax = plt.subplots()
    
    xmax = -1
    ymax = -100000
    
    for df, label in zip(episode_metrics, labels):
        best_scores = df.groupby("episode")["score"].max()
        best_scores = best_scores[~best_scores.isna()]
        curruent_best_score_episode = best_scores.index[0]
        current_best_score = best_scores[curruent_best_score_episode]
        for episode in best_scores.index:
            if best_scores[episode] > current_best_score:
                current_best_score = best_scores[episode]
                curruent_best_score_episode = episode
            best_scores[episode] = current_best_score
        ax.plot(best_scores, label=label)
        
        if current_best_score > ymax:
            xmax = curruent_best_score_episode
            ymax = current_best_score
        
    ax.set_xlabel("Episode")
    ax.set_ylabel(f"Best {title}")
    ax.legend()
    anoot_val(xmax, ymax, ax)
    
    return fig

def avg_int_reward_figure(episode_metric_df_list, title, labels, int_reward_type, n):
    if int_reward_type.lower() == "count":
        column_field = "avg_count_int_reward"
    elif int_reward_type.lower() == "rnd":
        column_field = "avg_rnd_int_reward"
    else:
        raise ValueError(f"Invalid int_reward_type: {int_reward_type}")
    
    filtered_df = []
    filtered_labels = []
    for df, label in zip(episode_metric_df_list, labels):
        if column_field in df.columns:
            filtered_df.append(df)
            filtered_labels.append(label)
            
    if len(filtered_df) == 0:
        return None
            
    avg_int_rewards_list = df_groupby_episode(
        filtered_df,
        lambda x: x[column_field].mean()
    )
    x_vals_list, y_vals_list = series_to_xy(avg_int_rewards_list)
    fig, _ = make_plot_figure(
        x_vals_list,
        y_vals_list,
        filtered_labels,
        "Episode",
        f"{title} Average {int_reward_type} Intrinsic Reward"
    )
    ma_fig, _ = make_plot_figure(
        x_vals_list,
        [moving_average(y_vals, n=n) for y_vals in y_vals_list],
        filtered_labels,
        "Episode",
        f"{title} Moving Average {int_reward_type} Intrinsic Reward (n: {n})"
    )
    return fig, ma_fig

def total_avg_int_reward_figure(
    episode_metric_df_list,
    title,
    labels,
    n,
    count_coefs,
    rnd_coefs
):  
    filtered_count_df = {}
    for df, label in zip(episode_metric_df_list, labels):
        if "avg_count_int_reward" in df.columns:
            filtered_count_df[label] = df
            
    if len(filtered_count_df) != len(count_coefs):
        raise ValueError("The number of Count coefficients must be equal to the number of Count int rewards.")
    
    filtered_rnd_df = {}
    for df, label in zip(episode_metric_df_list, labels):
        if "avg_rnd_int_reward" in df.columns:
            filtered_rnd_df[label] = df
            
    if len(filtered_rnd_df) != len(rnd_coefs):
        raise ValueError("The number of RND coefficients must be equal to the number of RND int rewards.")
            
    if len(filtered_count_df) == 0 and len(filtered_rnd_df) == 0:
        return None
    
    avg_count_int_reward_list = df_groupby_episode(
        filtered_count_df.values(),
        lambda x: x["avg_count_int_reward"].mean()
    )
    avg_rnd_int_rewards_list = df_groupby_episode(
        filtered_rnd_df.values(),
        lambda x: x["avg_rnd_int_reward"].mean()
    )
    avg_count_int_reward_list = [count_coefs[i] * avg_count_int_reward_list[i] for i in range(len(avg_count_int_reward_list))]
    avg_rnd_int_rewards_list = [rnd_coefs[i] * avg_rnd_int_rewards_list[i] for i in range(len(avg_rnd_int_rewards_list))]
    
    total_avg_int_rewards_dict = {}
    for i, label in enumerate(filtered_count_df.keys()):
        total_avg_int_rewards_dict[label] = avg_count_int_reward_list[i]
    for i, label in enumerate(filtered_rnd_df.keys()):
        if label in total_avg_int_rewards_dict:
            total_avg_int_rewards_dict[label] += avg_rnd_int_rewards_list[i]
        else:
            total_avg_int_rewards_dict[label] = avg_rnd_int_rewards_list[i]
    
    avg_count_int_reward_means = [avg_count_int_reward.mean() for avg_count_int_reward in avg_count_int_reward_list]
    avg_rnd_int_reward_means = [avg_rnd_int_reward.mean() for avg_rnd_int_reward in avg_rnd_int_rewards_list]
    # make a barplot
    count_color = "blue"
    rnd_color = "green"
    barfig, barax = plt.subplots()
    bottoms = {}
    for i, label in enumerate(filtered_count_df.keys()):
        barax.bar(label, avg_count_int_reward_means[i], color=count_color)
        bottoms[label] = avg_count_int_reward_means[i]
    for i, label in enumerate(filtered_rnd_df.keys()):
        bottom = bottoms[label] if label in bottoms else 0
        barax.bar(label, avg_rnd_int_reward_means[i], bottom=bottom, color=rnd_color)
    legend_elements = [
        Patch(facecolor=count_color, label="Count"),
        Patch(facecolor=rnd_color, label="RND")
    ]
    barax.set_title(f"{title} Intrinsic Reward Mean Bar by Type")
    barax.set_xlabel("Labels")
    barax.set_ylabel("Mean")
    # barax.set_yscale("log")
    barax.legend(handles=legend_elements)
    
    
    x_vals_list, y_vals_list = series_to_xy(total_avg_int_rewards_dict.values())
    total_y_vals = np.concatenate(y_vals_list)
    total_mean = total_y_vals.mean()
    total_std = total_y_vals.std()
    ylim = total_mean + 3 * total_std
    fig, _ = make_plot_figure(
        x_vals_list,
        y_vals_list,
        total_avg_int_rewards_dict.keys(),
        "Episode",
        f"{title} Average Intrinsic Reward",
        ylim
    )
    ma_fig, _ = make_plot_figure(
        x_vals_list,
        [moving_average(y_vals, n=n) for y_vals in y_vals_list],
        total_avg_int_rewards_dict.keys(),
        "Episode",
        f"{title} Moving Average Intrinsic Reward (n: {n})",
        ylim
    )
    return fig, ma_fig, barfig

def best_comparison_table(episode_metrics, labels) -> pd.DataFrame:    
    comparison_df = pd.DataFrame(
        columns=["Label", "Episode", "Score", "Selfies"]
    )
    
    for i, df in enumerate(episode_metrics):
        best_row = df.iloc[df["score"].argmax()]
        comparison_df.loc[i] = [labels[i], best_row["episode"], best_row["score"], best_row["selfies"]]
    
    return comparison_df

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('TITLE', type=str, help='Title for the data')
    parser.add_argument('EXPERIMENT_RESULT_DIR', nargs='+', type=str, help='Directories of experiment results')
    parser.add_argument('-e', '--episode', type=int, help='Episode number (default = max)', default=None)
    parser.add_argument('-m', '--moving_average', type=int, help='Moving average n (default = max_episode / 100)', default=None)
    
    args = parser.parse_args()

    title = args.TITLE
    experiment_result_dirs = args.EXPERIMENT_RESULT_DIR
    episode = args.episode
    moving_average_n = args.moving_average
    
    labels = []
    count_coefs = []
    rnd_coefs = []
    for d in experiment_result_dirs:
        config_dict = load_yaml(f"{d}/config.yaml")
        label = tuple(config_dict.keys())[0]
        labels.append(label)
        config_dict = config_dict[label]
        if "CountIntReward" in config_dict and "crwd_coef" in config_dict["CountIntReward"]:
            count_coefs.append(config_dict["CountIntReward"]["crwd_coef"])
        if "nonepi_adv_coef" in config_dict["Agent"]:
            rnd_coefs.append(config_dict["Agent"]["nonepi_adv_coef"])
    
    episode_metrics = []
    for d in experiment_result_dirs:
        episode_metrics.append(pd.read_csv(f"{d}/episode_metric.csv"))
    
    if episode is None:
        episode = min([df["episode"].max() for df in episode_metrics])
    
    if moving_average_n is None:
        moving_average_n = episode // 100
    
    episode_metrics = [df[df["episode"] <= episode] for df in episode_metrics]
    episode_metrics = [df.sort_values(by=["episode", "env_id"]) for df in episode_metrics]

    avg_score_fig, moving_avg_score_fig = avg_score_figure(episode_metrics, title, labels, moving_average_n)
    best_score_fig = best_score_figure(episode_metrics, title, labels)
    avg_count_int_reward_figs = avg_int_reward_figure(episode_metrics, title, labels, "Count", moving_average_n)
    avg_rnd_int_reward_figs = avg_int_reward_figure(episode_metrics, title, labels, "RND", moving_average_n)
    avg_int_reward_figs = total_avg_int_reward_figure(
        episode_metrics,
        title,
        labels,
        moving_average_n,
        count_coefs,
        rnd_coefs
    )
    best_comparison_df = best_comparison_table(episode_metrics, labels)
    
    export_dir = f"exports/{title}"
    
    try_create_dir(export_dir)
    
    avg_score_fig.savefig(f"{export_dir}/avg_score.png")
    moving_avg_score_fig.savefig(f"{export_dir}/moving_avg_score.png")
    best_score_fig.savefig(f"{export_dir}/best_score.png")
    if avg_count_int_reward_figs is not None:
        avg_count_int_reward_figs[0].savefig(f"{export_dir}/avg_count_int_reward.png")
        avg_count_int_reward_figs[1].savefig(f"{export_dir}/moving_avg_count_int_reward.png")
    if avg_rnd_int_reward_figs is not None:
        avg_rnd_int_reward_figs[0].savefig(f"{export_dir}/avg_rnd_int_reward.png")
        avg_rnd_int_reward_figs[1].savefig(f"{export_dir}/moving_avg_rnd_int_reward.png")
    if avg_int_reward_figs is not None:
        avg_int_reward_figs[0].savefig(f"{export_dir}/avg_int_reward.png")
        avg_int_reward_figs[1].savefig(f"{export_dir}/moving_avg_int_reward.png")
        avg_int_reward_figs[2].savefig(f"{export_dir}/avg_int_reward_bar.png")
    best_comparison_df.to_csv(f"{export_dir}/best_comparison.csv", index=False)
    
    print(f"Exported to {export_dir}")
;;;

metric.py
;;;
import itertools
from typing import List, Optional

from util import suppress_print

with suppress_print():
    from moleval.metrics.metrics import internal_diversity, novelty, preprocess_gen

from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.DataStructs import TanimotoSimilarity
from tqdm import tqdm


def canonicalize(smiles_list: List[str], pbar: bool = False) -> List[str]:
    """Returns a set of canonical smiles from a list of smiles."""
    canonical_smiles = []
    smiles_list_pbar = tqdm(smiles_list, desc="Canonicalize") if pbar else smiles_list
    for smi in smiles_list_pbar:
        mol = Chem.MolFromSmiles(smi)
        canonical_smi = Chem.MolToSmiles(mol, canonical=True)
        canonical_smiles.append(canonical_smi)
    return canonical_smiles

def calc_uniqueness(smiles_list: List[str], pbar: bool = False) -> float:
    canonical_smiles = set(canonicalize(smiles_list, pbar=pbar))
    return len(canonical_smiles) / len(smiles_list)

def calc_diversity(smiles_list: List[str], pbar: bool= False) -> float:
    # Generate fingerprints
    canonical_smiles = set(canonicalize(smiles_list, pbar=pbar))
    if len(canonical_smiles) <= 1:
        return 0.0
    canonical_smiles_pbar = tqdm(canonical_smiles, desc="Fingerprint") if pbar else canonical_smiles
    fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smi), 2, nBits=2048) for smi in canonical_smiles_pbar]
    # Calculate pairwise similarities
    comb_pbar = tqdm(itertools.combinations(fingerprints, 2), desc="Similarity") if pbar else itertools.combinations(fingerprints, 2)
    similarities = [TanimotoSimilarity(fp1, fp2) for fp1, fp2 in comb_pbar]
    # Calculate diversity
    average_similarity = sum(similarities) / len(similarities)
    return 1.0 - average_similarity

def calc_novelty(smiles_list: List[str], smiles_list_ref: List[str], pbar: bool = False) -> float:
    canonical_smiles = set(canonicalize(smiles_list, pbar=pbar))
    canonical_smiles_ref = set(canonicalize(smiles_list_ref, pbar=pbar))
    novel_smiles = canonical_smiles - canonical_smiles_ref
    return len(novel_smiles) / len(canonical_smiles)

class MolMetric:
    def __init__(self) -> None:
        self.smiles_refset = None
        self.mols_refset = None
        self.smiles_generated = None
        self.mols_generated = None
        self.uniqueness = None
                
    def preprocess(self, smiles_refset: Optional[List[str]] = None, smiles_generated: Optional[List[str]] = None) -> "MolMetric":
        if smiles_refset is None and smiles_generated is None:
            raise ValueError("Please provide a reference set or generated smiles.")
        
        if smiles_refset is not None:
            self.smiles_refset, self.mols_refset, _, _, _ = preprocess_gen(smiles_refset, n_jobs=self._n_jobs(smiles_refset))
        
        if smiles_generated is not None:
            self.smiles_generated, self.mols_generated, _, _, self.uniqueness = preprocess_gen(smiles_generated, n_jobs=self._n_jobs(smiles_generated))
        
        return self
    
    def calc_uniqueness(self) -> float:
        if self.uniqueness is None: raise ValueError("Please preprocess the generated smiles first.")
        return self.uniqueness
    
    def calc_diversity(self) -> float:
        if self.mols_generated is None: raise ValueError("Please preprocess the generated smiles first.")
        return internal_diversity(self.mols_generated, n_jobs=self._n_jobs(self.mols_generated))
    
    def calc_novelty(self) -> float:
        if self.smiles_refset is None: raise ValueError("Please preprocess a reference set first.")
        if self.smiles_generated is None: raise ValueError("Please preprocess the generated smiles first.")
        return novelty(self.smiles_generated, self.smiles_refset, n_jobs=self._n_jobs(self.smiles_generated))
    
    def _n_jobs(self, smiles_or_mols_list) -> int:
        return min(len(smiles_or_mols_list) // 1000 + 1, 100)

;;;

run.py
;;;
import argparse
from train import MolRLTrainFactory, MolRLInferenceFactory, MolRLPretrainFactory

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config_path", type=str, help="path to config file")
    parser.add_argument("-i", "--inference", action="store_true", help="inference mode")
    parser.add_argument("-p", "--pretrain", action="store_true", help="pretrain mode")
    args = parser.parse_args()
    config_path = args.config_path
    
    # Pretraining
    if args.pretrain:
        MolRLPretrainFactory.from_yaml(config_path) \
            .create_pretrain() \
            .pretrain() \
            .close()
    # RL Inference
    elif args.inference:
        MolRLInferenceFactory.from_yaml(config_path) \
            .create_inference() \
            .inference() \
            .close()
    # RL Training
    else:
        MolRLTrainFactory.from_yaml(config_path) \
            .create_train() \
            .train() \
            .close()
;;;

test.py
;;;
import unittest

import numpy as np

from envs.chem_env import ChemEnv, make_async_chem_env
from train import MolRLTrainFactory
import shutil

class Test(unittest.TestCase):
    def test_chem_env(self):
        print("=== test env ===")
        
        env = ChemEnv(
            plogp_coef=1.0,
            max_str_len=10
        )
        
        obs = env.reset()
        # check all elements of obs are -1
        self.assertTrue(np.all(obs == -1), f"obs: {obs}")
        terminated = False
        time_step = 0
        while not terminated:
            action = np.random.randint(env.num_actions, size=(1, 1))
            next_obs, reward, terminated, real_final_next_obs, _ = env.step(action)
            self.assertTrue(reward.shape == (1,), f"reward: {reward}")
            self.assertTrue(terminated.shape == (1,), f"terminated: {terminated}")
            if not terminated:
                self.assertTrue(next_obs[0, time_step] == action[0], f"time_step: {time_step}, next_obs: {next_obs}, action: {action}")
            else:
                self.assertTrue(np.all(next_obs == -1), f"next_obs: {next_obs}")
                self.assertTrue(real_final_next_obs[0, time_step] == action[0], f"time_step: {time_step}, real_final_next_obs: {real_final_next_obs}, action: {action}")
            time_step += 1
            
        env.close()
        
        print("=== test env completed ===")
        
    def test_async_chem_env(self):
        print("=== test make_async_chem_env ===")
        env = make_async_chem_env(
            num_envs=3,
            crwd_coef=1.0,
            plogp_coef=1.0,
            max_str_len=5,
        )
        
        self.assertTrue(env.num_envs == 3, f"num_envs: {env.num_envs}")
        self.assertTrue(env.obs_shape == (31,), f"obs_shape: {env.obs_shape}")
        self.assertTrue(env.num_actions == 31, f"num_actions: {env.num_actions}")
        
        obs = env.reset()
        self.assertTrue(obs.shape == (3, 31), f"obs shape: {obs.shape}")
        for _ in range(10):
            next_obs, reward, terminated, real_final_next_obs, info = env.step(
                np.random.randint(env.num_actions, size=(env.num_envs, 1))
            )
            self.assertTrue(next_obs.shape == (3, 31), f"next_obs shape: {next_obs.shape}")
            self.assertTrue(reward.shape == (3,), f"reward shape: {reward.shape}")
            self.assertTrue(terminated.shape == (3,), f"terminated shape: {terminated.shape}")
            self.assertTrue(real_final_next_obs.shape == (terminated.sum(), 31), f"real_final_next_obs shape: {real_final_next_obs.shape}")
            
            if "valid_termination" in info:
                print(f"valid_termination: {info['valid_termination']}")
                
            if "metric" in info:
                for metric_info in info["metric"]:
                    if metric_info is not None and "avg_count_int_reward" in metric_info["episode_metric"]["values"]:
                        print(f"avg_count_int_reward: {metric_info['episode_metric']['values']['avg_count_int_reward']}")
                
        env.close()
        print("=== test make_async_chem_env completed ===")
        
    def test_train_ppo(self):
        print("=== test train PPO ===")
        
        id = "Test_PPO"
        shutil.rmtree(f"results/{id}", ignore_errors=True)
        config = {
            "Agent": {
                "type": "PPO",
                "n_steps": 64,
                "seq_len": 35,
                "seq_mini_batch_size": 16,
                "epoch": 3,
            },
            "Env": {
                "plogp_coef": 1.0,
                "max_str_len": 10
            },
            "Train": {
                "num_envs": 3,
                "seed": 0,
                "total_time_steps": 1000,
                "summary_freq": 200,
            },
        }
        MolRLTrainFactory(id, config) \
            .create_train() \
            .train() \
            .close()
            
        print("=== test train PPO completed ===")
        
    def test_train_mol_air(self):
        print("=== test train MolAIR ===")
        
        id = "Test_MolAIR"
        shutil.rmtree(f"results/{id}", ignore_errors=True)
        config = {
            "Agent": {
                "type": "RND",
                "n_steps": 64,
                "seq_len": 35,
                "seq_mini_batch_size": 16,
                "epoch": 3,
            },
            "Env": {
                "plogp_coef": 1.0,
                "max_str_len": 10
            },
            "Train": {
                "num_envs": 3,
                "seed": 0,
                "total_time_steps": 1000,
                "summary_freq": 200,
            },
            "CountIntReward": {
                "crwd_coef": 1.0
            }
        }
        MolRLTrainFactory(id, config) \
            .create_train() \
            .train() \
            .close()
            
        print("=== test train MolAIR completed ===")
    
if __name__ == '__main__':
    unittest.main()
;;;

util.py
;;;
import builtins
import csv
import warnings
from collections import defaultdict
from dataclasses import dataclass
from typing import (Callable, Generic, Iterable, List, Optional, Tuple, Type,
                    TypeVar, Union)

import yaml

warnings.filterwarnings(action="ignore")
from torch.utils.tensorboard.writer import SummaryWriter

warnings.filterwarnings(action="default")
import inspect
import json
import os
import random
import sys
from contextlib import contextmanager
from io import TextIOWrapper

import matplotlib.pyplot as plt
import numpy as np
import selfies as sf
import torch
import torch.backends.cudnn as cudnn
from rdkit import Chem
from rdkit.Chem import Draw
from tbparse import SummaryReader
from tqdm import tqdm

T = TypeVar("T")

_random_seed = None

def seed(value: int):
    global _random_seed
    _random_seed = value
    torch.manual_seed(value)
    torch.cuda.manual_seed(value)
    torch.cuda.manual_seed_all(value)
    np.random.seed(value)
    cudnn.benchmark = False
    cudnn.deterministic = True
    random.seed(value)

class LoggerException(Exception):
    pass

class logger:
    @dataclass(frozen=True)
    class __log_file:
        tb_logger: SummaryWriter
        log_msg_file: TextIOWrapper
        
        def close(self):
            self.tb_logger.flush()
            self.tb_logger.close()
            self.log_msg_file.close()
            
    _enabled = False
    _LOG_BASE_DIR: str = "results"
    _log_dir: Optional[str] = None
    _log_file: Optional[__log_file] = None
    
    @classmethod
    def enabled(cls) -> bool:
        return cls._enabled
            
    @classmethod
    def enable(cls, id: str, enable_log_file: bool = True):
        if not cls._enabled:
            cls._enabled = True
            cls._log_dir = f"{cls._LOG_BASE_DIR}/{id}"
            if enable_log_file:
                cls._log_file = logger.__log_file(
                    tb_logger=SummaryWriter(log_dir=cls._log_dir),
                    log_msg_file=open(f"{cls._log_dir}/log.txt", "a"),
                )
        else:
            raise LoggerException("logger is already enabled")
        
    @classmethod
    def disable(cls):
        if cls._enabled:
            if cls._log_file is not None:
                cls._log_file.log_msg_file.write("\n")
                cls._log_file.close()
                cls._log_file = None
            cls._log_dir = None
            cls._enabled = False
        else:
            raise LoggerException("logger is already disabled")
        
    @classmethod
    def print(cls, message: str, prefix: str = "[Mol-AIR] "):
        builtins.print(f"{prefix}{message}")
        if cls._log_file is not None:
            cls._log_file.log_msg_file.write(f"{prefix}{message}\n")
            cls._log_file.log_msg_file.flush()
            
    @classmethod
    def log_data(cls, key, value, t):
        if cls._log_file is None:
            raise LoggerException("you need to enable the logger with enable_log_file option")
        cls._log_file.tb_logger.add_scalar(key, value, t)
        
    @classmethod
    def dir(cls) -> str:
        if cls._log_dir is None:
            raise LoggerException("logger is not enabled")
        return cls._log_dir
    
    @classmethod
    def plot_logs(cls):
        if cls._log_file is None:
            raise LoggerException("you need to enable the logger with enable_log_file option")
        cls._log_file.tb_logger.flush()
        
        # Read the logs
        reader = SummaryReader(cls.dir())
        
        # Get the DataFrame
        df = reader.scalars
        
        unique_tags = df["tag"].unique()
        
        for tag in unique_tags:
            elements = tag.split('/')
            file_name = elements[-1]
            dir_path = os.path.join(cls.dir(), "plots", *elements[:-1])
            os.makedirs(dir_path, exist_ok=True)
            file_path = os.path.join(dir_path, f"{file_name}.png")
            
            df_scalar = df[df["tag"] == tag]
            plt.figure()
            plt.plot(df_scalar["step"], df_scalar["value"])
            plt.title(tag)
            plt.xlabel("Step")
            plt.ylabel("Value")
            plt.grid(True)
            plt.savefig(file_path)
            plt.close()

class TextInfoBox:
    def __init__(self, right_margin: int = 10) -> None:
        self._texts = []
        self._right_margin = right_margin
        self._max_text_len = 0
        
    def add_text(self, text: Optional[str]) -> "TextInfoBox":
        if text is None:
            return self
        self._max_text_len = max(self._max_text_len, len(text))
        self._texts.append((f" {text} ", " "))
        return self
        
    def add_line(self, marker: str = "-") -> "TextInfoBox":
        if len(marker) != 1:
            raise ValueError(f"marker must be one character, but {marker}")
        self._texts.append(("", marker))
        return self
    
    def make(self) -> str:
        text_info_box = f"+{self._horizontal_line()}+\n"
        for text, marker in self._texts:
            text_info_box += f"|{text}{marker * (self._max_space_len - len(text))}|\n"
        text_info_box += f"+{self._horizontal_line()}+"
        return text_info_box
    
    def _horizontal_line(self, marker: str = "-") -> str:
        return marker * (self._max_space_len)

    @property
    def _max_space_len(self) -> int:
        return self._max_text_len + self._right_margin
    
def load_yaml(file_path: str) -> dict:
    with open(file_path, "r") as f:
        return yaml.load(f, Loader=yaml.FullLoader)
    
def save_yaml(file_path: str, data: dict):
    with open(file_path, "w") as f:
        yaml.dump(data, f, default_flow_style=False)
        
def dict_from_keys(d: dict, keys: Iterable) -> dict:
    matched_dict = dict()
    dict_keys = d.keys()
    for key in keys:
        if key in dict_keys:
            matched_dict[key] = d[key]
    return matched_dict

def instance_from_dict(class_type: Type[T], d: dict) -> T:
    params = tuple(inspect.signature(class_type).parameters)
    param_dict = dict_from_keys(d, params)
    return class_type(**param_dict)

def exists_dir(directory) -> bool:
    return os.path.exists(directory)

def file_exists(file_path: str) -> bool:
    return os.path.isfile(file_path)

def try_create_dir(directory):
    """If there's no directory, create it."""
    try:
        if not exists_dir(directory):
            os.makedirs(directory)
    except OSError:
        print("Error: Failed to create the directory.")

class ItemUpdateFollower(Generic[T]):
    def __init__(self, init_item: T, include_init: bool = True):
        self._item = init_item
        self._items = []
        if include_init:
            self._items.append(init_item)
        
    def update(self, item: T):
        self._item = item
        self._items.append(item)
        
    def popall(self) -> Tuple[T, ...]:
        items = tuple(self._items)
        self._items.clear()
        return items
    
    @property
    def item(self) -> T:
        return self._item
    
    def __len__(self) -> int:
        return len(self._items)

def moving_average(values: np.ndarray, n: Optional[int] = None, smooth: Optional[float] = None):
    if (n is None and smooth is None) or (n is not None and smooth is not None):
        raise ValueError("you must specify either n or smooth")
    if smooth is not None:
        if smooth < 0.0 or smooth > 1.0:
            raise ValueError(f"smooth must be in [0, 1], but got {smooth}")
        n = int((1.0 - smooth) * 1 + smooth * len(values))
    ret = np.cumsum(values, dtype=float)
    ret[n:] = (ret[n:] - ret[:-n]) / n
    ret[:n] = ret[:n] / np.arange(1, n + 1)
    return ret

def exponential_moving_average(values, smooth: float) -> np.ndarray:
    if smooth < 0.0 or smooth > 1.0:
        raise ValueError(f"smooth must be in [0, 1], but got {smooth}")
    ema = np.zeros_like(values)
    ema[0] = values[0]
    for i in range(1, len(values)):
        ema[i] = smooth * values[i] + (1.0 - smooth) * ema[i - 1]
    return ema


class SyncFixedBuffer(Generic[T]):
    def __init__(self, max_size: int, callback: Optional[Callable[[Iterable[T]], None]] = None):
        self._max_size = max_size
        self._buffer: List[Optional[T]] = [None for _ in range(self._max_size)]
        self._updated = [False for _ in range(self._max_size)]
        self._sync_count = 0
        self._callback = callback
        
    @property
    def sync_done(self) -> bool:
        return self._sync_count == self._max_size
        
    def __len__(self):
        return len(self._buffer)
    
    def __getitem__(self, index) -> Optional[T]:
        return self._buffer[index]
    
    def __setitem__(self, index, value: T):
        self._buffer[index] = value # type: ignore
        if not self._updated[index]:
            self._updated[index] = True
            self._sync_count += 1
        if self._callback is not None and self.sync_done:
            self._callback(tuple(self._buffer)) # type: ignore
        
    def __iter__(self):
        return iter(self._buffer)

class CSVSyncWriter:
    """
    Write a csv file with key and value fields. The key fields are used to identify the data.
    """
    def __init__(
        self,
        file_path: str,
        key_fields: Iterable[str],
        value_fields: Iterable[str],
    ) -> None:
        self._key_fields = tuple(key_fields)
        self._value_fields = tuple(value_fields)        
        self._check_fields_unique()
        self._value_buffer = defaultdict(dict)
        self._field_types = {}
        
        self._file_path = file_path
        try:
            with open(self._file_path, "r") as f:
                reader = csv.DictReader(f)
                if reader.fieldnames is None:
                    raise ValueError
                if len(reader.fieldnames) != len(self.fields):
                    raise FileExistsError(f"The number of fields in the csv file is different from the number of fields in the config. Create a new csv file.")
        except (FileNotFoundError, ValueError):
            # if the file does not exist or the file has no header, create a new csv file
            self._reset_csv()
                
    def add(self, keys: Union[Tuple, dict], values: dict):
        """
        Add a new data to the csv file. If the data has all required values, write it to the csv file.
        
        Args:
            keys (tuple | dict): keys of the data. You must specify all keys.
            values (dict): values of the data. It automatically extracts required values from the `values` dict.
        """
        if len(keys) != len(self._key_fields):
            raise ValueError(f"keys must have {len(self._key_fields)} elements, but got {len(keys)}")
        if isinstance(keys, dict):
            keys = tuple(keys[key_field] for key_field in self._key_fields)
        # update the buffer with the new data only if values fields is in value_fields
        self._value_buffer[keys].update(dict_from_keys(values, self._value_fields))
        # check if it has all required values for these keys
        if len(self._value_buffer[keys]) == len(self._value_fields):
            if len(self._field_types) != len(self.fields):
                key_field_types = {key_field: type(key) for key_field, key in zip(self._key_fields, keys)}
                value_field_types = {value_field: type(value) for value_field, value in self._value_buffer[keys].items() if value is not None}
                self._field_types.update(key_field_types)
                self._field_types.update(value_field_types)
            self._write_csv(keys)
            # remove the keys from the buffer
            del self._value_buffer[keys]
            
    @property
    def key_fields(self) -> Tuple[str, ...]:
        return self._key_fields
    
    @property
    def value_fields(self) -> Tuple[str, ...]:
        return self._value_fields
    
    @value_fields.setter
    def value_fields(self, value: Iterable[str]):
        # update the value fields
        self._value_fields = tuple(value)
        self._check_fields_unique()
        # update the buffer from the old csv file
        with open(self._file_path, "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                keys = tuple(self._field_types[key_field](row[key_field]) for key_field in self._key_fields)
                raw_value_dict = dict_from_keys(row, self._value_fields)
                # type conversion
                value_dict = {}
                for value_field, raw_value in raw_value_dict.items():
                    if raw_value is None or raw_value == "":
                        value_dict[value_field] = raw_value
                    else:
                        value_dict[value_field] = self._field_types[value_field](raw_value)                
                self._value_buffer[keys] = value_dict
        self._reset_csv()
    
    @property
    def fields(self) -> Tuple[str, ...]:
        return self.key_fields + self.value_fields
    
    def _check_fields_unique(self):
        if len(self.fields) != len(set(self.fields)):
            raise ValueError(f"all key and value fields must be unique")
    
    def _write_csv(self, keys: Tuple):
        with open(self._file_path, "a") as f:
            writer = csv.DictWriter(f, fieldnames=self.fields)
            writer.writerow({**dict(zip(self._key_fields, keys)), **self._value_buffer[keys]})
            
    def _reset_csv(self):
        with open(self._file_path, "w") as f:
            writer = csv.DictWriter(f, fieldnames=self.fields)
            writer.writeheader()
            
def draw_molecules(smiles_list: List[str], scores: Optional[List[float]] = None, mols_per_row: int = 5, title: str = ""):
    molecules = [Chem.MolFromSmiles(smiles) for smiles in smiles_list]
    labels = [f"SMILES: {smiles}" for smiles in smiles_list]
    if scores is not None:
        if len(molecules) != len(scores):
            raise ValueError(f"The number of molecules and scores must be the same, but got {len(molecules)} and {len(scores)}")
        labels = [f"{label}\nScore: {score}" for label, score in zip(labels, scores)]
    try:
        return Draw.MolsToGridImage(molecules, molsPerRow=mols_per_row, subImgSize=(500, 500), legends=labels)
    except ImportError:
        raise ImportError("You cannot draw molecules due to the lack of libXrender.so.1. Install it with `sudo apt-get install libxrender1` or `conda install -c conda-forge libxrender`.")
    
def save_vocab(vocab: List[str], max_str_len: int, file_path: str):
    with open(file_path, "w") as f:
        json.dump({"vocabulary": vocab, "max_str_len": max_str_len}, f, indent=4)
        
def load_vocab(file_path: str) -> Tuple[List[str], int]:
    with open(file_path, "r") as f:
        data = json.load(f)
    return data["vocabulary"], data["max_str_len"]

def save_smiles_or_selfies(smiles_or_selfies_list: List[str], file_path: str):
    with open(file_path, "w") as f:
        for smiles_or_selfies in smiles_or_selfies_list:
            f.write(f"{smiles_or_selfies}\n")

def load_smiles_or_selfies(file_path: str) -> List[str]:
    with open(file_path, "r") as f:
        return f.read().splitlines()
    
def to_selfies(smiles_or_selfies_list: List[str], verbose: bool = True) -> List[str]:
    if smiles_or_selfies_list[0].count("[") > 0:
        return smiles_or_selfies_list
    
    smiles_or_selfies_iter = tqdm(smiles_or_selfies_list, desc="Converting SMILES to SELFIES") if verbose else smiles_or_selfies_list
    selfies_list = [sf.encoder(s) for s in smiles_or_selfies_iter]
    return selfies_list

def to_smiles(smiles_or_selfies_list: List[str], verbose: bool = True) -> List[str]:
    if smiles_or_selfies_list[0].count("[") == 0:
        return smiles_or_selfies_list
    
    smiles_or_selfies_iter = tqdm(smiles_or_selfies_list, desc="Converting SELFIES to SMILES") if verbose else smiles_or_selfies_list
    smiles_list = [sf.decoder(s) for s in smiles_or_selfies_iter]
    return smiles_list # type: ignore

@contextmanager
def suppress_print():
    original_stdout = sys.stdout  # Save original stdout
    original_stderr = sys.stderr  # Save original stderr
    sys.stdout = open(os.devnull, 'w')  # Redirect stdout to /dev/null
    sys.stderr = open(os.devnull, 'w')  # Redirect stderr to /dev/null
    try:
        yield
    finally:
        sys.stdout.close()  # Close redirected stdout
        sys.stderr.close()  # Close redirected stderr
        sys.stdout = original_stdout  # Restore original stdout
        sys.stderr = original_stderr  # Restore original stderr
;;;

drl/exp.py
;;;
from typing import Callable
from dataclasses import dataclass
from torch import Tensor

@dataclass(frozen=True)
class Experience:
    """
    Experience data type. 
    
    Args:
        obs (Tensor): `(num_envs, *obs_shape)`
        action (Tensor): `(num_envs, num_actions)`
        next_obs (Tensor): `(num_envs, *obs_shape)`
        reward (Tensor): `(num_envs, 1)`
        terminated (Tensor): `(num_envs, 1)`
    """
    obs: Tensor
    action: Tensor
    next_obs: Tensor
    reward: Tensor
    terminated: Tensor
    
    def transform(self, func: Callable[[Tensor], Tensor]) -> "Experience":
        """
        Transform the experience data type.

        Args:
            func (Callable[[TSource], TResult]): transform function

        Returns:
            Experience[TResult]: transformed experience
        """
        return Experience(
            obs=func(self.obs),
            action=func(self.action),
            next_obs=func(self.next_obs),
            reward=func(self.reward),
            terminated=func(self.terminated)
        )
        
;;;

drl/net.py
;;;
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Iterable, Tuple, Union

import torch
import torch.nn as nn
from torch.nn.utils.clip_grad import clip_grad_norm_


class Trainer:
    """
    PyTorch optimizer wrapper for single scalar loss.
    """
    @dataclass(frozen=True)
    class __ClipGradNormConfig:
        parameters: Union[torch.Tensor, Iterable[torch.Tensor]]
        max_norm: float
        norm_type: float = 2.0
        error_if_nonfinite: bool = False
    
    def __init__(self, optimizer: torch.optim.Optimizer) -> None:
        self._optimizer = optimizer
        self._clip_grad_norm_config = None
        
    def enable_grad_clip(
        self,
        parameters: Union[torch.Tensor, Iterable[torch.Tensor]],
        max_norm: float,
        norm_type: float = 2.0,
        error_if_nonfinite: bool = False
    ) -> "Trainer":
        """
        Enable gradient clipping when parameter update. Clips gradient norm of an iterable of parameters.

        The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.

        Args:
            parameters (Tensor | Iterable[Tensor]): an iterable of Tensors or a single Tensor that will have gradients normalized
            max_norm (float): max norm of the gradients
            norm_type (float, optional): type of the used p-norm. Can be `inf` for infinity norm.
            error_if_nonfinite (bool, optional): if True, an error is thrown if the total norm of the gradients from `parameters` is `nan`, `inf`, or `-inf`. Default: False (will switch to True in the future)

        Returns:
            Trainer: self
        """
        self._clip_grad_norm_config = Trainer.__ClipGradNormConfig(
            parameters,
            max_norm,
            norm_type,
            error_if_nonfinite
        )
        return self
    
    def step(self, loss: torch.Tensor, training_steps: int):
        """
        Performs a single optimization step (parameter update).
        """
        self._optimizer.zero_grad()
        loss.backward()
        if self._clip_grad_norm_config is not None:
            clip_grad_norm_(**self._clip_grad_norm_config.__dict__)
        self._optimizer.step()

class Network(ABC):
    @abstractmethod
    def model(self) -> nn.Module:
        raise NotImplementedError
    
    @property
    def device(self) -> torch.device:
        return next(self.model().parameters()).device

class RecurrentNetwork(Network):
    """
    Recurrent neural network (RNN).
    """
    
    @abstractmethod
    def hidden_state_shape(self) -> Tuple[int, int]:
        """
        Returns the shape of the rucurrent hidden state `(D x num_layers, H)`.
        
        * `num_layers`: the number of recurrent layers
        * `D`: 2 if bidirectional otherwise 1
        * `H`: the value depends on the type of the recurrent network

        When you use LSTM, `H` = `H_cell` + `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html. 
        When you use GRU, `H` = `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.
        """
        raise NotImplementedError

def wrap_lstm_hidden_state(
    h: torch.Tensor, 
    c: torch.Tensor
) -> torch.Tensor:
    """
    Wrap the hidden state of LSTM.
    
    Args:
        h (Tensor): `(D x num_layers, seq_batch_size, H_out)`
        c (Tensor): `(D x num_layers, seq_batch_size, H_cell)`
    
    Returns:
        hc (Tensor): `(D x num_layers, seq_batch_size, H_out + H_cell)`
    """
    return torch.cat((h, c), dim=2)

def unwrap_lstm_hidden_state(
    hc: torch.Tensor, 
    h_size: Union[int, None] = None, 
    c_size: Union[int, None] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Unwrap the hidden state of LSTM.
    
    Note if `H_out` and `H_cell` are different size, you must specify both of them.

    Args:
        hc (Tensor): `(D x num_layers, seq_batch_size, H_out + H_cell)`
        h_size (int | None): `H_out`. Defaults to `H_out` = `H_cell`.
        c_size (int | None): `H_cell`. Defaults to `H_cell` = `H_out`.

    Returns:
        h (Tensor): `(D x num_layers, seq_batch_size, H_out)`
        c (Tensor): `(D x num_layers, seq_batch_size, H_cell)`
    """
    if (h_size is None) ^ (c_size is None):
        raise ValueError("if `H_out` and `H_cell` are different size, you must specify both of them.")
    
    if (h_size is None) and (c_size is None):
        h_size = c_size = hc.shape[2] // 2
    
    h, c = hc.split([h_size, c_size], dim=2)
    return h.contiguous(), c.contiguous()
;;;

drl/policy.py
;;;
from typing import Optional

import torch
import torch.nn as nn

from drl.policy_dist import CategoricalDist


class CategoricalPolicy(nn.Module):
    def __init__(
        self,
        in_features: int,
        num_discrete_actions: int,
        bias: bool = True,
        device: Optional[torch.device] = None,
        dtype = None,
        temperature: float = 1.0
    ) -> None:
        super().__init__()
        
        self._layer = nn.Linear(
            in_features,
            num_discrete_actions,
            bias,
            device,
            dtype
        )
        
        self._temperature = temperature
        
    def forward(self, x: torch.Tensor) -> CategoricalDist:
        logits = self._layer(x) / self._temperature
        return CategoricalDist(logits=logits)
;;;

drl/policy_dist.py
;;;
# drl/policy_dist.py
import torch
import torch.nn.functional as F
from abc import abstractmethod
from typing import Optional

class Distribution:
    @abstractmethod
    def sample(self) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def entropy(self) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def mode(self) -> torch.Tensor:
        raise NotImplementedError

class CategoricalDist(Distribution):
    """
    Categorical distribution based on logits.
    """
    def __init__(self, logits: torch.Tensor):
        # Ensure logits are float for stability with distributions
        self._logits = logits.float()
        # It's generally safer to create the distribution object as needed
        # rather than storing it, especially if logits change.
        # self.dist = torch.distributions.Categorical(logits=self._logits)

    def sample(self) -> torch.Tensor:
        # Create distribution on the fly
        dist = torch.distributions.Categorical(logits=self._logits)
        # Sample and add action dimension
        return dist.sample().unsqueeze(-1)

    def log_prob(self, actions: torch.Tensor) -> torch.Tensor:
        # Create distribution on the fly
        dist = torch.distributions.Categorical(logits=self._logits)
        # Remove action dimension if present before calculating log_prob
        actions_squeezed = actions.squeeze(-1)
        # Calculate log_prob and add action dimension back
        return dist.log_prob(actions_squeezed).unsqueeze(-1)

    def entropy(self) -> torch.Tensor:
        # Create distribution on the fly
        dist = torch.distributions.Categorical(logits=self._logits)
        # Calculate entropy and add action dimension
        return dist.entropy().unsqueeze(-1)

    @property
    def logits(self) -> torch.Tensor:
        return self._logits

    def mode(self) -> torch.Tensor:
        """Returns the action with the highest probability (logit)."""
        # Find the index of the max logit along the action dimension
        # Add an action dimension at the end
        return torch.argmax(self._logits, dim=-1, keepdim=True)

# Keep SACDistribution interface if needed for other distribution types later
class SACDistribution(Distribution):
    @abstractmethod
    def sample(self) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def entropy(self) -> torch.Tensor:
        raise NotImplementedError
    @abstractmethod
    def mode(self) -> torch.Tensor:
        raise NotImplementedError

;;;

drl/rl_loss.py
;;;
import torch
import torch.nn.functional as F

def gae(
    state_value: torch.Tensor, 
    reward: torch.Tensor, 
    terminated: torch.Tensor,
    gamma: float,
    lam: float
) -> torch.Tensor:
    """
    Compute generalized advantage estimation (GAE) during n-step transitions. See details in https://arxiv.org/abs/1506.02438.

    Args:
        state_value (Tensor): state value `(num_envs, n_steps + 1)`, 
        which includes the next state value of final transition
        reward (Tensor): `(num_envs, n_steps)`
        terminated (Tensor): `(num_envs, n_steps)`
        gamma (float): discount factor
        lam (float): lambda or bias-variance trade-off parameter

    Returns:
        GAE (Tensor): `(num_envs, n_steps)`
    """
    
    n_step = reward.shape[1]
    advantage = torch.empty_like(reward)
    discounted_gae = 0.0 # GAE at time step t+n
    not_terminated = 1 - terminated
    delta = reward + not_terminated * gamma * state_value[:, 1:] - state_value[:, :-1]
    discount_factor = gamma * lam
    
    # compute GAE
    for t in reversed(range(n_step)):
        discounted_gae = delta[:, t] + not_terminated[:, t] * discount_factor * discounted_gae
        advantage[:, t] = discounted_gae
     
    return advantage

def bellman_value_loss(
    predicted_state_value: torch.Tensor,
    target_state_value: torch.Tensor
) -> torch.Tensor:
    """
    Bellman value loss which is L2 loss.

    Args:
        predicted_state_value (Tensor): `(batch_size, 1)`
        target_state_value (torch.Tensor): `(batch_size, 1)`

    Returns:
        loss (Tensor): scalar value
    """
    return F.mse_loss(predicted_state_value, target_state_value)

def ppo_clipped_loss(
    advantage: torch.Tensor,
    old_action_log_prob: torch.Tensor,
    new_action_log_prob: torch.Tensor,
    epsilon: float = 0.2
) -> torch.Tensor:
    """
    PPO clipped surrogate loss according to the policy gradient theorem.
    
    It uses mean loss (but not sum loss).

    Args:
        advantage (Tensor): whose shape is `(batch_size, 1)`
        old_action_log_prob (Tensor): log(pi_old) `(batch_size, 1)`
        new_action_log_prob (Tensor): log(pi_new) `(batch_size, 1)`
        epsilon (float, optional): clamps the probability ratio (pi_new / pi_old) into the range [1-eps, 1+eps]. Defaults to 0.2.

    Returns:
        loss (Tensor): scalar value
    """
    # r = pi_new / pi_old
    old_action_log_prob = old_action_log_prob.detach()
    ratio = torch.exp(new_action_log_prob - old_action_log_prob)
    
    # clipped surrogate loss
    sur1 = ratio * advantage
    sur2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage
    return -torch.min(sur1, sur2).mean()

def rnd_loss(
    predicted_feature: torch.Tensor,
    target_feature: torch.Tensor,
    rnd_pred_exp_proportion: float = 0.25
) -> torch.Tensor:
    """
    Random network distillation (RND) loss which is L2 loss.
    Experiences are randomly selected to keep the effective batch size.

    Args:
        predicted_feature (torch.Tensor): `(batch_size, feature_dim)`
        target_feature (torch.Tensor): `(batch_size, feature_dim)`
        rnd_pred_exp_proportion (float, optional): the proportion of experiences. Defaults to 0.25.

    Returns:
        loss (Tensor): scalar value
    """
    loss = F.mse_loss(
        predicted_feature,
        target_feature.detach(),
        reduction="none"
    ).mean(dim=-1)
    # the proportion of experiences to keep the effective batch size
    mask = torch.rand(len(loss), device=loss.device)
    mask = (mask < rnd_pred_exp_proportion).to(dtype=loss.dtype)
    loss = (loss * mask).sum() / torch.max(mask.sum(), torch.tensor(1.0, device=loss.device))
    return loss
;;;

drl/__init__.py
;;;
from .exp import Experience
from .net import Trainer, Network, RecurrentNetwork

;;;

drl/agent/agent.py
;;;
from abc import ABC, abstractmethod
from typing import Dict, Optional, TypeVar, Type, Callable

import torch
import torch.nn as nn

from drl.exp import Experience
from drl.net import Network

T = TypeVar('T', bound='Agent')

def agent_config(name: str) -> Callable[[Type[T]], Type[T]]:    
    def decorator(cls: Type[T]) -> Type[T]:
        if not issubclass(cls, Agent):
            raise TypeError("Class must inherit from Agent")
        cls.name = name
        return cls
    return decorator

class Agent(ABC):
    """
    Deep reinforcement learning agent.
    """
    
    name: str
    
    def __init__(
        self,
        num_envs: int, 
        network: Network,
        device: Optional[str] = None,
    ) -> None:
        assert num_envs >= 1, "The number of environments must be greater than or euqal to 1."
                
        self._model = network.model()
        if device is not None:
            self._model = self._model.to(device=torch.device(device))
        self._device = network.device
        self._num_envs = num_envs
        
        self._training_steps = 0
                
    @abstractmethod
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Select actions from the observation. 

        Args:
            obs (Tensor): observation `(num_envs, *obs_shape)`

        Returns:
            action (Tensor): `(num_envs, num_action_branches)`
        """
        raise NotImplementedError()
        
    @abstractmethod
    def update(self, exp: Experience) -> Optional[dict]:
        """
        Update the agent. It stores data and trains the agent.

        Args:
            exp (Experience): one-step experience tuple.
        """
        raise NotImplementedError()
    
    @abstractmethod
    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> "Agent":
        """
        Create an inference agent.

        Args:
            num_envs (int, optional): The number of environments for inference. Defaults to 1.
            device (Optional[str], optional): Defaults to `Agent` device.

        Returns:
            Agent: inference agent
        """
        raise NotImplementedError
    
    @property
    def config_dict(self) -> dict:
        return dict()
    
    @property
    def model(self) -> nn.Module:
        return self._model
    
    @property
    def device(self) -> torch.device:
        return self._device
    
    @property
    def num_envs(self) -> int:
        return self._num_envs
    
    @property
    def training_steps(self) -> int:
        return self._training_steps
    
    def _tick_training_steps(self):
        self._training_steps += 1
        
    @property
    def log_data(self) -> Dict[str, tuple]:
        """
        Returns log data and reset it.

        Returns:
            dict[str, tuple]: key: (value, time)
        """
        return dict()
            
    @property
    def state_dict(self) -> dict:
        """Returns the state dict of the agent."""
        return dict(
            training_steps=self._training_steps,
            model=self._model.state_dict()
        )
    
    def load_state_dict(self, state_dict: dict):
        """Load the state dict."""
        self._training_steps = state_dict["training_steps"]
        self._model.load_state_dict(state_dict["model"])

;;;

drl/agent/config.py
;;;
# drl/agent/config.py
from dataclasses import dataclass, field
from typing import Optional, Tuple

@dataclass(frozen=True)
class RecurrentPPOConfig:
    """
    Recurrent PPO configurations.
    """
    n_steps: int
    epoch: int
    seq_len: int
    seq_mini_batch_size: int
    padding_value: float = 0.0
    gamma: float = 0.99
    lam: float = 0.95
    epsilon_clip: float = 0.2
    critic_loss_coef: float = 0.5
    entropy_coef: float = 0.001

@dataclass(frozen=True)
class RecurrentPPORNDConfig:
    """
    Recurrent PPO with RND configurations.
    """
    n_steps: int
    epoch: int
    seq_len: int
    seq_mini_batch_size: Optional[int] = None
    padding_value: float = 0.0
    gamma: float = 0.99
    gamma_n: float = 0.99 # Discount factor for non-episodic (intrinsic) rewards
    nonepi_adv_coef: float = 1.0 # Coefficient for non-episodic advantage
    lam: float = 0.95
    epsilon_clip: float = 0.2
    critic_loss_coef: float = 0.5
    entropy_coef: float = 0.001
    rnd_pred_exp_proportion: float = 0.25
    init_norm_steps: Optional[int] = 50
    obs_norm_clip_range: Tuple[float, float] = (-5.0, 5.0)
    hidden_state_norm_clip_range: Tuple[float, float] = (-5.0, 5.0)

# --- New SAC Configuration ---
@dataclass(frozen=True)
class RecurrentSACConfig:
    """
    Recurrent Soft Actor-Critic (SAC) configurations.
    """
    # --- SAC Specific ---
    gamma: float = 0.99 # Discount factor
    tau: float = 0.005 # Coefficient for soft update of target networks
    alpha: float = 0.2 # Initial entropy regularization coefficient
    actor_lr: float = 3e-4 # Learning rate for the actor network
    critic_lr: float = 3e-4 # Learning rate for the critic network(s)
    alpha_lr: float = 3e-4 # Learning rate for the entropy coefficient alpha (if learned)
    target_update_interval: int = 1 # Frequency of target network updates (in terms of gradient steps)
    learn_alpha: bool = True # Whether to automatically tune the entropy coefficient alpha
    target_entropy_ratio: float = 0.98 # Target entropy ratio (relative to max entropy: -log(1/|A|)*ratio)

    # --- Buffer & Training ---
    buffer_size: int = 100000 # Size of the replay buffer
    batch_size: int = 256 # Minibatch size for sampling from the replay buffer
    learning_starts: int = 1000 # Number of steps to collect before starting training
    gradient_steps: int = 1 # Number of gradient updates per environment step

    # --- Recurrent Specific (Adjust based on network needs) ---
    # seq_len might not be needed as SAC learns from transitions, not sequences like PPO
    # seq_len: int = 1

    # --- Common Agent Params (May have different meaning/usage in SAC) ---
    n_steps: int = 1 # Number of environment steps per agent interaction cycle (often 1 for SAC)

    # --- Optional RND Integration (If combining SAC with RND) ---
    # Add RND-specific parameters here if needed, similar to RecurrentPPORNDConfig
    # gamma_n: float = 0.99
    # nonepi_adv_coef: float = 1.0
    # rnd_pred_exp_proportion: float = 0.25
    # init_norm_steps: Optional[int] = 50
    # obs_norm_clip_range: Tuple[float, float] = (-5.0, 5.0)
    # hidden_state_norm_clip_range: Tuple[float, float] = (-5.0, 5.0)

;;;

drl/agent/net.py
;;;
from abc import abstractmethod
from typing import Tuple

import torch

from drl.net import RecurrentNetwork
from drl.policy_dist import CategoricalDist

class PretrainedRecurrentNetwork(RecurrentNetwork):
    """
    Pretrained recurrent network.
    """
    
    @abstractmethod
    def forward(
        self, 
        obs_seq: torch.Tensor, 
        hidden_state: torch.Tensor
    ) -> Tuple[CategoricalDist, torch.Tensor]:
        """
        ## Summary
        
        Feed forward method to compute policy distribution using the recurrent network.
        
        It's recommended to set your recurrent network to `batch_first=True`.

        Args:
            obs_seq (Tensor): observation sequences
            hidden_state (Tensor): hidden states at the beginning of each sequence

        Returns:
            policy_dist_seq (CategoricalDist): policy distribution sequences
            next_seq_hidden_state (Tensor): hidden state which will be used for the next sequence
            
        ## Input/Output Details
        
        |Input|Shape|
        |:---|:---|
        |obs_seq|`(seq_batch_size, seq_len, *obs_shape)`|
        |hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Output:
        
        |Output|Shape|
        |:---|:---|
        |policy_dist_seq|`*batch_shape` = `(seq_batch_size, seq_len)`, details in `PolicyDist` docs|
        |next_seq_hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Refer to the following explanation:
        
        * `seq_batch_size`: the size of sequence batch
        * `seq_len`: the length of each sequence
        * `num_layers`: the number of recurrent layers
        * `D`: 2 if bidirectional otherwise 1
        * `H`: the value depends on the type of the recurrent network
        
        When you use LSTM, `H` = `H_cell` + `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html. 
        When you use GRU, `H` = `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.
        """
        raise NotImplementedError

class RecurrentPPONetwork(RecurrentNetwork):
    """
    Recurrent Proximal Policy Optimization (PPO) shared network.
    
    Since it uses the recurrent network, 
    you must consider the hidden state which can acheive the action-observation history.
    
    Note that since it uses the Actor-Critic architecure and the parameter sharing, 
    the encoding layer must be shared between Actor and Critic. 
    """
        
    @abstractmethod
    def forward(
        self, 
        obs_seq: torch.Tensor, 
        hidden_state: torch.Tensor
    ) -> Tuple[CategoricalDist, torch.Tensor, torch.Tensor]:
        """
        ## Summary
        
        Feed forward method to compute policy distribution 
        and state value using the recurrent network.
        
        It's recommended to set your recurrent network to `batch_first=True`.

        Args:
            obs_seq (Tensor): observation sequences
            hidden_state (Tensor): hidden states at the beginning of each sequence

        Returns:
            policy_dist_seq (CategoricalDist): policy distribution sequences
            state_value_seq (Tensor): state value sequences
            next_seq_hidden_state (Tensor): hidden state which will be used for the next sequence
            
        ## Input/Output Details
        
        |Input|Shape|
        |:---|:---|
        |obs_seq|`(seq_batch_size, seq_len, *obs_shape)`|
        |hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Output:
        
        |Output|Shape|
        |:---|:---|
        |policy_dist_seq|`*batch_shape` = `(seq_batch_size, seq_len)`, details in `PolicyDist` docs|
        |state_value_seq|`(seq_batch_size, seq_len, 1)`|
        |next_seq_hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Refer to the following explanation:
        
        * `seq_batch_size`: the size of sequence batch
        * `seq_len`: the length of each sequence
        * `num_layers`: the number of recurrent layers
        * `D`: 2 if bidirectional otherwise 1
        * `H`: the value depends on the type of the recurrent network
        
        When you use LSTM, `H` = `H_cell` + `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html. 
        When you use GRU, `H` = `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.
        """
        raise NotImplementedError
    
class RecurrentPPORNDNetwork(RecurrentNetwork):
    """
    Recurrent Proximal Policy Optimization (PPO) shared network with Random Network Distillation (RND).
    
    Since it uses the recurrent network, you must consider the hidden state which can acheive the action-observation history.
    
    Note that since PPO uses the Actor-Critic architecure and the parameter sharing, 
    the encoding layer must be shared between Actor and Critic. 
    Be careful not to share parameters between PPO and RND networks.
    
    RND uses episodic and non-episodic reward streams. 
    RND constitutes of the predictor and target networks. 
    Both of them should have the similar architectures (not must same) but their initial parameters should not be the same.
    The target network is determinsitic, which means it will be never updated. 
    """
        
    @abstractmethod
    def forward_actor_critic(
        self, 
        obs_seq: torch.Tensor, 
        hidden_state: torch.Tensor
    ) -> Tuple[CategoricalDist, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ## Summary
        
        Feed forward method to compute policy distribution, 
        episodic state value and non-episodic state value using the recurrent network.
                
        It's recommended to set your recurrent network to `batch_first=True`.
        
        Args:
            obs_seq (Tensor): observation sequences
            hidden_state (Tensor): hidden states at the beginning of each sequence

        Returns:
            policy_dist_seq (Tensor): policy distribution sequences
            epi_state_value_seq (Tensor): episodic state value sequences
            nonepi_state_value_seq (Tensor): non-episodic state value sequences
            next_seq_hidden_state (Tensor): hidden state which will be used for the next sequence
        
        ## Input/Output Details
                
        Input:
        
        |Input|Shape|
        |:---|:---|
        |obs_seq|`(seq_batch_size, seq_len, *obs_shape)`|
        |hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Output:
        
        |Output|Shape|
        |:---|:---|
        |policy_dist_seq|`*batch_shape` = `(seq_batch_size, seq_len)`, details in `PolicyDist` docs|
        |epi_state_value_seq|`(seq_batch_size, seq_len, 1)`|
        |nonepi_state_value_seq|`(seq_batch_size, seq_len, 1)`|
        |next_seq_hidden_state|`(D x num_layers, seq_batch_size, H)`|
        
        Refer to the following explanation:
        
        * `seq_batch_size`: the size of sequence batch
        * `seq_len`: the length of each sequence
        * `num_layers`: the number of recurrent layers
        * `D`: 2 if bidirectional otherwise 1
        * `H`: the value depends on the type of the recurrent network
        
        When you use LSTM, `H` = `H_cell` + `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html. 
        When you use GRU, `H` = `H_out`. See details in https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.
        """
        raise NotImplementedError

    @abstractmethod
    def forward_rnd(self, obs: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ## Summary
        
        Feed forward method tp to compute both predicted feature and target feature. 
        You can use the hidden state by concatenating it with either the observation or the embedding of the observation. 

        Args:
            obs (Tensor): observation batch
            hidden_state (Tensor): hidden state batch with flattened features

        Returns:
            predicted_feature (Tensor): predicted feature whose gradient flows
            target_feature (Tensor): target feature whose gradient doesn't flow
            
        ## Input/Output Details
        
        The value of `out_features` depends on you.
        
        Input:
        
        |Input|Shape|
        |:---|:---|
        |obs|`(batch_size, *obs_shape)`|
        |hidden_state|`(batch_size, D x num_layers x H)`|
        
        Output:
        
        |Input|Shape|
        |:---|:---|
        |predicted_feature|`(batch_size, out_features)`|
        |target_feature|`(batch_size, out_features)`|
        """
        raise NotImplementedError

from abc import abstractmethod
from typing import Tuple, Union, Iterable

import torch
import torch.nn as nn

from drl.net import RecurrentNetwork, Network # Assuming drl.net contains base Network/RecurrentNetwork
from drl.policy_dist import CategoricalDist # Keep for PPO/Pretrained


# --- New Interface for Recurrent SAC Agent ---
class RecurrentSACNetwork(RecurrentNetwork):
    """
    Recurrent Soft Actor-Critic (SAC) network interface.
    Requires separate actor and critic components.
    """

    @abstractmethod
    def forward_actor(
        self,
        obs_seq: torch.Tensor,
        hidden_state: torch.Tensor
    ) -> Tuple[CategoricalDist, torch.Tensor]:
        """
        Forward pass for the actor network.

        Args:
            obs_seq (torch.Tensor): Observation sequences (batch_size, seq_len, *obs_shape).
            hidden_state (torch.Tensor): Actor's hidden states (num_layers * D, batch_size, H_actor).

        Returns:
            Tuple[CategoricalDist, torch.Tensor]:
                - Policy distribution sequences.
                - Next actor hidden state (num_layers * D, batch_size, H_actor).
        """
        raise NotImplementedError

    @abstractmethod
    def forward_critic(
        self,
        obs_seq: torch.Tensor,
        action_seq: torch.Tensor, # Action sequences needed for Q-value estimation
        hidden_state: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass for the critic network(s). SAC typically uses two Q-networks.

        Args:
            obs_seq (torch.Tensor): Observation sequences (batch_size, seq_len, *obs_shape).
            action_seq (torch.Tensor): Action sequences (batch_size, seq_len, *action_shape).
            hidden_state (torch.Tensor): Critic's hidden states (num_layers * D, batch_size, H_critic).

        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
                - Q1 value sequences (batch_size, seq_len, 1).
                - Q2 value sequences (batch_size, seq_len, 1).
                - Next critic hidden state (num_layers * D, batch_size, H_critic).
        """
        raise NotImplementedError

    @abstractmethod
    def actor_parameters(self) -> Iterable[torch.nn.Parameter]:
        """Return parameters of the actor network."""
        raise NotImplementedError

    @abstractmethod
    def critic_parameters(self) -> Iterable[torch.nn.Parameter]:
        """Return parameters of the critic network(s)."""
        raise NotImplementedError

;;;

drl/agent/pretrained.py
;;;
from typing import Optional

import torch

from drl.agent.agent import Agent, agent_config
from drl.agent.net import PretrainedRecurrentNetwork
from drl.exp import Experience

@agent_config(name="Pretrained Agent")
class PretrainedRecurrentAgent(Agent):
    def __init__(
        self,
        network: PretrainedRecurrentNetwork,
        num_envs: int,
        device: Optional[str] = None,
    ):
        super().__init__(num_envs, network, device)
        
        self._network = network
        
        hidden_state_shape = (network.hidden_state_shape()[0], self._num_envs, network.hidden_state_shape()[1])
        self._hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._next_hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device)
        
    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        self._hidden_state = self._next_hidden_state * (1.0 - self._prev_terminated)
        policy_dist_seq, next_hidden_state = self._network.forward(
            obs.unsqueeze(dim=1),
            self._hidden_state
        )
        self._next_hidden_state = next_hidden_state
        return policy_dist_seq.sample().squeeze(dim=1)
    
    def update(self, exp: Experience) -> Optional[dict]:
        self._prev_terminated = exp.terminated

    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        return PretrainedRecurrentAgent(self._network, num_envs, device or str(self.device))

;;;

drl/agent/recurrent_ppo.py
;;;
from typing import Dict, Optional

import torch

import drl.rl_loss as L
import drl.util.func as util_f
from drl.agent.agent import Agent, agent_config
from drl.agent.config import RecurrentPPOConfig
from drl.agent.net import RecurrentPPONetwork
from drl.agent.trajectory import RecurrentPPOExperience, RecurrentPPOTrajectory
from drl.exp import Experience
from drl.net import Trainer
from drl.util import IncrementalMean, TruncatedSequenceGenerator


@agent_config(name="Recurrent PPO")
class RecurrentPPO(Agent):
    def __init__(
        self, 
        config: RecurrentPPOConfig,
        network: RecurrentPPONetwork,
        trainer: Trainer,
        num_envs: int,
        device: Optional[str] = None
    ) -> None:
        super().__init__(num_envs, network, device)
        
        self._config = config
        self._network = network
        self._trainer = trainer
        self._trajectory = RecurrentPPOTrajectory(self._config.n_steps)
        
        self._action_log_prob: torch.Tensor = None # type: ignore
        self._state_value: torch.Tensor = None # type: ignore    
        hidden_state_shape = (network.hidden_state_shape()[0], self._num_envs, network.hidden_state_shape()[1])
        self._hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._next_hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device)
                
        # log data
        self._actor_average_loss = IncrementalMean()
        self._critic_average_loss = IncrementalMean()
    
    @property
    def config_dict(self) -> dict:
        return self._config.__dict__
        
    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        # update hidden state H_t
        self._hidden_state = self._next_hidden_state * (1.0 - self._prev_terminated)
        
        # feed forward
        # when interacting with environment, sequence_length must be 1
        # that is (seq_batch_size, seq_len) = (num_envs, 1)
        policy_dist_seq, state_value_seq, next_hidden_state = self._network.forward(
            obs.unsqueeze(dim=1),
            self._hidden_state
        )
        
        # action sampling
        action_seq = policy_dist_seq.sample()
        
        # (num_envs, 1, *shape) -> (num_envs, *shape)
        action = action_seq.squeeze(dim=1)
        self._action_log_prob = policy_dist_seq.log_prob(action_seq).squeeze_(dim=1)
        self._state_value = state_value_seq.squeeze_(dim=1)
        
        self._next_hidden_state = next_hidden_state
        
        return action
    
    def update(self, exp: Experience) -> Optional[dict]:
        self._prev_terminated = exp.terminated
        
        self._trajectory.add(RecurrentPPOExperience(
            **exp.__dict__,
            action_log_prob=self._action_log_prob,
            state_value=self._state_value,
            hidden_state=self._hidden_state,
        ))
        
        if self._trajectory.reached_n_steps:
            self._train()
            
    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        return RecurrentPPOInference(self._network, num_envs, device or str(self.device))
    
    def _train(self):
        exp_batch = self._trajectory.sample()
        # compute advantage and target state value
        advantage, target_state_value = self._compute_adv_target(exp_batch)
        
        # batch (batch_size, *shape) to truncated sequence (seq_batch_size, seq_len, *shape)
        # it is useful to train recurrent network which requires sequence data
        seq_generator = TruncatedSequenceGenerator(
            self._config.seq_len,
            self._num_envs,
            self._config.n_steps,
            self._config.padding_value
        )
        
        def add_to_seq_gen(batch, start_idx = 0, seq_len = 0):
            seq_generator.add(util_f.batch_to_perenv(batch, self._num_envs), start_idx, seq_len)
    
        add_to_seq_gen(exp_batch.hidden_state.swapdims(0, 1), seq_len=1)
        add_to_seq_gen(exp_batch.obs)
        add_to_seq_gen(exp_batch.action)
        add_to_seq_gen(exp_batch.action_log_prob)
        add_to_seq_gen(advantage)
        add_to_seq_gen(target_state_value)
        
        sequences = seq_generator.generate(util_f.batch_to_perenv(exp_batch.terminated, self._num_envs).squeeze_(dim=-1))
        mask, seq_init_hidden_state, obs_seq, action_seq, old_action_log_prob_seq, advantage_seq, target_state_value_seq = sequences

        entire_seq_batch_size = len(mask)
        # (entire_seq_batch_size, 1, D x num_layers, H) -> (D x num_layers, entire_seq_batch_size, H)
        seq_init_hidden_state = seq_init_hidden_state.squeeze_(dim=1).swapdims_(0, 1)
        
        for _ in range(self._config.epoch):
            # shuffle indexes to randomly sample sequence mini-batches
            shuffled_seq = torch.randperm(entire_seq_batch_size)
            for i in range(entire_seq_batch_size // self._config.seq_mini_batch_size):
                # when sliced by sample_seq, (entire_seq_batch_size,) -> (seq_mini_batch_size,)
                sample_seq = shuffled_seq[self._config.seq_mini_batch_size * i : self._config.seq_mini_batch_size * (i + 1)]
                # when masked by sample_mask, (seq_mini_batch_size, seq_len) -> (masked_batch_size,)
                sample_mask = mask[sample_seq]
                
                # feed forward
                sample_policy_dist_seq, sample_state_value_seq, _ = self._network.forward(
                    obs_seq[sample_seq],
                    seq_init_hidden_state[:, sample_seq]
                )
                
                # compute actor loss
                sample_new_action_log_prob_seq = sample_policy_dist_seq.log_prob(action_seq[sample_seq])
                actor_loss = L.ppo_clipped_loss(
                    advantage_seq[sample_seq][sample_mask],
                    old_action_log_prob_seq[sample_seq][sample_mask],
                    sample_new_action_log_prob_seq[sample_mask],
                    self._config.epsilon_clip
                )
                
                # compute critic loss
                critic_loss = L.bellman_value_loss(
                    sample_state_value_seq[sample_mask],
                    target_state_value_seq[sample_seq][sample_mask],
                )
                
                # compute entropy
                entropy = sample_policy_dist_seq.entropy()[sample_mask].mean()
                
                # train step
                loss = actor_loss + self._config.critic_loss_coef * critic_loss - self._config.entropy_coef * entropy
                self._trainer.step(loss, self.training_steps)
                self._tick_training_steps()
                
                # update log data
                self._actor_average_loss.update(actor_loss.item())
                self._critic_average_loss.update(critic_loss.item())
    
    def _compute_adv_target(self, exp_batch: RecurrentPPOExperience):
        """
        Compute advantage `(batch_size, 1)` and target state value `(batch_size, 1)`.
        """
        
        # (num_envs, *obs_shape)
        final_next_obs = exp_batch.next_obs[-self._num_envs:]
        final_next_hidden_state = self._next_hidden_state
        
        with torch.no_grad():
            # compute final next state value
            _, final_next_state_value_seq, _ = self._network.forward(
                final_next_obs.unsqueeze(dim=1), # (num_envs, 1, *obs_shape) because sequence length is 1
                final_next_hidden_state
            )
        
        # (num_envs, 1, 1) -> (num_envs, 1)
        final_next_state_value = final_next_state_value_seq.squeeze_(dim=1)
        # (num_envs x (n_steps + 1), 1)
        entire_state_value = torch.cat((exp_batch.state_value, final_next_state_value), dim=0)
        
        # (num_envs x T, 1) -> (num_envs, T)
        b2e = lambda x: util_f.batch_to_perenv(x, self._num_envs)
        entire_state_value = b2e(entire_state_value).squeeze_(dim=-1)
        reward = b2e(exp_batch.reward).squeeze_(dim=-1)
        terminated = b2e(exp_batch.terminated).squeeze_(dim=-1)
        
        # compute advantage (num_envs, n_steps) using GAE
        advantage = L.gae(
            entire_state_value,
            reward,
            terminated,
            self._config.gamma,
            self._config.lam
        )
        
        # compute target state value (num_envs, n_steps)
        target_state_value = advantage + entire_state_value[:, :-1]
        
        # (num_envs, n_steps) -> (num_envs x n_steps, 1)
        e2b = lambda x: util_f.perenv_to_batch(x).unsqueeze_(dim=-1)
        advantage = e2b(advantage)
        target_state_value = e2b(target_state_value)
        
        return advantage, target_state_value
    
    @property
    def log_data(self) -> Dict[str, tuple]:
        ld = super().log_data
        if self._actor_average_loss.count > 0:
            ld["Training/Actor Loss"] = (self._actor_average_loss.mean, self.training_steps)
            ld["Training/Critic Loss"] = (self._critic_average_loss.mean, self.training_steps)
            self._actor_average_loss.reset()
            self._critic_average_loss.reset()
        return ld

@agent_config(name="Recurrent PPO Inference")
class RecurrentPPOInference(Agent):
    def __init__(self, network: RecurrentPPONetwork, num_envs: int, device: Optional[str] = None) -> None:
        super().__init__(num_envs, network, device)
        
        self._network = network
        
        hidden_state_shape = (network.hidden_state_shape()[0], self._num_envs, network.hidden_state_shape()[1])
        self._hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._next_hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device)
        
    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        self._hidden_state = self._next_hidden_state * (1.0 - self._prev_terminated)
        policy_dist_seq, _, next_hidden_state = self._network.forward(
            obs.unsqueeze(dim=1),
            self._hidden_state
        )
        self._next_hidden_state = next_hidden_state
        return policy_dist_seq.sample().squeeze(dim=1)
    
    def update(self, exp: Experience) -> Optional[dict]:
        self._prev_terminated = exp.terminated

    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        return RecurrentPPOInference(self._network, num_envs, device or str(self.device))

;;;

drl/agent/recurrent_ppo_rnd.py
;;;
from typing import Dict, Optional

import torch
import torch.nn.functional as F
import numpy as np

import drl.rl_loss as L
import drl.util.func as util_f
from drl.agent.agent import Agent, agent_config
from drl.agent.config import RecurrentPPORNDConfig
from drl.agent.net import RecurrentPPORNDNetwork
from drl.agent.trajectory import (RecurrentPPORNDExperience,
                                  RecurrentPPORNDTrajectory)
from drl.exp import Experience
from drl.net import Network, Trainer
from drl.util import (IncrementalMean, IncrementalMeanVarianceFromBatch,
                      TruncatedSequenceGenerator)

@agent_config("Recurrent PPO RND")
class RecurrentPPORND(Agent):
    def __init__(
        self, 
        config: RecurrentPPORNDConfig,
        network: RecurrentPPORNDNetwork,
        trainer: Trainer,
        num_envs: int,
        device: Optional[str] = None
    ) -> None:
        super().__init__(num_envs, network, device)
        
        self._config = config
        self._network = network
        self._trainer = trainer
        self._trajectory = RecurrentPPORNDTrajectory(self._config.n_steps)
        
        self._action_log_prob: torch.Tensor = None # type: ignore
        self._epi_state_value: torch.Tensor = None # type: ignore
        self._nonepi_state_value: torch.Tensor = None # type: ignore
        self._prev_discounted_rnd_int_return = 0.0
        self._current_init_norm_steps = 0
        # compute normalization parameters of non-episodic reward of each env along time steps
        self._rnd_int_return_mean_var = IncrementalMeanVarianceFromBatch(dim=1)
        # compute normalization parameters of each feature of next observation along batches
        self._obs_feature_mean_var = IncrementalMeanVarianceFromBatch(dim=0)
        # compute normalization parameters of each feature of next hidden state along batches
        self._hidden_state_feature_mean_var = IncrementalMeanVarianceFromBatch(dim=0)
        hidden_state_shape = (network.hidden_state_shape()[0], self._num_envs, network.hidden_state_shape()[1])
        self._hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._next_hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device)
                
        # log data
        self._actor_avg_loss = IncrementalMean()
        self._epi_critic_avg_loss = IncrementalMean()
        self._nonepi_critic_avg_loss = IncrementalMean()
        self._rnd_avg_loss = IncrementalMean()
        self._avg_rnd_int_reward = IncrementalMean()
        self._episodes = np.zeros((self.num_envs, 1), dtype=np.int32)
        self._rnd_int_rewards = []
        self._avg_rnd_int_rewards = [IncrementalMeanVarianceFromBatch() for _ in range(self.num_envs)]
        
        self._time_steps = 0
    
    @property
    def config_dict(self) -> dict:
        return self._config.__dict__
    
    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        # update hidden state H_t
        self._hidden_state = self._next_hidden_state * (1.0 - self._prev_terminated)
        
        # feed forward
        # when interacting with environment, sequence_length must be 1
        # that is (seq_batch_size, seq_len) = (num_envs, 1)
        policy_dist_seq, epi_state_value_seq, nonepi_state_value_seq, next_hidden_state = self._network.forward_actor_critic(
            obs.unsqueeze(dim=1),
            self._hidden_state
        )
        
        # action sampling
        action_seq = policy_dist_seq.sample()
        
        # (num_envs, 1, *shape) -> (num_envs, *shape)
        action = action_seq.squeeze(dim=1)
        self._action_log_prob = policy_dist_seq.log_prob(action_seq).squeeze_(dim=1)
        self._epi_state_value = epi_state_value_seq.squeeze_(dim=1)
        self._nonepi_state_value = nonepi_state_value_seq.squeeze_(dim=1)
        
        self._next_hidden_state = next_hidden_state
        
        return action
    
    def update(self, exp: Experience) -> Optional[dict]:
        self._time_steps += 1
        self._prev_terminated = exp.terminated
        
        # (D x num_layers, num_envs, H) -> (num_envs, D x num_layers, H)
        next_hidden_state_along_envs = self._next_hidden_state.swapdims(0, 1)
        
        # initialize normalization parameters
        if (self._config.init_norm_steps is not None) and (self._current_init_norm_steps < self._config.init_norm_steps):
            self._current_init_norm_steps += 1
            self._obs_feature_mean_var.update(exp.next_obs)
            self._hidden_state_feature_mean_var.update(next_hidden_state_along_envs)
            return
        
        # compute RND intrinsic reward
        normalized_next_obs = self._normalize_obs(exp.next_obs.to(device=self.device))
        normalized_next_hidden_state = self._normalize_hidden_state(next_hidden_state_along_envs)
        rnd_int_reward = self._compute_rnd_int_reward(normalized_next_obs, normalized_next_hidden_state)
        self._rnd_int_rewards.append(rnd_int_reward.detach().cpu().numpy())
        
        # add an experience
        self._trajectory.add(RecurrentPPORNDExperience(
            **exp.__dict__,
            rnd_int_reward=rnd_int_reward,
            action_log_prob=self._action_log_prob,
            epi_state_value=self._epi_state_value,
            nonepi_state_value=self._nonepi_state_value,
            hidden_state=self._hidden_state,
            next_hidden_state=self._next_hidden_state
        ))
        
        if self._trajectory.reached_n_steps:
            metric_info_dicts = self._train()
            info_dict = {"metric": metric_info_dicts}
            return info_dict
    
    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        return RecurrentPPORNDInference(self._network, num_envs, device or str(self.device))
    
    def _train(self):
        exp_batch = self._trajectory.sample()
        # compute advantage, extrinsic and intrinsic target state value
        advantage, epi_target_state_value, nonepi_target_state_value, metric_info_dicts = self._compute_adv_target(exp_batch)
        
        # batch (batch_size, *shape) to truncated sequence (seq_batch_size, seq_len, *shape)
        seq_generator = TruncatedSequenceGenerator(
            self._config.seq_len,
            self._num_envs,
            self._config.n_steps,
            self._config.padding_value
        )
        
        def add_to_seq_gen(batch, start_idx = 0, seq_len = 0):
            seq_generator.add(util_f.batch_to_perenv(batch, self._num_envs), start_idx, seq_len)
    
        add_to_seq_gen(exp_batch.hidden_state.swapdims(0, 1), seq_len=1)
        add_to_seq_gen(exp_batch.next_hidden_state.swapdims(0, 1))
        add_to_seq_gen(exp_batch.obs)
        add_to_seq_gen(exp_batch.next_obs)
        add_to_seq_gen(exp_batch.action)
        add_to_seq_gen(exp_batch.action_log_prob)
        add_to_seq_gen(advantage)
        add_to_seq_gen(epi_target_state_value)
        add_to_seq_gen(nonepi_target_state_value)
        
        sequences = seq_generator.generate(util_f.batch_to_perenv(exp_batch.terminated, self._num_envs).squeeze_(dim=-1))
        (mask, seq_init_hidden_state, next_hidden_state_seq, obs_seq, next_obs_seq, action_seq, old_action_log_prob_seq, 
         advantage_seq, epi_target_state_value_seq, nonepi_target_state_value_seq) = sequences

        entire_seq_batch_size = len(mask)
        # (entire_seq_batch_size, 1, D x num_layers, H) -> (D x num_layers, entire_seq_batch_size, H)
        seq_init_hidden_state = seq_init_hidden_state.squeeze_(dim=1).swapdims_(0, 1)
        
        # update the normalization parameters of the observation and the hidden state
        # when masked by mask, (entire_seq_batch_size, seq_len,) -> (masked_batch_size,)
        masked_next_obs = next_obs_seq[mask]
        masked_next_hidden_state = next_hidden_state_seq[mask]
        self._obs_feature_mean_var.update(masked_next_obs)
        self._hidden_state_feature_mean_var.update(masked_next_hidden_state)
        
        # normalize the observation and the hidden state
        normalized_next_obs_seq = next_obs_seq
        normalized_hidden_state_seq = next_hidden_state_seq
        normalized_next_obs_seq[mask] = self._normalize_obs(masked_next_obs)
        normalized_hidden_state_seq[mask] = self._normalize_hidden_state(masked_next_hidden_state)
        
        for _ in range(self._config.epoch):
            # if seq_mini_batch_size is None, use the entire sequence batch to executes iteration only once
            # otherwise, use the randomly shuffled mini batch to executes iteration multiple times
            if self._config.seq_mini_batch_size is None:
                shuffled_seq = torch.arange(entire_seq_batch_size)
                seq_mini_batch_size = entire_seq_batch_size
            else:
                shuffled_seq = torch.randperm(entire_seq_batch_size)
                seq_mini_batch_size = self._config.seq_mini_batch_size
                
            for i in range(entire_seq_batch_size // seq_mini_batch_size):
                # when sliced by sample_seq, (entire_seq_batch_size,) -> (seq_mini_batch_size,)
                sample_seq = shuffled_seq[seq_mini_batch_size * i : seq_mini_batch_size * (i + 1)]
                # when masked by sample_mask, (seq_mini_batch_size, seq_len) -> (masked_batch_size,)
                sample_mask = mask[sample_seq]
                
                # feed forward
                sample_policy_dist_seq, sample_epi_state_value_seq, sample_nonepi_state_value_seq, _ = self._network.forward_actor_critic(
                    obs_seq[sample_seq],
                    seq_init_hidden_state[:, sample_seq]
                )
                predicted_feature, target_feature = self._network.forward_rnd(
                    normalized_next_obs_seq[sample_seq][sample_mask],
                    normalized_hidden_state_seq[sample_seq][sample_mask].flatten(1, 2)
                )
                
                # compute actor loss
                sample_new_action_log_prob_seq = sample_policy_dist_seq.log_prob(action_seq[sample_seq])
                actor_loss = L.ppo_clipped_loss(
                    advantage_seq[sample_seq][sample_mask],
                    old_action_log_prob_seq[sample_seq][sample_mask],
                    sample_new_action_log_prob_seq[sample_mask],
                    self._config.epsilon_clip
                )
                
                # compute critic loss
                epi_critic_loss = L.bellman_value_loss(
                    sample_epi_state_value_seq[sample_mask],
                    epi_target_state_value_seq[sample_seq][sample_mask],
                )
                nonepi_critic_loss = L.bellman_value_loss(
                    sample_nonepi_state_value_seq[sample_mask],
                    nonepi_target_state_value_seq[sample_seq][sample_mask],
                )
                critic_loss = epi_critic_loss + nonepi_critic_loss
                
                # compute entropy
                entropy = sample_policy_dist_seq.entropy()[sample_mask].mean()
                
                # compute RND loss
                rnd_loss = L.rnd_loss(
                    predicted_feature,
                    target_feature,
                    self._config.rnd_pred_exp_proportion
                )
                
                # train step
                loss = actor_loss + self._config.critic_loss_coef * critic_loss - self._config.entropy_coef * entropy + rnd_loss
                self._trainer.step(loss, self.training_steps)
                self._tick_training_steps()
                
                # update log data
                self._actor_avg_loss.update(actor_loss.item())
                self._epi_critic_avg_loss.update(epi_critic_loss.item())
                self._nonepi_critic_avg_loss.update(nonepi_critic_loss.item())
                self._rnd_avg_loss.update(rnd_loss.item())
                
        return metric_info_dicts
    
    def _compute_adv_target(self, exp_batch: RecurrentPPORNDExperience):
        """
        Compute advantage `(batch_size, 1)`, episodic and non-episodic target state value `(batch_size, 1)`.
        """
        # (num_envs, *obs_shape)
        final_next_obs = exp_batch.next_obs[-self._num_envs:]
        final_next_hidden_state = self._next_hidden_state
        
        with torch.no_grad():
            # compute final next state value
            _, final_epi_next_state_value_seq, final_nonepi_next_state_value_seq, _ = self._network.forward_actor_critic(
                final_next_obs.unsqueeze(dim=1), # (num_envs, 1, *obs_shape) because sequence length is 1
                final_next_hidden_state
            )
        
        # (num_envs, 1, 1) -> (num_envs, 1)
        final_epi_next_state_value = final_epi_next_state_value_seq.squeeze_(dim=1)
        final_nonepi_next_state_value = final_nonepi_next_state_value_seq.squeeze_(dim=1)
        # (num_envs x (n_steps + 1), 1)
        entire_epi_state_value = torch.cat((exp_batch.epi_state_value, final_epi_next_state_value), dim=0)
        entire_nonepi_state_value = torch.cat((exp_batch.nonepi_state_value, final_nonepi_next_state_value), dim=0)
        
        # (num_envs x T, 1) -> (num_envs, T)
        b2e = lambda x: util_f.batch_to_perenv(x, self._num_envs)
        entire_epi_state_value = b2e(entire_epi_state_value).squeeze_(dim=-1)
        entire_nonepi_state_value = b2e(entire_nonepi_state_value).squeeze_(dim=-1)
        reward = b2e(exp_batch.reward).squeeze_(dim=-1)
        rnd_int_reward = b2e(exp_batch.rnd_int_reward).squeeze_(dim=-1)
        terminated = b2e(exp_batch.terminated).squeeze_(dim=-1)
        
        # compute discounted RND intrinsic returns
        discounted_rnd_int_return = torch.empty_like(rnd_int_reward)
        for t in range(self._config.n_steps):
            self._prev_discounted_rnd_int_return = rnd_int_reward[:, t] + self._config.gamma_n * self._prev_discounted_rnd_int_return
            discounted_rnd_int_return[:, t] = self._prev_discounted_rnd_int_return
        
        # update RND intrinsic reward normalization parameters
        self._rnd_int_return_mean_var.update(discounted_rnd_int_return)
        
        # normalize RND intrinsic rewards
        rnd_int_reward /= torch.sqrt(self._rnd_int_return_mean_var.variance).unsqueeze(dim=-1) + 1e-8
        self._avg_rnd_int_reward.update(rnd_int_reward.mean().item())
        
        metric_info_dicts = []
        for env_id in range(self.num_envs):
            end_idxes = torch.where(terminated[env_id])[0].cpu() + 1
            start = 0
            for end in end_idxes:
                mean, _ = self._avg_rnd_int_rewards[env_id].update(rnd_int_reward[env_id, start:end].cpu())
                metric_info_dicts.append({
                    "episode_metric": {
                        "keys": {
                            "episode": self._episodes[env_id].item(),
                            "env_id": env_id
                        },
                        "values": {
                            "avg_rnd_int_reward": mean.item()
                        }
                    }
                })
                self._episodes[env_id] += 1
                self._avg_rnd_int_rewards[env_id].reset()
                start = end
            if start < len(rnd_int_reward[env_id]):
                self._avg_rnd_int_rewards[env_id].update(rnd_int_reward[env_id, start:].cpu())
        
        # compute advantage (num_envs, n_steps) using GAE
        epi_advantage = L.gae(
            entire_epi_state_value,
            reward,
            terminated,
            self._config.gamma,
            self._config.lam
        )
        nonepi_advantage = L.gae(
            entire_nonepi_state_value,
            rnd_int_reward, # RND intrinsic reward is non-episodic stream
            torch.zeros_like(terminated), # non-episodic
            self._config.gamma_n,
            self._config.lam
        )
        advantage = epi_advantage + self._config.nonepi_adv_coef * nonepi_advantage
        
        # compute target state value (num_envs, n_steps)
        epi_target_state_value = epi_advantage + entire_epi_state_value[:, :-1]
        nonepi_target_state_value = nonepi_advantage + entire_nonepi_state_value[:, :-1]
        
        # (num_envs, n_steps) -> (num_envs x n_steps, 1)
        e2b = lambda x: util_f.perenv_to_batch(x).unsqueeze_(dim=-1)
        advantage = e2b(advantage)
        epi_target_state_value = e2b(epi_target_state_value)
        nonepi_target_state_value = e2b(nonepi_target_state_value)
        
        return advantage, epi_target_state_value, nonepi_target_state_value, metric_info_dicts
    
    def _compute_rnd_int_reward(self, obs: torch.Tensor, hidden_state: torch.Tensor) -> torch.Tensor:
        """
        Compute intrinsic reward.

        Args:
            obs (Tensor): `(batch_size, *obs_shape)`
            hidden_state (Tensor): `(batch_size, D x num_layers, H)`

        Returns:
            int_reward (Tensor): intrinsic reward `(batch_size, 1)`
        """
        with torch.no_grad():
            predicted_feature, target_feature = self._network.forward_rnd(obs, hidden_state.flatten(1, 2))
            int_reward = 0.5 * ((target_feature - predicted_feature)**2).sum(dim=1, keepdim=True)
            return int_reward
    
    def _normalize_obs(self, obs: torch.Tensor) -> torch.Tensor:
        if self._config.init_norm_steps is None:
            return obs
        obs_feature_mean = self._obs_feature_mean_var.mean
        obs_feature_std = torch.sqrt(self._obs_feature_mean_var.variance) + 1e-8
        normalized_obs = (obs - obs_feature_mean) / obs_feature_std
        return normalized_obs.clamp(self._config.obs_norm_clip_range[0], self._config.obs_norm_clip_range[1])

    def _normalize_hidden_state(self, hidden_state: torch.Tensor) -> torch.Tensor:
        if self._config.init_norm_steps is None:
            return hidden_state
        hidden_state_feature_mean = self._hidden_state_feature_mean_var.mean
        hidden_state_feature_std = torch.sqrt(self._hidden_state_feature_mean_var.variance) + 1e-8
        normalized_hidden_state = (hidden_state - hidden_state_feature_mean) / hidden_state_feature_std
        return normalized_hidden_state.clamp(self._config.hidden_state_norm_clip_range[0], self._config.hidden_state_norm_clip_range[1])
    
    @property
    def log_data(self) -> Dict[str, tuple]:
        ld = super().log_data
        if self._actor_avg_loss.count > 0:
            ld["Training/Actor Loss"] = (self._actor_avg_loss.mean, self.training_steps)
            ld["Training/Extrinsic Critic Loss"] = (self._epi_critic_avg_loss.mean, self.training_steps)
            ld["Training/Intrinsic Critic Loss"] = (self._nonepi_critic_avg_loss.mean, self.training_steps)
            ld["Training/RND Loss"] = (self._rnd_avg_loss.mean, self.training_steps)
            self._actor_avg_loss.reset()
            self._epi_critic_avg_loss.reset()
            self._nonepi_critic_avg_loss.reset()
            self._rnd_avg_loss.reset()
        return ld

class RecurrentPPORNDInference(Agent):
    def __init__(self, network: RecurrentPPORNDNetwork, num_envs: int, device: Optional[str] = None) -> None:
        super().__init__(num_envs, network, device)
        
        self._network = network
        
        hidden_state_shape = (network.hidden_state_shape()[0], self._num_envs, network.hidden_state_shape()[1])
        self._hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._next_hidden_state = torch.zeros(hidden_state_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device)
        
    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        self._hidden_state = self._next_hidden_state * (1.0 - self._prev_terminated)
        policy_dist_seq, _, _, next_hidden_state = self._network.forward_actor_critic(
            obs.unsqueeze(dim=1),
            self._hidden_state
        )
        self._next_hidden_state = next_hidden_state
        return policy_dist_seq.sample().squeeze(dim=1)
    
    def update(self, exp: Experience) -> Optional[dict]:
        self._prev_terminated = exp.terminated

    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        return RecurrentPPORNDInference(self._network, num_envs, device or str(self.device))

;;;

drl/agent/recurrent_sac.py
;;;
# drl/agent/recurrent_sac.py
from typing import Dict, Optional, Tuple, Iterable

import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
import drl.util.func as util_f
from drl.agent.agent import Agent, agent_config
from drl.agent.config import RecurrentSACConfig
# Import the INTERFACE and the specific IMPLEMENTATION
from drl.agent.net import RecurrentSACNetwork
from train.net import SelfiesRecurrentSACNet # Import the concrete network class
from drl.agent.trajectory import ReplayBuffer # Import ReplayBuffer
from drl.exp import Experience
from drl.net import Network, Trainer # Keep Trainer if used for optimizers
from drl.util import IncrementalMean

@agent_config("Recurrent SAC")
class RecurrentSAC(Agent):
    def __init__(
        self,
        config: RecurrentSACConfig,
        network: SelfiesRecurrentSACNet, # Expect the concrete implementation
        # trainer: Trainer, # SAC manages its own optimizers
        num_envs: int,
        obs_shape: Tuple, # Pass obs_shape explicitly
        action_shape: Tuple, # Pass action_shape explicitly (usually (1,) for discrete)
        num_actions: int, # Pass num_actions explicitly for target entropy
        device: Optional[str] = None
    ) -> None:
        # Initialize Agent base class with the network
        # The network passed in should already be the concrete SelfiesRecurrentSACNet
        super().__init__(num_envs, network, device)

        self._config = config
        self._network = network # This is the main (online) network instance
        self._num_actions = num_actions

        # --- Initialize Actor Critic Networks ---
        # These are part of the main network instance
        self._actor = self._network
        self._critic = self._network

        # --- Target Networks ---
        # Create target networks by copying the structure and loading state dict
        # Use the concrete class SelfiesRecurrentSACNet for instantiation
        self._critic_target = SelfiesRecurrentSACNet( # Use concrete class
            # Pass necessary args for network initialization
             obs_shape=obs_shape[0], # Get vocab size from obs_shape
             num_actions=num_actions,
             # Get other params from the main network instance for consistency
             hidden_dim=network._hidden_dim,
             n_recurrent_layers=network._n_recurrent_layers,
             # Temperature is usually associated with the policy, not critic target
             # temperature=network._actor_head._temperature # If needed, but unlikely
        ).to(self.device) # Move target network to the correct device

        self._critic_target.load_state_dict(self._critic.state_dict())
        # Freeze target network parameters
        for param in self._critic_target.parameters():
            param.requires_grad = False

        # --- Optimizers ---
        # Ensure parameters() method exists and returns correct parameters
        self._actor_optimizer = optim.Adam(self._network.actor_parameters(), lr=config.actor_lr)
        self._critic_optimizer = optim.Adam(self._network.critic_parameters(), lr=config.critic_lr)

        # --- Entropy Coefficient (Alpha) ---
        self._learn_alpha = config.learn_alpha
        if self._learn_alpha:
            # Target entropy: H = -|A| * ratio
            # For discrete actions, max entropy is log(|A|)
            # Target entropy is typically negative, e.g., -dim(A) * ratio
            # Use num_actions directly for discrete case
            max_entropy = np.log(num_actions) if num_actions > 0 else 1.0 # Avoid log(0)
            self._target_entropy = -max_entropy * config.target_entropy_ratio

            self._log_alpha = torch.tensor(np.log(config.alpha), dtype=torch.float32, requires_grad=True, device=self.device)
            self._alpha_optimizer = optim.Adam([self._log_alpha], lr=config.alpha_lr)
            self._alpha = self._log_alpha.exp().item() # Keep track of current alpha value
        else:
            self._log_alpha = torch.tensor(np.log(config.alpha), dtype=torch.float32, device=self.device)
            self._alpha = config.alpha
            self._alpha_optimizer = None
            self._target_entropy = 0.0 # Not used if alpha is fixed

        # --- Replay Buffer ---
        # Get hidden state shapes from the network instance
        actor_hidden_shape_tuple = self._network.hidden_state_shape() # e.g., (layers*D, H_lstm)
        critic_hidden_shape_tuple = self._network.hidden_state_shape() # Assuming same for critic for now

        self._replay_buffer = ReplayBuffer(
            buffer_size=config.buffer_size,
            num_envs=num_envs,
            obs_shape=obs_shape,
            action_shape=action_shape, # Should be (1,) for discrete index
            actor_hidden_shape=actor_hidden_shape_tuple, # Pass the tuple
            critic_hidden_shape=critic_hidden_shape_tuple, # Pass the tuple
            device=self.device
        )

        # --- Hidden States Tracking ---
        # Shape: (num_envs, num_layers*D, H_lstm) - Batch dimension first
        self._current_actor_hidden_state = torch.zeros((num_envs,) + actor_hidden_shape_tuple, device=self.device)
        self._current_critic_hidden_state = torch.zeros((num_envs,) + critic_hidden_shape_tuple, device=self.device)
        self._next_actor_hidden_state = torch.zeros((num_envs,) + actor_hidden_shape_tuple, device=self.device)
        self._next_critic_hidden_state = torch.zeros((num_envs,) + critic_hidden_shape_tuple, device=self.device)

        # Shape: (num_envs, 1)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device, dtype=torch.float32)

        # --- Logging ---
        self._actor_avg_loss = IncrementalMean()
        self._critic_avg_loss = IncrementalMean()
        self._alpha_avg_loss = IncrementalMean()
        self._alpha_value_log = IncrementalMean() # To log the actual alpha value

    @property
    def config_dict(self) -> dict:
        # Return a dictionary representation of the config
        cfg = self._config.__dict__.copy()
        # Add runtime info if needed
        cfg['current_alpha'] = self._alpha
        return cfg

    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        """ Select action greedily or sample from policy """
        # Reset hidden state if previous step was terminal
        # Hidden state shape is (num_envs, num_layers*D, H)
        # Need to reshape for LSTM: (num_layers*D, num_envs, H)
        # Termination shape: (num_envs, 1) -> broadcast to hidden state shape
        term_mask = (1.0 - self._prev_terminated).unsqueeze(-1) # Shape (num_envs, 1, 1)

        actor_h_batch_first = self._next_actor_hidden_state * term_mask
        critic_h_batch_first = self._next_critic_hidden_state * term_mask # Track critic state too

        # Permute to (num_layers*D, num_envs, H) for LSTM input
        actor_h_lstm = actor_h_batch_first.permute(1, 0, 2).contiguous()
        critic_h_lstm = critic_h_batch_first.permute(1, 0, 2).contiguous()


        # Store current hidden states (before this action) for the buffer
        # Store in (batch, layers*D, H) format
        self._current_actor_hidden_state = actor_h_batch_first
        self._current_critic_hidden_state = critic_h_batch_first

        # Add sequence dimension (seq_len=1)
        obs_seq = obs.unsqueeze(1)

        # Actor forward pass requires hidden state in LSTM format
        policy_dist, next_actor_h_lstm = self._network.forward_actor(obs_seq, actor_h_lstm)

        # Sample action from the policy distribution
        action = policy_dist.sample() # Shape: (batch_size, seq_len=1, action_dim=1)

        # Critic forward pass to get next hidden state (needed for next step)
        # Action needs to be LongTensor for embedding lookup in critic
        action_long = action.long()
        _, _, next_critic_h_lstm = self._network.forward_critic(obs_seq, action_long, critic_h_lstm)

        # Store next hidden states (resulting from this action) for the next step
        # Permute back to (batch, layers*D, H)
        self._next_actor_hidden_state = next_actor_h_lstm.permute(1, 0, 2).contiguous()
        self._next_critic_hidden_state = next_critic_h_lstm.permute(1, 0, 2).contiguous()

        # Remove sequence dimension from action
        return action.squeeze(1) # Return shape (batch_size, 1)

    def update(self, exp: Experience) -> Optional[dict]:
        """ Add experience to buffer and perform gradient updates """
        # Store previous terminated state for hidden state resetting in select_action
        self._prev_terminated = exp.terminated.clone() # Shape (num_envs, 1)

        # Add experience to replay buffer (use numpy arrays)
        # Ensure hidden states stored are the ones *before* the action was taken
        self._replay_buffer.add(
            exp.obs.cpu().numpy(),
            exp.action.cpu().numpy(), # Shape (num_envs, 1)
            exp.reward.cpu().numpy().squeeze(-1), # Shape (num_envs,)
            exp.next_obs.cpu().numpy(),
            exp.terminated.cpu().numpy().squeeze(-1), # Shape (num_envs,)
            # Pass hidden states in (num_envs, layers*D, H) format
            self._current_actor_hidden_state.cpu().numpy(),
            self._current_critic_hidden_state.cpu().numpy()
        )

        # Perform gradient steps if buffer is large enough
        if len(self._replay_buffer) < self._config.learning_starts:
            return None

        metrics = {}
        for _ in range(self._config.gradient_steps):
            # Sample a batch from the replay buffer
            batch = self._replay_buffer.sample(self._config.batch_size)
            step_metrics = self._learn(batch)
            # Aggregate metrics if needed (e.g., average over gradient steps)
            metrics.update(step_metrics) # Simplest: just keep last step's metrics

        # Log aggregated metrics
        log_data = {}
        if metrics:
             log_data["Training/Critic Loss"] = (self._critic_avg_loss.mean, self.training_steps)
             log_data["Training/Actor Loss"] = (self._actor_avg_loss.mean, self.training_steps)
             if self._learn_alpha:
                 log_data["Training/Alpha Loss"] = (self._alpha_avg_loss.mean, self.training_steps)
             log_data["Training/Alpha"] = (self._alpha_value_log.mean, self.training_steps)

             # Reset incremental means after logging
             self._critic_avg_loss.reset()
             self._actor_avg_loss.reset()
             self._alpha_avg_loss.reset()
             self._alpha_value_log.reset()

        return {"metric": log_data} if log_data else None


    def _learn(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """ Perform one gradient update step """
        obs = batch["obs"]
        actions = batch["actions"] # Shape (batch_size, 1)
        rewards = batch["rewards"] # Shape (batch_size, 1)
        next_obs = batch["next_obs"]
        terminated = batch["terminated"] # Shape (batch_size, 1)
        # Hidden states from buffer are for the *start* of the transition (obs)
        # Shape: (batch_size, num_layers*D, H)
        actor_h_batch = batch["actor_hidden_states"]
        critic_h_batch = batch["critic_hidden_states"]

        # Permute hidden states to LSTM format: (num_layers*D, batch_size, H)
        actor_h_lstm = actor_h_batch.permute(1, 0, 2).contiguous()
        critic_h_lstm = critic_h_batch.permute(1, 0, 2).contiguous()

        # Add sequence dimension (seq_len=1) for network inputs
        obs_seq = obs.unsqueeze(1)
        next_obs_seq = next_obs.unsqueeze(1)
        # Unsqueeze actions to add sequence dimension: (batch_size, 1) -> (batch_size, 1, 1)
        actions_seq = actions.unsqueeze(1) # Add sequence dimension

        # --- Critic Update ---
        with torch.no_grad():
            # Get next action and log prob from *current* policy using next_obs
            # We need the hidden state that *results* from processing next_obs
            # First, get the next hidden state from the actor using the initial hidden state from the buffer
            _, next_actor_h_lstm = self._actor.forward_actor(next_obs_seq, actor_h_lstm)
            # Then, use this resulting hidden state to get the policy distribution for next_obs
            next_policy_dist, _ = self._actor.forward_actor(next_obs_seq, next_actor_h_lstm)

            next_actions_pi = next_policy_dist.sample() # Shape (batch, 1, 1)
            next_log_prob = next_policy_dist.log_prob(next_actions_pi) # Shape (batch, 1, 1)

            # Get Q values from target critic network for next_obs and next_actions_pi
            # We need the hidden state that *results* from processing next_obs with the critic
            _, _, next_critic_h_lstm = self._critic_target.forward_critic(next_obs_seq, next_actions_pi.long(), critic_h_lstm)
            # Use this resulting hidden state to get the target Q values
            q1_next_target, q2_next_target, _ = self._critic_target.forward_critic(next_obs_seq, next_actions_pi.long(), next_critic_h_lstm)
            # Shape: (batch, 1, 1)

            # Remove sequence dimension
            q1_next_target = q1_next_target.squeeze(1) # Shape (batch, 1)
            q2_next_target = q2_next_target.squeeze(1) # Shape (batch, 1)
            next_log_prob = next_log_prob.squeeze(1)   # Shape (batch, 1)

            min_q_next_target = torch.min(q1_next_target, q2_next_target)
            # TD target value
            next_q_value = min_q_next_target - self._alpha * next_log_prob
            target_q = rewards + (1.0 - terminated) * self._config.gamma * next_q_value # Shape (batch, 1)

        # Get current Q estimates using obs and actions from buffer
        # Use the initial hidden state from the buffer
        # Pass actions_seq which now has shape (batch_size, 1, 1)
        current_q1, current_q2, _ = self._critic.forward_critic(obs_seq, actions_seq.long(), critic_h_lstm)
        # Shape: (batch, 1, 1)

        # Remove sequence dimension
        current_q1 = current_q1.squeeze(1) # Shape (batch, 1)
        current_q2 = current_q2.squeeze(1) # Shape (batch, 1)

        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        # Optimize the critic
        self._critic_optimizer.zero_grad()
        critic_loss.backward()
        # Optional: Gradient clipping for critic
        # torch.nn.utils.clip_grad_norm_(self._network.critic_parameters(), max_norm=...)
        self._critic_optimizer.step()

        # --- Actor Update ---
        # <<<< REMOVE CRITIC FREEZING >>>>
        # for param in self._critic.parameters():
        #     param.requires_grad = False

        # Get actions and log probs from current policy using obs
        # Use the initial hidden state from the buffer
        policy_dist, _ = self._actor.forward_actor(obs_seq, actor_h_lstm)
        actions_pi = policy_dist.sample() # Shape (batch, 1, 1)
        log_prob = policy_dist.log_prob(actions_pi) # Shape (batch, 1, 1)

        # Get Q values for the policy's actions using obs
        # Use the initial hidden state from the buffer
        # Pass actions_pi which has shape (batch_size, 1, 1)
        # Detach actions_pi before passing to critic to prevent gradients flowing through action
        q1_pi, q2_pi, _ = self._critic.forward_critic(obs_seq, actions_pi.long().detach(), critic_h_lstm)
        # Shape: (batch, 1, 1)

        # Remove sequence dimension
        q1_pi = q1_pi.squeeze(1) # Shape (batch, 1)
        q2_pi = q2_pi.squeeze(1) # Shape (batch, 1)
        log_prob = log_prob.squeeze(1) # Shape (batch, 1)

        min_q_pi = torch.min(q1_pi, q2_pi)
        # Detach min_q_pi to ensure loss gradient only comes from log_prob term w.r.t actor params
        actor_loss = (self._alpha * log_prob - min_q_pi.detach()).mean() # <<< DETACH Q VALUE

        # Optimize the actor
        self._actor_optimizer.zero_grad()
        actor_loss.backward() # <<< Error occurred here previously
        # Optional: Gradient clipping for actor
        # torch.nn.utils.clip_grad_norm_(self._network.actor_parameters(), max_norm=...)
        self._actor_optimizer.step()

        # <<<< REMOVE CRITIC UNFREEZING >>>>
        # for param in self._critic.parameters():
        #     param.requires_grad = True

        # --- Alpha Update (Optional) ---
        alpha_loss_val = 0.0
        if self._learn_alpha and self._alpha_optimizer:
            # Use the log_prob calculated for the actor update
            # Detach log_prob here as alpha loss shouldn't affect policy parameters
            alpha_loss = - (self._log_alpha * (log_prob + self._target_entropy).detach()).mean()

            self._alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self._alpha_optimizer.step()
            # Clamp log_alpha to avoid extreme values if necessary
            # self._log_alpha.data.clamp_(min_log_alpha, max_log_alpha)
            self._alpha = self._log_alpha.exp().item() # Update tracked alpha value
            alpha_loss_val = alpha_loss.item()
            self._alpha_avg_loss.update(alpha_loss_val)


        # --- Target Network Update ---
        # Note: training_steps should be incremented *after* the check
        if self.training_steps % self._config.target_update_interval == 0:
            self._polyak_update(self._critic, self._critic_target, self._config.tau)

        # --- Logging & Step Increment ---
        self._tick_training_steps() # Increment training steps *after* potential target update
        self._critic_avg_loss.update(critic_loss.item())
        self._actor_avg_loss.update(actor_loss.item())
        self._alpha_value_log.update(self._alpha)

        return {
            "critic_loss": critic_loss.item(),
            "actor_loss": actor_loss.item(),
            "alpha_loss": alpha_loss_val,
            "alpha": self._alpha,
        }

    def _polyak_update(self, source_net: nn.Module, target_net: nn.Module, tau: float) -> None:
        """ Soft update target network parameters """
        with torch.no_grad():
            for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):
                target_param.data.mul_(1.0 - tau)
                torch.add(target_param.data, source_param.data, alpha=tau, out=target_param.data)

    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        # Create an inference-specific agent instance
        from .recurrent_sac_inference import RecurrentSACInference # Avoid circular import
        inference_device = device or str(self.device)
        # Pass the *main* network instance (which contains the actor logic)
        return RecurrentSACInference(self._network, num_envs, inference_device)

    @property
    def log_data(self) -> Dict[str, tuple]:
        """ Returns log data and resets incremental means """
        # This is handled within the update method now to ensure
        # logs are generated only when updates happen.
        return {} # Return empty dict as logging is done in update

    @property
    def state_dict(self) -> dict:
        """Returns the state dict of the agent."""
        sd = super().state_dict # Get base state_dict (training_steps, model)
        sd.update({
            'critic_target_state_dict': self._critic_target.state_dict(),
            'actor_optimizer_state_dict': self._actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self._critic_optimizer.state_dict(),
        })
        if self._learn_alpha and self._alpha_optimizer:
            sd['log_alpha_state_dict'] = self._log_alpha.detach().cpu() # Save tensor value on CPU
            sd['alpha_optimizer_state_dict'] = self._alpha_optimizer.state_dict()
        return sd

    def load_state_dict(self, state_dict: dict):
        """Load the state dict."""
        super().load_state_dict(state_dict) # Load base state_dict (training_steps, model)
        self._critic_target.load_state_dict(state_dict['critic_target_state_dict'])
        self._actor_optimizer.load_state_dict(state_dict['actor_optimizer_state_dict'])
        self._critic_optimizer.load_state_dict(state_dict['critic_optimizer_state_dict'])
        if self._learn_alpha and self._alpha_optimizer:
             # Load log_alpha tensor and update optimizer state
             # Ensure it's loaded back to the correct device and requires grad
            self._log_alpha.data = state_dict['log_alpha_state_dict'].to(self.device)
            self._log_alpha.requires_grad = True
            self._alpha_optimizer.load_state_dict(state_dict['alpha_optimizer_state_dict'])
            # Re-link log_alpha to the optimizer (important!)
            self._alpha_optimizer.param_groups[0]['params'] = [self._log_alpha]
            self._alpha = self._log_alpha.exp().item() # Update tracked alpha

;;;

drl/agent/recurrent_sac_inference.py
;;;
# drl/agent/recurrent_sac_inference.py
from typing import Optional, Tuple

import torch

from drl.agent.agent import Agent, agent_config
from drl.agent.net import RecurrentSACNetwork # Use the main SAC network interface
from drl.exp import Experience
from drl.net import Network # Base network class

@agent_config(name="Recurrent SAC Inference")
class RecurrentSACInference(Agent):
    """
    Inference agent for Recurrent SAC. Uses the deterministic action (mean).
    """
    def __init__(
        self,
        network: RecurrentSACNetwork, # Expects the full network, will only use actor part
        num_envs: int,
        device: Optional[str] = None
    ) -> None:
        # Initialize Agent base class. Pass the full network,
        # but internally we'll primarily use its actor forward pass.
        super().__init__(num_envs, network, device)
        self._network = network

        # Hidden state tracking for the actor
        actor_hidden_shape = network.hidden_state_shape() # Assuming actor/critic use same shape
        self._actor_hidden_state = torch.zeros((num_envs,) + actor_hidden_shape, device=self.device)
        self._next_actor_hidden_state = torch.zeros((num_envs,) + actor_hidden_shape, device=self.device)
        self._prev_terminated = torch.zeros(self._num_envs, 1, device=self.device, dtype=torch.float32)

    @torch.no_grad()
    def select_action(self, obs: torch.Tensor) -> torch.Tensor:
        """ Select action deterministically (using mode/mean) """
        # Reset hidden state if previous step was terminal
        actor_h = self._next_actor_hidden_state * (1.0 - self._prev_terminated.view(-1, 1, 1)) # Adjust view based on hidden state dims

        # Ensure hidden state has correct shape (e.g., (layers*D, batch, H))
        actor_h = actor_h.permute(1, 0, 2) # Example: (batch, layers*D, H) -> (layers*D, batch, H)

        # Add sequence dimension (seq_len=1)
        obs_seq = obs.unsqueeze(1)

        # Forward pass through the actor part of the network
        policy_dist, next_actor_h = self._network.forward_actor(obs_seq, actor_h)

        # Select the mode (most likely action) for deterministic inference
        action = policy_dist.mode() # Shape: (batch_size, seq_len, action_dim)

        # Store next hidden state for the next step
        self._next_actor_hidden_state = next_actor_h.permute(1, 0, 2) # Back to (batch, layers*D, H)

        # Remove sequence dimension
        return action.squeeze(1)

    def update(self, exp: Experience) -> Optional[dict]:
        """ Inference agent does not learn, only updates hidden state """
        # Update internal state based on termination, needed for select_action
        self._prev_terminated = exp.terminated.clone()
        return None # No learning update

    def inference_agent(self, num_envs: int = 1, device: Optional[str] = None) -> Agent:
        """ Returns itself, as it's already an inference agent """
        # If num_envs or device changes, create a new instance
        if num_envs != self.num_envs or (device and device != str(self.device)):
             inference_device = device or str(self.device)
             return RecurrentSACInference(self._network, num_envs, inference_device)
        return self

;;;

drl/agent/trajectory.py
;;;
from dataclasses import dataclass
import random
from typing import Optional, List, Tuple , Any, Dict

import torch
import numpy as np

@dataclass(frozen=True)
class RecurrentPPOExperience:
    obs: torch.Tensor
    action: torch.Tensor
    next_obs: torch.Tensor
    reward: torch.Tensor
    terminated: torch.Tensor
    action_log_prob: torch.Tensor
    state_value: torch.Tensor
    hidden_state: torch.Tensor
    
class RecurrentPPOTrajectory:
    def __init__(self, n_steps: int) -> None:
        self._n_steps = n_steps
        self.reset()
        
    @property
    def reached_n_steps(self) -> bool:
        return self._recent_idx + 1 >= self._n_steps
        
    def reset(self):
        self._recent_idx = -1
        
        self._obs_buffer = self._make_buffer()
        self._action_buffer = self._make_buffer()
        self._reward_buffer = self._make_buffer()
        self._terminated_buffer = self._make_buffer()
        self._action_log_prob_buffer = self._make_buffer()
        self._state_value_buffer = self._make_buffer()
        self._hidden_state_buffer = self._make_buffer()
        
        self._final_next_obs = None
        
    def add(self, exp: RecurrentPPOExperience):
        self._recent_idx += 1
        
        self._obs_buffer[self._recent_idx] = exp.obs
        self._action_buffer[self._recent_idx] = exp.action
        self._reward_buffer[self._recent_idx] = exp.reward
        self._terminated_buffer[self._recent_idx] = exp.terminated
        self._action_log_prob_buffer[self._recent_idx] = exp.action_log_prob
        self._state_value_buffer[self._recent_idx] = exp.state_value
        self._hidden_state_buffer[self._recent_idx] = exp.hidden_state
        
        self._final_next_obs = exp.next_obs
        
    def sample(self) -> RecurrentPPOExperience:
        self._obs_buffer.append(self._final_next_obs)
        exp_batch = RecurrentPPOExperience(
            torch.concat(self._obs_buffer[:-1]),
            torch.concat(self._action_buffer),
            torch.concat(self._obs_buffer[1:]),
            torch.concat(self._reward_buffer),
            torch.concat(self._terminated_buffer),
            torch.concat(self._action_log_prob_buffer),
            torch.concat(self._state_value_buffer),
            torch.concat(self._hidden_state_buffer, dim=1),
        )
        self.reset()
        return exp_batch
        
    def _make_buffer(self) -> list:
        return [None] * self._n_steps
    
@dataclass(frozen=True)
class RecurrentPPORNDExperience:
    obs: torch.Tensor
    action: torch.Tensor
    next_obs: torch.Tensor
    reward: torch.Tensor
    rnd_int_reward: torch.Tensor
    terminated: torch.Tensor
    action_log_prob: torch.Tensor
    epi_state_value: torch.Tensor
    nonepi_state_value: torch.Tensor
    hidden_state: torch.Tensor
    next_hidden_state: torch.Tensor
    
class RecurrentPPORNDTrajectory:
    def __init__(self, n_steps: int) -> None:
        self._n_steps = n_steps
        self.reset()
        
    @property
    def reached_n_steps(self) -> bool:
        return self._recent_idx + 1 >= self._n_steps
        
    def reset(self):
        self._recent_idx = -1
        
        self._obs_buffer = self._make_buffer()
        self._action_buffer = self._make_buffer()
        self._reward_buffer = self._make_buffer()
        self._rnd_int_reward_buffer = self._make_buffer()
        self._terminated_buffer = self._make_buffer()
        self._action_log_prob_buffer = self._make_buffer()
        self._epi_state_value_buffer = self._make_buffer()
        self._nonepi_state_value_buffer = self._make_buffer()
        self._hidden_state_buffer = self._make_buffer()
        
        self._final_next_obs = None
        self._final_next_hidden_state = None
        
    def add(self, exp: RecurrentPPORNDExperience):
        self._recent_idx += 1
        
        self._obs_buffer[self._recent_idx] = exp.obs
        self._action_buffer[self._recent_idx] = exp.action
        self._reward_buffer[self._recent_idx] = exp.reward
        self._rnd_int_reward_buffer[self._recent_idx] = exp.rnd_int_reward
        self._terminated_buffer[self._recent_idx] = exp.terminated
        self._action_log_prob_buffer[self._recent_idx] = exp.action_log_prob
        self._epi_state_value_buffer[self._recent_idx] = exp.epi_state_value
        self._nonepi_state_value_buffer[self._recent_idx] = exp.nonepi_state_value
        self._hidden_state_buffer[self._recent_idx] = exp.hidden_state
        
        self._final_next_obs = exp.next_obs
        self._final_next_hidden_state = exp.next_hidden_state
        
    def sample(self) -> RecurrentPPORNDExperience:
        self._obs_buffer.append(self._final_next_obs)
        self._hidden_state_buffer.append(self._final_next_hidden_state)
        exp_batch = RecurrentPPORNDExperience(
            torch.concat(self._obs_buffer[:-1]),
            torch.concat(self._action_buffer),
            torch.concat(self._obs_buffer[1:]),
            torch.concat(self._reward_buffer),
            torch.concat(self._rnd_int_reward_buffer),
            torch.concat(self._terminated_buffer),
            torch.concat(self._action_log_prob_buffer),
            torch.concat(self._epi_state_value_buffer),
            torch.concat(self._nonepi_state_value_buffer),
            torch.concat(self._hidden_state_buffer[:-1], dim=1),
            torch.concat(self._hidden_state_buffer[1:], dim=1)
        )
        self.reset()
        return exp_batch
    
    def _make_buffer(self) -> list:
        return [None] * self._n_steps # type: ignore

@dataclass(frozen=True)
class RecurrentPPOEpisodicRNDExperience:
    obs: torch.Tensor
    action: torch.Tensor
    next_obs: torch.Tensor
    ext_reward: torch.Tensor
    int_reward: torch.Tensor
    terminated: torch.Tensor
    action_log_prob: torch.Tensor
    state_value: torch.Tensor
    hidden_state: torch.Tensor
    next_hidden_state: torch.Tensor
    
class RecurrentPPOEpisodicRNDTrajectory:
    def __init__(self, n_steps: int) -> None:
        self._n_steps = n_steps
        self.reset()
        
    @property
    def reached_n_steps(self) -> bool:
        return self._recent_idx + 1 >= self._n_steps
        
    def reset(self):
        self._recent_idx = -1
        
        self._obs_buffer = self._make_buffer()
        self._action_buffer = self._make_buffer()
        self._reward_buffer = self._make_buffer()
        self._int_reward_buffer = self._make_buffer()
        self._terminated_buffer = self._make_buffer()
        self._action_log_prob_buffer = self._make_buffer()
        self._state_value_buffer = self._make_buffer()
        self._hidden_state_buffer = self._make_buffer()
        
        self._final_next_obs = None
        self._final_next_hidden_state = None
        
    def add(self, exp: RecurrentPPOEpisodicRNDExperience):
        self._recent_idx += 1
        
        self._obs_buffer[self._recent_idx] = exp.obs
        self._action_buffer[self._recent_idx] = exp.action
        self._reward_buffer[self._recent_idx] = exp.ext_reward
        self._int_reward_buffer[self._recent_idx] = exp.int_reward
        self._terminated_buffer[self._recent_idx] = exp.terminated
        self._action_log_prob_buffer[self._recent_idx] = exp.action_log_prob
        self._state_value_buffer[self._recent_idx] = exp.state_value
        self._hidden_state_buffer[self._recent_idx] = exp.hidden_state
        
        self._final_next_obs = exp.next_obs
        self._final_next_hidden_state = exp.next_hidden_state
        
    def sample(self) -> RecurrentPPOEpisodicRNDExperience:
        self._obs_buffer.append(self._final_next_obs)
        self._hidden_state_buffer.append(self._final_next_hidden_state)
        exp_batch = RecurrentPPOEpisodicRNDExperience(
            torch.concat(self._obs_buffer[:-1]),
            torch.concat(self._action_buffer),
            torch.concat(self._obs_buffer[1:]),
            torch.concat(self._reward_buffer),
            torch.concat(self._int_reward_buffer),
            torch.concat(self._terminated_buffer),
            torch.concat(self._action_log_prob_buffer),
            torch.concat(self._state_value_buffer),
            torch.concat(self._hidden_state_buffer[:-1], dim=1),
            torch.concat(self._hidden_state_buffer[1:], dim=1)
        )
        self.reset()
        return exp_batch
    
    def _make_buffer(self) -> list:
        return [None] * self._n_steps # type: ignore




# --- New SAC Experience and Replay Buffer ---

@dataclass(frozen=True)
class RecurrentSACTransition:
    """
    Represents a single transition for Recurrent SAC.
    Stores numpy arrays for efficiency in the buffer.
    Includes hidden states.
    """
    obs: np.ndarray # (obs_shape)
    action: np.ndarray # (action_shape)
    reward: float
    next_obs: np.ndarray # (obs_shape)
    terminated: bool
    # Store hidden states *before* taking the action
    actor_hidden_state: np.ndarray # (num_layers * D, H_actor)
    critic_hidden_state: np.ndarray # (num_layers * D, H_critic)

class ReplayBuffer:
    """
    Replay buffer for off-policy RL algorithms like SAC.
    Handles recurrent hidden states.
    """
    def __init__(self, buffer_size: int, num_envs: int, obs_shape: Tuple, action_shape: Tuple, actor_hidden_shape: Tuple, critic_hidden_shape: Tuple, device: torch.device):
        self._buffer_size = buffer_size
        self._num_envs = num_envs
        self._obs_shape = obs_shape
        self._action_shape = action_shape
        self._actor_hidden_shape = actor_hidden_shape
        self._critic_hidden_shape = critic_hidden_shape
        self.device = device

        # Calculate total size needed per environment
        self.obs = np.zeros((self._buffer_size, num_envs) + obs_shape, dtype=np.float32)
        self.actions = np.zeros((self._buffer_size, num_envs) + action_shape, dtype=np.int64) # Assuming discrete actions (indices)
        self.rewards = np.zeros((self._buffer_size, num_envs), dtype=np.float32)
        self.next_obs = np.zeros((self._buffer_size, num_envs) + obs_shape, dtype=np.float32)
        self.terminated = np.zeros((self._buffer_size, num_envs), dtype=np.float32) # Use float for multiplication
        self.actor_hidden_states = np.zeros((self._buffer_size, num_envs) + actor_hidden_shape, dtype=np.float32)
        self.critic_hidden_states = np.zeros((self._buffer_size, num_envs) + critic_hidden_shape, dtype=np.float32)

        self._pos = 0
        self._full = False

    def add(self, obs: np.ndarray, action: np.ndarray, reward: np.ndarray, next_obs: np.ndarray, terminated: np.ndarray, actor_hidden: np.ndarray, critic_hidden: np.ndarray) -> None:
        """
        Add a new transition to the buffer.
        Expects inputs for all environments at a single time step.
        Args:
            obs: (num_envs, *obs_shape)
            action: (num_envs, *action_shape)
            reward: (num_envs,)
            next_obs: (num_envs, *obs_shape)
            terminated: (num_envs,)
            actor_hidden: (num_envs, num_layers*D, H_actor) - Hidden state *before* action
            critic_hidden: (num_envs, num_layers*D, H_critic) - Hidden state *before* action
        """
        if actor_hidden.shape[1:] != self._actor_hidden_shape or critic_hidden.shape[1:] != self._critic_hidden_shape:
             raise ValueError(f"Unexpected hidden state shapes. Actor: got {actor_hidden.shape}, expected {self._actor_hidden_shape}. Critic: got {critic_hidden.shape}, expected {self._critic_hidden_shape}")


        self.obs[self._pos] = obs
        self.actions[self._pos] = action
        self.rewards[self._pos] = reward
        self.next_obs[self._pos] = next_obs
        self.terminated[self._pos] = terminated.astype(np.float32)
        self.actor_hidden_states[self._pos] = actor_hidden
        self.critic_hidden_states[self._pos] = critic_hidden

        self._pos += 1
        if self._pos == self._buffer_size:
            self._full = True
            self._pos = 0

    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:
        """
        Sample a batch of transitions from the buffer.
        Args:
            batch_size: The number of transitions to sample.
        Returns:
            A dictionary containing tensors for obs, actions, rewards, next_obs, terminated,
            actor_hidden_states, and critic_hidden_states.
        """
        upper_bound = self._buffer_size if self._full else self._pos
        if upper_bound < batch_size:
             print(f"Warning: Buffer size ({upper_bound}) is smaller than batch size ({batch_size}). Sampling with replacement or fewer samples might occur implicitly if indices repeat.")
             # Consider raising an error or sampling with replacement explicitly if needed.

        # Sample time steps
        batch_inds = np.random.randint(0, upper_bound, size=batch_size)
        # Sample environments for each time step
        env_inds = np.random.randint(0, self._num_envs, size=batch_size)

        return self._get_samples(batch_inds, env_inds)

    def _get_samples(self, batch_inds: np.ndarray, env_inds: np.ndarray) -> Dict[str, torch.Tensor]:
        """
        Retrieve samples from the buffer based on indices.
        """
        # Retrieve data using advanced indexing
        obs_batch = self.obs[batch_inds, env_inds]
        actions_batch = self.actions[batch_inds, env_inds]
        rewards_batch = self.rewards[batch_inds, env_inds]
        next_obs_batch = self.next_obs[batch_inds, env_inds]
        terminated_batch = self.terminated[batch_inds, env_inds]
        actor_hidden_batch = self.actor_hidden_states[batch_inds, env_inds]
        critic_hidden_batch = self.critic_hidden_states[batch_inds, env_inds]

        # Convert to tensors
        data = {
            "obs": torch.as_tensor(obs_batch, device=self.device),
            "actions": torch.as_tensor(actions_batch, device=self.device), # Keep as int64 for embedding/indexing
            "rewards": torch.as_tensor(rewards_batch, device=self.device).unsqueeze(-1), # Add dim for consistency
            "next_obs": torch.as_tensor(next_obs_batch, device=self.device),
            "terminated": torch.as_tensor(terminated_batch, device=self.device).unsqueeze(-1), # Add dim for consistency
            "actor_hidden_states": torch.as_tensor(actor_hidden_batch, device=self.device),
            "critic_hidden_states": torch.as_tensor(critic_hidden_batch, device=self.device),
        }
        return data

    def __len__(self) -> int:
        return self._buffer_size if self._full else self._pos

    def _make_buffer(self) -> list:
        # This method might not be needed for the numpy-based buffer
        # Kept for potential compatibility if switching storage later
        return [None] * self._buffer_size # type: ignore

# --- Optional: Define RecurrentSACExperience if needed for clarity ---
# Although the buffer stores transitions, you might define this for type hinting
# or if the agent's update logic expects this structure.
@dataclass(frozen=True)
class RecurrentSACExperience:
    """
    Dataclass to potentially represent a batch sampled from the ReplayBuffer.
    Contains tensors ready for training.
    """
    obs: torch.Tensor
    actions: torch.Tensor
    rewards: torch.Tensor
    next_obs: torch.Tensor
    terminated: torch.Tensor
    actor_hidden_states: torch.Tensor
    critic_hidden_states: torch.Tensor
;;;

drl/agent/__init__.py
;;;
from .agent import Agent
from .config import RecurrentPPOConfig, RecurrentPPORNDConfig
from .net import RecurrentPPONetwork, RecurrentPPORNDNetwork, PretrainedRecurrentNetwork
from .recurrent_ppo import RecurrentPPO
from .recurrent_ppo_rnd import RecurrentPPORND
from .pretrained import PretrainedRecurrentAgent


# --- Add SAC Imports ---
from .config import RecurrentSACConfig
from .net import RecurrentSACNetwork # Import the SAC network interface
from .recurrent_sac import RecurrentSAC
from .recurrent_sac_inference import RecurrentSACInference
# Import ReplayBuffer if it's in trajectory.py
from .trajectory import ReplayBuffer, RecurrentSACTransition # Or RecurrentSACExperience if defined

;;;

drl/util/clock.py
;;;
import time

class Clock:
    """
    Time check utility.
    """
    def __init__(self, num_envs: int) -> None:
        self._num_envs = num_envs
        self.reset()
    
    def reset(self):
        self._global_time_steps = 0
        self._episode = 0
        self._episode_len = 0
        self._real_start_time = time.time()
        self._real_time = 0.0
        self._training_step = 0
        
    @property
    def global_time_steps(self) -> int:
        return self._global_time_steps
    
    @property
    def episode(self) -> int:
        return self._episode
    
    @property
    def episode_len(self) -> int:
        return self._episode_len
    
    @property
    def training_step(self) -> int:
        return self._training_step
    
    @property
    def real_time(self) -> float:
        return self._real_time
    
    def tick_gloabl_time_step(self):
        self._episode_len += 1
        self._global_time_steps += self._num_envs
        self._real_time = self._get_real_time()
        
    def tick_episode(self):
        self._episode_len = 0
        self._episode += 1
        
    def tick_training_step(self):
        self._training_step += 1
        
    def check_global_time_step_freq(self, frequency: int) -> bool:
        """
        Check if the global time step is reached to the frequency. It considers multiple environments.
        """
        return self._global_time_steps % frequency < self._num_envs
    
    @property
    def state_dict(self) -> dict:
        return dict(
            global_time_step=self._global_time_steps,
            episode=self._episode,
            episode_len=self._episode_len,
            training_step=self._training_step,
        )
    
    def load_state_dict(self, state_dict: dict):
        for key, value in state_dict.items():
            setattr(self, f"_{key}", value)
            
    def _get_real_time(self):
        return time.time() - self._real_start_time
;;;

drl/util/func.py
;;;
import torch
import torch.nn as nn

def get_model_device(model: nn.Module) -> torch.device:
    """Returns the device of the model."""
    return next(model.parameters()).device

def batch_to_perenv(batch: torch.Tensor, num_envs: int) -> torch.Tensor:
    """
    `(num_envs x n_steps, *shape)` -> `(num_envs, n_steps, *shape)`
    
    The input `batch` must be like the following example `Before`:
    
    `num_envs` = 2, `n_steps` = 3
    
    Before::
    
        [env1_step0, 
         env2_step0, 
         env1_step1, 
         env2_step1, 
         env1_step2, 
         env2_step2]
         
    After::
    
        [[env1_step0, env1_step1, env1_step2],
         [env2_step0, env2_step1, env2_step2]]
    
    """
    shape = batch.shape
    # scalar data (num_envs * n,)
    if len(shape) < 2:
        return batch.reshape(-1, num_envs).T
    # non-scalar data (num_envs * n, *shape)
    else:
        shape = (-1, num_envs) + shape[1:]
        return batch.reshape(shape).transpose(0, 1)

def perenv_to_batch(per_env: torch.Tensor) -> torch.Tensor:
    """
    `(num_envs, n_steps, *shape)` -> `(num_envs x n_steps, *shape)`
    
    The input `per_env` must be like the following example `Before`:
    
    `num_envs` = 2, `n_steps` = 3
         
    Before::
    
        [[env1_step0, env1_step1, env1_step2],
         [env2_step0, env2_step1, env2_step2]]
         
    After::
    
        [env1_step0, 
         env2_step0, 
         env1_step1, 
         env2_step1, 
         env1_step2, 
         env2_step2]
    """
    shape = per_env.shape
    # scalar data (num_envs, n,)
    if len(shape) < 3:
        return per_env.T.reshape(-1)
    # non-scalar data (num_envs, n, *shape)
    else:
        shape = (-1,) + shape[2:]
        return per_env.transpose(0, 1).reshape(shape)
    
;;;

drl/util/incr_calc.py
;;;
from typing import Optional, Union, Tuple
import torch

class IncrementalMean:
    """
    Incremental mean calculation. It uses only two memories: mean, n.
    
    See details in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm.
    """
    def __init__(self) -> None:
        self.reset()
        
    def reset(self):
        self._mean = 0.0
        self._n = 0
        
    def update(self, value):
        """Update current mean."""
        self._n += 1
        self._mean += (value - self._mean) / self._n
        return self._mean
        
    @property
    def mean(self):
        """Returns current mean."""
        return self._mean
    
    @property
    def count(self) -> int:
        return self._n
    
class IncrementalMeanVarianceFromBatch:
    """
    ## Summary
    
    Incremental mean and variance calculation from batch. 
    See details in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm.
    
    Args:
        unbiased (bool, optional): whether unbiased or biased variance. Defaults to unbiased variance.
        dim (int | None, optional): dimension along which mean and variance are computed. The default is to compute the values of the flattened array.
    """
    def __init__(self, unbiased: bool = True, dim: Optional[int] = None, dtype: torch.dtype = torch.float32) -> None:
        self._unbiased = unbiased
        self._ddof = 1 if unbiased else 0
        self._dim = dim
        self._dtype = dtype
        self.reset()
        
    @property
    def mean(self) -> torch.Tensor:
        return self._mean
    
    @property
    def variance(self) -> torch.Tensor:
        return self._var
    
    @property
    def batch_size(self) -> int:
        return self._n
    
    def reset(self):
        """Reset the mean and variance to zero."""
        self._mean = torch.tensor(0.0, dtype=self._dtype)
        self._var = torch.tensor(0.0, dtype=self._dtype)
        self._n = 0
    
    def update(self, batch: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Update the mean and variance from a batch of data.
        
        Returns:
            mean (float | np.ndarray): updated mean
            variance (float | np.ndarray): updated variance
        """
        if self._dim is None:
            batch_mean = batch.mean()
            batch_var = batch.var(unbiased=self._unbiased)
            batch_size = batch.numel()
        else:
            batch_mean = batch.mean(dim=self._dim)
            batch_var = batch.var(unbiased=self._unbiased, dim=self._dim)
            batch_size = batch.shape[self._dim]
        return self._update_from_batch_mean_var(batch_mean, batch_var, batch_size)
        
    def _update_from_batch_mean_var(self, 
                                    batch_mean: Union[float, torch.Tensor], 
                                    batch_var: Union[float, torch.Tensor], 
                                    batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # n: batch size
        # M: sum of squares
        # a: old batch
        # b: new batch
        # d: DDOF
        d = self._ddof
        
        n_a = self._n
        n_b = batch_size
        n = n_a + n_b
        
        # update mean
        delta = batch_mean - self._mean
        self._mean = self._mean + delta * n_b / n

        # update variance
        M_a = self._var * (n_a - d)
        M_b = batch_var * (n_b - d)
        M = M_a + M_b + delta**2 * n_a * n_b / n
        self._var = M / (n - d)
        
        # update batch size
        self._n = n
        
        return self._mean, self._var
    
class IncrementalEMA:
    def __init__(self, alpha: float):
        self._alpha = alpha
        self._ema = None

    def update(self, batch: torch.Tensor) -> torch.Tensor:
        # If this is the first batch, initialize ema_prev with the batch average
        if self._ema is None:
            self._ema = batch

        # Compute the EMA using the average of the current batch
        self._ema += self._alpha * (batch - self._ema)

        return self._ema

    @property
    def ema(self) -> torch.Tensor:
        if self._ema is None:
            raise ValueError("EMA is not initialized yet.")
        return self._ema
;;;

drl/util/scheduler.py
;;;
from abc import ABC, abstractmethod

class Schduler(ABC):
    @abstractmethod
    def value(self, t: float) -> float:
        pass
    
    def __call__(self, t: float) -> float:
        return self.value(t)
    
class ConstantScheduler(Schduler):
    def __init__(self, value: float):
        self._value = value
        
    def value(self, t: float) -> float:
        return self._value
    
class LinearScheduler(Schduler):
    def __init__(self, t0: float, t1: float, y0: float, y1: float):
        self._slope = (y1 - y0) / (t1 - t0)
        self._intercept = y0 - self._slope * t0
        
        self._t0 = t0
        self._t1 = t1
        self._y0 = y0
        self._y1 = y1
        
    def value(self, t: float) -> float:
        if t <= self._t0:
            val = self._y0
        elif t >= self._t1:
            val = self._y1
        else:
            val = self._slope * t + self._intercept
        return val
        
;;;

drl/util/train_step.py
;;;
from typing import Iterator
import torch
from torch.nn.parameter import Parameter
from torch.nn.utils.clip_grad import clip_grad_norm_

class TrainStep:
    def __init__(self,
                 optimizer: torch.optim.Optimizer):
        self._optimizer = optimizer
        
        self._parameters = None
        self._grad_clip_max_norm = 0.0
    
    def enable_grad_clip(self, parameters: Iterator[Parameter], grad_clip_max_norm: float):
        """
        Enable gradient clipping.

        Args:
            parameters (Iterator[Parameter]): an iterable of Tensors or a single Tensor that will have gradients normalized
            grad_clip_max_norm (float): max norm of the gradients
        """
        self._parameters = parameters
        self._grad_clip_max_norm = grad_clip_max_norm
        
    def train_step(self, loss: torch.Tensor):
        """
        Gradient step method.

        Args:
            loss (torch.Tensor): single loss value
        """
        self._optimizer.zero_grad()
        loss.backward()
        if self._parameters is not None:
            clip_grad_norm_(self._parameters, self._grad_clip_max_norm)
        self._optimizer.step()

;;;

drl/util/truncated_sequence_generator.py
;;;
from typing import Tuple, NamedTuple, List, Optional
import torch
from torch.nn.utils.rnn import pad_sequence

class TruncatedSequenceGenerator:
    """
    ## Summary
    
    Truncated sequence generator from batch.
    
    Args:
        full_seq_len (int): full sequence length
        num_envs (int): number of environments
        n_steps (int): number of time steps
        padding_value (float, optional): padding value. Defaults to 0.
        
    ## Example
    
    ::
    
        full_seq_len, num_envs, n_steps = 4, 2, 7
        
        seq_generator = TruncatedSequenceGenerator(full_seq_len, num_envs, n_steps)
        
        hidden_state = torch.randn((num_envs, n_steps, 3, 8)) # (num_envs, n_steps, D x num_layers, H)
        obs = torch.randn((num_envs, n_steps, 3)) # (num_envs, n_steps, obs_features)
        terminated = torch.zeros((num_envs, n_steps))
        terminated[0, 2], terminated[0, 4] = 1, 1
        
        seq_generator.add(hidden_state, seq_len=1)
        seq_generator.add(obs)
        
        mask, seq_init_hidden_state, obs_seq = seq_generator.generate(terminated)
        seq_init_hidden_state.squeeze_(dim=1).swapaxes_(0, 1) # (D x num_layers, seq_batch_size, H)
        
        >>> mask.shape, seq_init_hidden_state.shape, obs_seq.shape
        (torch.Size([5, 4]), torch.Size([3, 5, 8]), torch.Size([5, 4, 3]))
    """
    
    class __SeqInfo(NamedTuple):
        batch: torch.Tensor
        start_idx: int
        seq_len: int
    
    def __init__(self, 
                 full_seq_len: int, 
                 num_envs: int, 
                 n_steps: int,
                 padding_value: float = 0.0) -> None:
        if full_seq_len <= 0:
            raise ValueError(f"full_seq_len must be greater than 0, but got {full_seq_len}")
        if num_envs <= 0:
            raise ValueError(f"num_envs must be greater than 0, but got {num_envs}")
        if n_steps <= 0: 
            raise ValueError(f"n_steps must be greater than 0, but got {n_steps}")
                
        self._seq_infos: List[TruncatedSequenceGenerator.__SeqInfo] = []
        self._full_seq_len = full_seq_len
        self._num_envs = num_envs
        self._n_steps = n_steps
        self._padding_value = padding_value
    
    def add(self, batch: torch.Tensor, start_idx: int = 0, seq_len: int = 0):
        """
        Add a batch to make a truncated sequence.
        
        Args:
            batch (Tensor): `(num_envs, n_steps, *shape)`
            start_idx (int, optional): start index of each sequence. Defaults to the sequence start point.
            seq_len (int, optional): the length of each sequence. Defaults to `full_seq_len` - `start_idx` which is from start to end of the sequence.
            
        Raises:
            ValueError: when `start_idx` or `seq_len` is out of sequence range
        """
        if len(batch.shape) < 2 or batch.shape[0] != self._num_envs or batch.shape[1] != self._n_steps:
            raise ValueError(f"batch must be (num_envs, n_steps, *shape), but got {batch.shape}")
        
        # pre-process start_idx
        if start_idx < -self._full_seq_len or start_idx >= self._full_seq_len:
            raise ValueError(f"start_idx={start_idx} is out of sequence range [{-self._full_seq_len}, {self._full_seq_len - 1}].")
        if start_idx < 0:
            start_idx += self._full_seq_len
            
        # pre-process seq_len
        len_to_end = self._full_seq_len - start_idx
        if seq_len <= -len_to_end or seq_len > len_to_end:
            raise ValueError(f"seq_len={seq_len} is out of sequence range [{-len_to_end + 1}, {len_to_end}].")
        if seq_len <= 0:
            seq_len += len_to_end
            
        # add sequence info
        self._seq_infos.append(TruncatedSequenceGenerator.__SeqInfo(batch, start_idx, seq_len))

    def generate(self, interrupted_binary_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, ...]:
        """
        Generate truncated sequences.
        
        Args:
            interrupted_binary_mask (Tensor | None, optional): interrupted binary mask `(num_envs, n_steps)`. 
            it's generally terminated tensor. Defaults to no interrupting.
            
        Raises:
            RuntimeError: No batch is added before calling this method.
            
        Returns:
            mask (Tensor): `(seq_batch_size, full_seq_len)`
            generated seuqences (Tensor, ...): `(seq_batch_size, seq_len, *shape)`
        """
        if len(self._seq_infos) == 0:
            raise RuntimeError("No batch is added before calling this method.")
        
        # field cashing
        seq_infos = self._seq_infos
        num_envs = self._num_envs
        n_steps = self._n_steps
        full_seq_len = self._full_seq_len
        
        # add the mask at the beginning of the list
        mask = torch.ones((num_envs, n_steps))
        seq_infos.insert(0, TruncatedSequenceGenerator.__SeqInfo(mask, 0, full_seq_len))
        
        stacked_batches = [[] for _ in range(len(seq_infos))]
        
        for env_id in range(num_envs):
            seq_start_t = 0
            interrupted_idx = 0
            
            interrupted_time_steps = torch.tensor([]) if interrupted_binary_mask is None else torch.where(interrupted_binary_mask[env_id] > 0.5)[0]
            
            while seq_start_t < n_steps:
                # when passing the last time step, it will be zero padded
                seq_end_t = min(seq_start_t + full_seq_len, n_steps)
                
                # if interupped in the middle of the sequence, it will be zero padded
                if interrupted_idx < len(interrupted_time_steps) and interrupted_time_steps[interrupted_idx] < seq_end_t:
                    seq_end_t = interrupted_time_steps[interrupted_idx].item() + 1
                    interrupted_idx += 1
                
                for i, seq_info in enumerate(seq_infos):
                    # determine the sequence range of the current sequence
                    current_seq_start_t = seq_start_t + seq_info.start_idx
                    current_seq_end_t = min(current_seq_start_t + seq_info.seq_len, seq_end_t)
                    current_seq_time_steps = torch.arange(current_seq_start_t, current_seq_end_t)
                    stacked_batches[i].append(seq_info.batch[env_id, current_seq_time_steps])
                    
                seq_start_t = seq_end_t
                
        # pad all sequences
        padded_sequences = []
        for stacked_batch in stacked_batches:
            padded_sequences.append(pad_sequence(stacked_batch, batch_first=True, padding_value=self._padding_value))

        # convert the float mask to boolean
        padded_sequences[0] = padded_sequences[0] > 0.5
        return tuple(padded_sequences)
;;;

drl/util/__init__.py
;;;
from .clock import Clock
from .incr_calc import IncrementalMean, IncrementalMeanVarianceFromBatch
from .truncated_sequence_generator import TruncatedSequenceGenerator
from .train_step import TrainStep

;;;

envs/chem_env.py
;;;
from dataclasses import dataclass, fields
from typing import Optional, Tuple, TypeVar, Generic, Callable, Union, List, Iterable
from enum import Flag, auto
import warnings

import numpy as np
import rdkit
import selfies as sf
from rdkit import RDLogger
from rdkit.Chem import AllChem
from rdkit.DataStructs.cDataStructs import TanimotoSimilarity
from tdc import Oracle

import envs.score_func as score_f
from envs.env import Env, EnvWrapper, env_config, AsyncEnv
from envs.selfies_util import is_finished
from envs.count_int_reward import CountIntReward
from envs.selfies_tokenizer import SelfiesTokenizer
from util import instance_from_dict, suppress_print

from drl.util import IncrementalMean


RDLogger.DisableLog('rdApp.*')

T = TypeVar("T")
    
@env_config(name="ChemEnv")
class ChemEnv(Env):
    @dataclass(frozen=True)
    class __Config:
        plogp_coef: float
        qed_coef: float
        similarity_coef: float
        gsk3b_coef: float
        jnk3_coef: float
        drd2_coef: float
        sa_coef: float
        final_only: bool
        max_str_len: int
        
        @property
        def enabled_props(self) -> Tuple[str, ...]:
            props = []
            for field in fields(self):
                if field.name.endswith("_coef") and getattr(self, field.name) > 0.0:
                    props.append(field.name[:-5])
            return tuple(props)
    
    @property
    def enabled_props(self) -> Tuple[str, ...]:
        props = []
        for field in fields(self):
            if field.name.endswith("_coef") and getattr(self, field.name) > 0.0:
                props.append(field.name[:-5])
        return tuple(props)
    
    class TerminalCondition(Flag):
        NONE = 0
        INVALID = auto()
        STOP_TOKEN = auto()
        UNKNOWN = auto()
        MAX_LEN = auto()
        
    @dataclass
    class MoleculeProperty(Generic[T]):
        plogp: T
        qed: T
        similarity: T
        gsk3b: T
        jnk3: T
        drd2: T
        sa: T
        
        @staticmethod
        def new(default: Union[T, Callable[[], T]]) -> "ChemEnv.MoleculeProperty[T]":
            field_dict = dict()
            for field in fields(ChemEnv.MoleculeProperty):
                field_dict[field.name] = default() if callable(default) else default
            return ChemEnv.MoleculeProperty(**field_dict)
        
        @staticmethod
        def none(default: Union[T, Callable[[], T], None] = None) -> "ChemEnv.MoleculeProperty[Optional[T]]":
            field_dict = dict()
            for field in fields(ChemEnv.MoleculeProperty):
                field_dict[field.name] = default() if callable(default) else default
            return ChemEnv.MoleculeProperty(**field_dict)
        
    _PROP_NAMES = MoleculeProperty(
        plogp="pLogP",
        qed="QED",
        similarity="Similarity",
        gsk3b="GSK3B",
        jnk3="JNK3",
        drd2="DRD2",
        sa="SA"
    )
    
    @classmethod
    def format_prop_name(cls, prop_name: str) -> str:
        return getattr(cls._PROP_NAMES, prop_name.lower())
    
    @classmethod
    def prop_names(cls) -> Tuple[str, ...]:
        return tuple(getattr(cls._PROP_NAMES, field.name) for field in fields(cls._PROP_NAMES))
    
    def __init__(
        self, 
        plogp_coef: float = 0.0,
        qed_coef: float = 0.0,
        similarity_coef: float = 0.0,
        gsk3b_coef: float = 0.0,
        jnk3_coef: float = 0.0,
        drd2_coef: float = 0.0,
        sa_coef: float = 0.0,
        final_only: bool = False,
        max_str_len: int = 35,
        seed: Optional[int] = None,
        env_id: Optional[int] = None,
        vocabulary: Optional[Iterable[str]] = None,
        init_selfies: Optional[Union[str, List[str]]] = None
    ) -> None:
        self._config = ChemEnv.__Config(
            plogp_coef=plogp_coef,
            qed_coef=qed_coef,
            similarity_coef=similarity_coef,
            gsk3b_coef=gsk3b_coef,
            jnk3_coef=jnk3_coef,
            drd2_coef=drd2_coef,
            sa_coef=sa_coef,
            final_only=final_only,
            max_str_len=max_str_len
        )
        self._np_rng = np.random.default_rng(seed=seed)
        self._env_id = env_id
        
        self._selfies_list = []
        self._tokenizer = SelfiesTokenizer(vocabulary)
        self._init_selfies = init_selfies
        
        self._episode = -1
        
        self._fp1 = AllChem.GetMorganFingerprint(AllChem.MolFromSmiles('CC1=CC=C(C=C1)C1=CC(=NN1C1=CC=C(C=C1)S(N)(=O)=O)C(F)(F)F'), 2)
        with suppress_print():
            if self._config.gsk3b_coef > 0.0:
                self._calc_gsk3b = Oracle(name='GSK3B')
            if self._config.jnk3_coef > 0.0:
                self._calc_jnk3 = Oracle(name='JNK3')
            if self._config.drd2_coef > 0.0:
                self._calc_drd2 = Oracle(name='DRD2')
            if self._config.sa_coef > 0.0:
                self._calc_sa = Oracle(name='SA')
            
        self._prop_keys = self._config.enabled_props
        
        self.obs_shape = (self._config.max_str_len,)
        self.num_actions = self._tokenizer.vocab_size
            
    def reset(self) -> np.ndarray:
        self._time_step = -1
        self._episode += 1
        
        self._init_selfies_idxes = self._make_init_selfies_idxes()
        # state initialization, (max_str_len,) shaped array
        self._encoded_selfies = self._tokenizer.encode("", seq_len=self._config.max_str_len)
        self._current_smiles = ""
        self._current_mol = None
        
        self._prev_score = 0.0
        self._current_prop = ChemEnv.MoleculeProperty[float].none()
        self._current_total_prop = None # score (weighted sum of properties)
        
        return self._encoded_selfies[np.newaxis, ...]
    
    def step(
        self, 
        action: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict]:
        # increment time step
        self._time_step += 1
        
        # update state
        a = int(action.item())
        self._update_state(a)
        
        # check termination
        terminal_cond = self._check_terminal_condition(a)
        
        # calcaulte reward
        reward, take_penalty = self._calc_reward(terminal_cond)
        
        # convert to ndarray
        terminated = np.array([terminal_cond != ChemEnv.TerminalCondition.NONE])
        reward = np.array([reward], dtype=np.float64)
        
        next_obs = self._encoded_selfies[np.newaxis, ...]
        real_final_next_obs = next_obs[terminated]
        
        info = dict()
        
        # if terminated, next_obs is reset to the first observation of the next episode
        if terminated:
            info["metric"] = self._episode_metric_info_dict(terminal_cond)
            info["valid_termination"] = not take_penalty
            next_obs = self.reset()
        
        return next_obs, reward, terminated, real_final_next_obs, info
    
    def close(self):
        pass
    
    def _make_init_selfies_idxes(self) -> List[int]:
        if self._init_selfies is not None:
            if isinstance(self._init_selfies, str):
                return self._tokenizer.encode(self._init_selfies).tolist()
            elif isinstance(self._init_selfies, list):
                selfies = self._np_rng.choice(self._init_selfies)
                return self._tokenizer.encode(selfies).tolist()
        
        # if (
        #     self._config.similarity_coef > 0.0 # Similarity
        # ):
        #     return self._tokenizer.encode("[C]").tolist()
        # if (
        #     self._config.gsk3b_coef > 0.0 # GSK3B
        #     or self._config.jnk3_coef > 0.0 # JNK3
        # ):
        #     selfies = self._np_rng.choice(['[C][C][C]', '[C][=C][C]', '[C][C][=N]', '[C][N][C]', '[C][O][C]'])
        #     return self._tokenizer.encode(selfies).tolist()
        
        return list()
    
    def _update_state(self, action: int):
        if self._time_step < len(self._init_selfies_idxes):
            # append initial selfies_idxes regardless of the action
            self._encoded_selfies[self._time_step] = self._init_selfies_idxes[self._time_step]
        else:
            self._encoded_selfies[self._time_step] = action
        self._current_smiles = self._decode_smiles()
        self._current_mol = AllChem.MolFromSmiles(self._current_smiles)
        
    def _check_terminal_condition(self, action: int) -> TerminalCondition:
        # check termination
        terminal_cond = ChemEnv.TerminalCondition.NONE
        if action == self._tokenizer.stop_token_val:
            terminal_cond |= ChemEnv.TerminalCondition.STOP_TOKEN
        if self._current_mol is None:
            terminal_cond |= ChemEnv.TerminalCondition.INVALID
        if terminal_cond == is_finished(self._tokenizer.decode(self._encoded_selfies, include_stop_token=False)):
            terminal_cond |= ChemEnv.TerminalCondition.UNKNOWN
        if self._time_step == self._config.max_str_len - 1:
            terminal_cond |= ChemEnv.TerminalCondition.MAX_LEN
        return terminal_cond
    
    def _calc_reward(self, terminal_cond: TerminalCondition) -> Tuple[float, bool]:
        """Calculate a reward of the current state."""
        # if the terminal condition is one of the following cases,
        # case 1: the episode is terminated at the initial time step
        # case 2: the molecule is invalid
        # then take penalty
        take_penalty = (self._time_step == 0 and terminal_cond != ChemEnv.TerminalCondition.NONE) or (ChemEnv.TerminalCondition.INVALID in terminal_cond)   
        terminated = terminal_cond != ChemEnv.TerminalCondition.NONE
        
        if take_penalty:
            self._current_prop = ChemEnv.MoleculeProperty[float].none()
            self._current_total_prop = None
            return -1.0, True
        
        reward = 0.0
        
        # calculate the reward only if the episode is terminated
        if self._config.final_only and terminated:
            reward = self._calc_current_score()
        else:
            current_score = self._calc_current_score()
            reward = current_score - self._prev_score # delta reward
            self._prev_score = current_score
            
        return reward, False
    
    @suppress_print()
    def _calc_current_score(self) -> float:
        score = 0.0
        
        # pLogP
        if self._config.plogp_coef > 0.0:
            # --- Previous Modification ---
            # penalized_logp = score_f.calculate_pLogP(self._current_smiles)
            # self._current_prop.plogp = penalized_logp
            # reward_component = max(penalized_logp, -10.0)
            # score += self._config.plogp_coef * reward_component
            # --- End Previous Modification ---

            # --- NEW Simplification START ---
            # Calculate raw logP only if the molecule is valid
            raw_logp = -10.0 # Assign a large penalty if molecule is invalid initially
            if self._current_mol is not None:
                 try:
                     # Use RDKit's raw LogP calculation
                     from rdkit.Chem import Descriptors
                     raw_logp = Descriptors.MolLogP(self._current_mol)
                 except Exception as e:
                     # Handle rare calculation errors on valid-ish molecules
                     # logger.print(f"Warning: RDKit LogP calculation failed for {self._current_smiles}: {e}")
                     raw_logp = -10.0 # Penalize calculation errors

            # Store the original penalized logP for logging purposes if desired
            # It's calculated here but not used directly in the reward signal below
            try:
                 penalized_logp_for_logging = score_f.calculate_pLogP(self._current_smiles) if self._current_mol else -10.0
            except Exception:
                 penalized_logp_for_logging = -10.0
            self._current_prop.plogp = penalized_logp_for_logging

            # --- Define the reward component based *only* on raw LogP ---
            # Scale raw logP to be in a reasonable range (e.g., similar magnitude to other rewards like QED)
            # We clip to prevent extreme values. Adjust the scaling factor (e.g., / 5.0) and clipping range if needed.
            # Aim for a signal that can become positive. Raw logP for typical molecules is often -2 to 6.
            reward_component = max(raw_logp / 5.0, -2.0) # Scale by 5, clip at -2

            # Add this simplified reward component to the total score
            # The plogp_coef (from YAML, likely 1.0) scales this component's contribution
            score += self._config.plogp_coef * reward_component
            # --- NEW Simplification END ---
            
        # QED
        if self._config.qed_coef > 0.0:
            self._current_prop.qed = rdkit.Chem.QED.qed(self._current_mol) # type: ignore
            score += self._config.qed_coef * self._current_prop.qed
            
        # Similarity
        if self._config.similarity_coef > 0.0:
            fp2 = AllChem.GetMorganFingerprint(self._current_mol, 2) # type: ignore
            self._current_prop.similarity = TanimotoSimilarity(self._fp1, fp2)
            score += self._config.similarity_coef * self._current_prop.similarity
            
        warnings.filterwarnings("ignore")
        
        # GSK3B
        if self._config.gsk3b_coef > 0.0:            
            self._current_prop.gsk3b = self._calc_gsk3b(self._current_smiles)
            score += self._config.gsk3b_coef * self._current_prop.gsk3b
        
        # JNK3
        if self._config.jnk3_coef > 0.0:
            self._current_prop.jnk3 = self._calc_jnk3(self._current_smiles)
            score += self._config.jnk3_coef * self._current_prop.jnk3
            
        # DRD2
        if self._config.drd2_coef > 0.0:
            self._current_prop.drd2 = self._calc_drd2(self._current_smiles)
            score += self._config.drd2_coef * self._current_prop.drd2
            
        # SA
        if self._config.sa_coef > 0.0:
            self._current_prop.sa = self._calc_sa(self._current_smiles)
            score += self._config.sa_coef * (0.1 * (10 - self._current_prop.sa))
            
        warnings.filterwarnings("default")
        
        self._current_total_prop = score
        
        return score
        
    def _decode_smiles(self) -> str:
        return sf.decoder(
            self._tokenizer.decode(self._encoded_selfies, include_stop_token=False)
        ) # type: ignore
        
    def _episode_metric_info_dict(self, terminal_cond: TerminalCondition) -> dict:
        metric_info_dict = dict()
        # keys
        metric_info_dict["keys"] = dict()
        metric_info_dict["keys"]["episode"] = self._episode
        if self._env_id is not None:
            metric_info_dict["keys"]["env_id"] = self._env_id
        # values
        metric_info_dict["values"] = dict()
        metric_info_dict["values"]["score"] = self._current_total_prop
        for prop_name in self._prop_keys:
            metric_info_dict["values"][prop_name] = getattr(self._current_prop, prop_name)
        metric_info_dict["values"]["selfies"] = self._tokenizer.decode(self._encoded_selfies)
        metric_info_dict["values"]["smiles"] = self._current_smiles
        return {"episode_metric": metric_info_dict}
    

class ChemEnvWrapper(EnvWrapper):
    def __init__(self, env: ChemEnv, count_int_reward: CountIntReward, crwd_coef: float = 0.0) -> None:
        super().__init__(env)
        
        self._count_int_reward = count_int_reward
        self._crwd_coef = crwd_coef
        
        self._avg_count_reward = IncrementalMean()
        self._tokenizer = env._tokenizer
        
    def reset(self) -> np.ndarray:
        obs = super().reset()
        self._avg_count_reward.reset()
        # in this case, the one-hot vector is a zero vector
        return self._last_token_one_hot(obs)
        
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict]:
        next_obs, reward, terminated, real_final_next_obs, info = super().step(action)
        
        if self._crwd_coef != 0.0:
            count_reward = 0.0
            # if either the episode is not terminated or the episode is terminated with a valid molecule,
            # then add the count-based intrinsic reward
            if info.get("valid_termination", True):
                encoded_sequences = next_obs[0] if not terminated else real_final_next_obs[0]
                smiles = sf.decoder(
                    self._tokenizer.decode(encoded_sequences, include_stop_token=False) # integer sequence -> selfies
                ) # selfies -> smiles
                count_reward = self._count_int_reward(smiles) # type: ignore
                
            reward += self._crwd_coef * count_reward
            self._avg_count_reward.update(count_reward)
                
            if "metric" in info and "episode_metric" in info["metric"]:
                info["metric"]["episode_metric"]["values"]["avg_count_int_reward"] = self._avg_count_reward.mean
                self._avg_count_reward.reset()
        
        # convert to one-hot vector of only the last token
        next_obs = self._last_token_one_hot(next_obs)
        real_final_next_obs = self._last_token_one_hot(real_final_next_obs)
        
        return next_obs, reward, terminated, real_final_next_obs, info
    
    @property
    def obs_shape(self) -> Tuple[int, ...]:
        return (self._tokenizer.vocab_size,)
    
    def _last_token_one_hot(self, obs: np.ndarray) -> np.ndarray:
        """from the integer sequence to the one-hot of the last token"""
        last_token_val = self._tokenizer.last_token_value(obs)
        return self._tokenizer.to_one_hot(last_token_val)
    
    
def make_async_chem_env(
    num_envs: int = 1,
    seed: Optional[int] = None,
    **kwargs
) -> AsyncEnv:
    def env_make_func(seed=None, env_id=None) -> Env:
        config = kwargs.copy()
        config["seed"] = seed
        config["env_id"] = env_id
        env = instance_from_dict(ChemEnv, config)
        
        config["np_rng"] = env._np_rng
        count_int_reward = instance_from_dict(CountIntReward, config)
        return ChemEnvWrapper(env, count_int_reward, config.get("crwd_coef", 0.0))
    
    if seed is not None:
        np_rng = np.random.default_rng(seed=seed)
        seeds = np_rng.integers(0, np.iinfo(np.int32).max, size=num_envs)
        env_make_func_list = [lambda seed=seed, env_id=i: env_make_func(seed=seed, env_id=env_id) for i, seed in enumerate(seeds)]
    else:
        env_make_func_list = [lambda env_id=i: env_make_func(env_id=i) for i in range(num_envs)]
    
    return AsyncEnv(env_make_func_list, trace_env=0)
    
;;;

envs/count_int_reward.py
;;;
import math
from collections import Counter
import numpy as np
from rdkit.Chem import AllChem, DataStructs
from typing import Union, Optional

class CountIntReward:
    def __init__(
        self, 
        max_mol_count: int = 10,
        fingerprint_bits: int = 256,
        fingerprint_radius: int = 2,
        lsh_bits: int = 32,
        np_rng: Optional[np.random.Generator] = None
    ) -> None:
        self._fingerprint_bits = fingerprint_bits
        self._fingerprint_radius = fingerprint_radius
        self._lsh_bits = lsh_bits
        self._max_mol_count = max_mol_count
        
        if np_rng is None:
            np_rng = np.random.default_rng()
        self._lsh_rand_matrix = np_rng.normal(size=(self._lsh_bits, self._fingerprint_bits))
        self._counter = Counter()

    def calc_reward(self, smiles: str) -> float:
        new_mol_count = self._update_mol_count(smiles)
        x = min(new_mol_count - 1, self._max_mol_count)
        return math.exp(-x)
    
    def __call__(self, smiles: str) -> float:
        return self.calc_reward(smiles)
    
    def _update_mol_count(self, smiles: str) -> int:
        fp = self._to_fingerprint(smiles)
        if type(fp) is not int:
            fp = self._lsh(fp).tobytes()
        self._counter.update((fp,))
        return self._counter[fp]    
    
    def _to_fingerprint(self, smiles: str) -> Union[np.ndarray, int]:
        mol = AllChem.MolFromSmiles(smiles)
        if mol is not None:
            fp = AllChem.GetMorganFingerprintAsBitVect(
                mol, 
                self._fingerprint_radius,
                nBits=self._fingerprint_bits
            )
            x = np.zeros((0,), dtype=np.int8)
            DataStructs.ConvertToNumpyArray(fp, x)
            return x
        else:
            return -1
        
    def _lsh(self, fp: np.ndarray) -> np.ndarray:
        return np.sign(np.dot(self._lsh_rand_matrix, fp))
;;;

envs/env.py
;;;
import multiprocessing as mp
from abc import ABC, abstractmethod
from enum import Enum
from multiprocessing.connection import Connection
from typing import Callable, Dict, Iterable, List, Optional, Tuple, Generic, TypeVar

import numpy as np

T = TypeVar('T')

def env_config(
    name: str,
    obs_shape: Optional[Tuple[int, ...]] = None,
    num_actions: Optional[int] = None,
):
    def decorator(cls: T) -> T:
        if not issubclass(cls, Env):
            raise TypeError("Class must inherit from Env")
        cls.name = name
        if obs_shape is not None:
            cls.obs_shape = obs_shape
        if num_actions is not None:
            cls.num_actions = num_actions
        return cls
    return decorator

class Env(ABC):
    name: str
    obs_shape: Tuple[int, ...]
    num_actions: int
    
    @abstractmethod 
    def reset(self) -> np.ndarray:
        """
        Resets the environment to an initial state and returns the initial observation.

        Returns:
            obs (ndarray): `(num_envs, *obs_shape)`. Initial observation.
        """
        raise NotImplementedError
    
    @abstractmethod
    def step(
        self, 
        action: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict]:
        """
        Takes a step in the environment using an action.

        Args:
            - action (ndarray): `(num_envs, num_actions)`. Action provided by the agent.

        Returns:
            - next_obs (ndarray): `(num_envs, *obs_shape)`. Next observation which is automatically reset to the first observation of the next episode. 
            - reward (ndarray): `(num_envs,)`. Scalar reward values.
            - terminated (ndarray): `(num_envs,)`. Whether the episode is terminated.
            - real_final_next_obs (ndarray): `(num_terminated_envs, *obs_shape)`. "Real" final next observation of the episode. You can access only if any environment is terminated. 
            - info (dict): Additional information.
        """
        raise NotImplementedError
    
    @abstractmethod
    def close(self):
        """Close the environment and release resources."""
        raise NotImplementedError
    
    @property
    def num_envs(self) -> int:
        return 1
    
    @property
    def config_dict(self) -> dict:
        """
        The configuration of the environment.
        """
        return dict()
    
    def save_data(self, base_dir: str):
        """
        Save the environment data. `base_dir` is different according to the configuration ID, training, inference, etc.
        """
        pass
    
EnvMakeFunc = Callable[[], Env]

class WorkerCommand(Enum):
    RESET = 0,
    STEP = 1,
    CLOSE = 2,
    SAVE_DATA = 3,
    
class AsyncEnv(Env):    
    def __init__(self, env_make_func_iter: Iterable[EnvMakeFunc], trace_env: int = 0) -> None:
        self._trace_env = trace_env
        # check the number of environments
        env_make_func_tuple = tuple(env_make_func_iter)
        self._num_envs = len(env_make_func_tuple)
        if self._num_envs == 0:
            raise ValueError('env_make_func must not be empty.')
        
        # set properties
        env_temp = env_make_func_tuple[0]()
        self._obs_shape = env_temp.obs_shape
        self._num_actions = env_temp.num_actions
        self._config_dict = env_temp.config_dict
        env_temp.close()
        
        # make workers
        self._parent_connections: List[Connection] = []
        self._workers: List[mp.Process] = []
        for env_make_func in env_make_func_tuple:
            parent_conn, child_conn = mp.Pipe()
            worker = mp.Process(
                target=AsyncEnv._worker,
                args=(
                    env_make_func,
                    child_conn,
                )
            )
            
            self._parent_connections.append(parent_conn)
            self._workers.append(worker)
            
            worker.start()
            child_conn.close()
        
    def reset(self) -> np.ndarray:
        for parent_conn in self._parent_connections:
            parent_conn.send((WorkerCommand.RESET, None))
        
        obs_tuple = tuple(parent_conn.recv() for parent_conn in self._parent_connections)
        return np.concatenate(obs_tuple, axis=0)
    
    def step(
        self, 
        action: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict]:
        for i, parent_conn in enumerate(self._parent_connections):
            parent_conn.send((WorkerCommand.STEP, action[i:i+1]))
            
        unwrapped_results = tuple(parent_conn.recv() for parent_conn in self._parent_connections)
        results_tuple = tuple(zip(*unwrapped_results))
        concat_ndarrays = tuple(np.concatenate(result, axis=0) for result in results_tuple[:-1])
        info = self._merge_info(results_tuple[-1])
        
        return concat_ndarrays + (info,) # type: ignore
    
    def _merge_info(self, info_tuple: Tuple[dict, ...]) -> dict:
        num_envs = len(info_tuple)
        assert num_envs == self._num_envs
        # get all keys in info_tuple
        info_keys = set()
        for info in info_tuple:
            info_keys.update(info.keys())
        # merge info with mask
        merged_info = dict()
        for key in info_keys:
            items = np.full((num_envs,), None, dtype=object)
            mask = np.full((num_envs,), False, dtype=bool)
            for i, info in enumerate(info_tuple):
                if key in info:
                    items[i] = info[key]
                    mask[i] = True
            merged_info[key] = items
            merged_info[f"_{key}"] = mask
        return merged_info
    
    def close(self):
        for parent_conn in self._parent_connections:
            parent_conn.send((WorkerCommand.CLOSE, None))
        
        for parent_conn in self._parent_connections:
            parent_conn.recv()
            
        for parent_conn in self._parent_connections:
            parent_conn.close()
        
        for worker in self._workers:
            worker.join()
    
    @property
    def obs_shape(self) -> Tuple[int, ...]:
        return self._obs_shape
    
    @property
    def num_actions(self) -> int:
        return self._num_actions
    
    @property
    def num_envs(self) -> int:
        return self._num_envs
    
    @property
    def config_dict(self) -> dict:
        return self._config_dict
    
    def _single_env_step(self, env_action: Tuple[Env, np.ndarray]):
        env = env_action[0]
        action = env_action[1]
        return env.step(action[np.newaxis, ...])
    
    @staticmethod
    def _worker(
        env_make_func: EnvMakeFunc,
        child: Connection,
    ):
        env = env_make_func()
        try:
            while True:
                command, data = child.recv()
                if command == WorkerCommand.RESET:
                    if data is not None:
                        raise ValueError('when you reset, data must be None.')
                    child.send(env.reset())
                elif command == WorkerCommand.STEP:
                    if not isinstance(data, np.ndarray):
                        raise ValueError(f'data must be np.ndarray, but got {type(data)}.')
                    child.send(env.step(data))
                elif command == WorkerCommand.CLOSE:
                    if data is not None:
                        raise ValueError('when you close, data must be None.')
                    env.close()
                    child.send(None)
                    break
                elif command == WorkerCommand.SAVE_DATA:
                    if not isinstance(data, str):
                        raise ValueError(f'since data is directory, it must be str, but got {type(data)}.')
                    child.send(env.save_data(data))
                else:
                    raise NotImplementedError(f'command {command} is not implemented.')
        except KeyboardInterrupt:
            command, data = child.recv()
            if command == WorkerCommand.CLOSE:
                if data is not None:
                    raise ValueError('when you close, data must be None.')
                child.send(None)
        except Exception as ex:
            raise ex
        finally:
            pass

                
class EnvWrapper(Env):
    def __init__(self, env: Env) -> None:
        self._env = env
    
    def reset(self) -> np.ndarray:
        return self._env.reset()
    
    def step(
        self, 
        action: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict]:
        return self._env.step(action)
    
    def close(self):
        self._env.close()
    
    @property
    def obs_shape(self) -> Tuple[int, ...]:
        return self._env.obs_shape
    
    @property
    def num_actions(self) -> int:
        return self._env.num_actions
    
    @property
    def num_envs(self) -> int:
        return self._env.num_envs
    
    @property
    def config_dict(self) -> dict:
        return self._env.config_dict
    
    def save_data(self, base_dir: str):
        self._env.save_data(base_dir)
    
    def __getattr__(self, name):
        return getattr(self._env, name)
;;;

envs/score_func.py
;;;
#
# calculation of synthetic accessibility score as described in:
#
# Estimation of Synthetic Accessibility Score of Drug-like Molecules based on Molecular Complexity and Fragment Contributions
# Peter Ertl and Ansgar Schuffenhauer
# Journal of Cheminformatics 1:8 (2009)
# http://www.jcheminf.com/content/1/1/8
#
# several small modifications to the original paper are included
# particularly slightly different formula for marocyclic penalty
# and taking into account also molecule symmetry (fingerprint density)
#
# for a set of 10k diverse molecules the agreement between the original method
# as implemented in PipelinePilot and this implementation is r2 = 0.97
#
# peter ertl & greg landrum, september 2013
#


from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import pickle

import math
from collections import defaultdict

import os.path as op

import numpy as np
from rdkit import Chem
from rdkit.Chem import MolFromSmiles as smi2mol
from rdkit.Chem import MolToSmiles as mol2smi
from rdkit.Chem import Descriptors


_fscores = None

def readFragmentScores(name='fpscores'):
    import gzip
    global _fscores
    # generate the full path filename:
    if name == "fpscores":
        name = op.join(op.dirname(__file__), f"data/{name}")
    _fscores = pickle.load(gzip.open('%s.pkl.gz' % name))
    outDict = {}
    for i in _fscores:
        for j in range(1, len(i)):
            outDict[i[j]] = float(i[0])
    _fscores = outDict


def numBridgeheadsAndSpiro(mol, ri=None):
    nSpiro = rdMolDescriptors.CalcNumSpiroAtoms(mol)
    nBridgehead = rdMolDescriptors.CalcNumBridgeheadAtoms(mol)
    return nBridgehead, nSpiro


def calculateScore(m):
    if _fscores is None:
        readFragmentScores()

    # fragment score
    fp = rdMolDescriptors.GetMorganFingerprint(m,2)  # <- 2 is the *radius* of the circular fingerprint
    fps = fp.GetNonzeroElements()
    score1 = 0.
    nf = 0
    for bitId, v in fps.items():
        nf += v
        sfp = bitId
        score1 += _fscores.get(sfp, -4) * v
    
    score1 /= (nf + 1e-7)

    # features score
    nAtoms = m.GetNumAtoms()
    nChiralCenters = len(Chem.FindMolChiralCenters(m, includeUnassigned=True))
    ri = m.GetRingInfo()
    nBridgeheads, nSpiro = numBridgeheadsAndSpiro(m, ri)
    nMacrocycles = 0
    for x in ri.AtomRings():
        if len(x) > 8:
            nMacrocycles += 1

    sizePenalty = nAtoms**1.005 - nAtoms
    stereoPenalty = math.log10(nChiralCenters + 1)
    spiroPenalty = math.log10(nSpiro + 1)
    bridgePenalty = math.log10(nBridgeheads + 1)
    macrocyclePenalty = 0.
    # ---------------------------------------
    # This differs from the paper, which defines:
    #  macrocyclePenalty = math.log10(nMacrocycles+1)
    # This form generates better results when 2 or more macrocycles are present
    if nMacrocycles > 0:
        macrocyclePenalty = math.log10(2)

    score2 = 0. - sizePenalty - stereoPenalty - spiroPenalty - bridgePenalty - macrocyclePenalty

    # correction for the fingerprint density
    # not in the original publication, added in version 1.1
    # to make highly symmetrical molecules easier to synthetise
    score3 = 0.
    if nAtoms > len(fps):
        score3 = math.log(float(nAtoms) / len(fps)) * .5

    sascore = score1 + score2 + score3

    # need to transform "raw" value into scale between 1 and 10
    min = -4.0
    max = 2.5
    sascore = 11. - (sascore - min + 1) / (max - min) * 9.
    # smooth the 10-end
    if sascore > 8.:
        sascore = 8. + math.log(sascore + 1. - 9.)
    if sascore > 10.:
        sascore = 10.0
    elif sascore < 1.:
        sascore = 1.0

    return sascore


def processMols(mols):
    print('smiles\tName\tsa_score')
    for i, m in enumerate(mols):
        if m is None:
            continue

        s = calculateScore(m)

        smiles = Chem.MolToSmiles(m)
        print(smiles + "\t" + m.GetProp('_Name') + "\t%3f" % s)




def sanitize_smiles(smi):
    '''Return a canonical smile representation of smi
    
    Parameters:
    smi (string) : smile string to be canonicalized 
    
    Returns:
    mol (rdkit.Chem.rdchem.Mol) : RdKit mol object                          (None if invalid smile string smi)
    smi_canon (string)          : Canonicalized smile representation of smi (None if invalid smile string smi)
    conversion_successful (bool): True/False to indicate if conversion was  successful 
    '''
    try:
        mol = smi2mol(smi, sanitize=True)
        smi_canon = mol2smi(mol, isomericSmiles=False, canonical=True)
        return (mol, smi_canon, True)
    except:
        return (None, None, False)
         
    
def get_logP(mol):
    '''Calculate logP of a molecule 
    
    Parameters:
    mol (rdkit.Chem.rdchem.Mol) : RdKit mol object, for which logP is to calculates
    
    Returns:
    float : logP of molecule (mol)
    '''
    return Descriptors.MolLogP(mol)


def get_SA(mol):
    return calculateScore(mol)

def calc_RingP(mol):
    '''Calculate Ring penalty for each molecule in unseen_smile_ls,
       results are recorded in locked dictionary props_collect 
    '''
    cycle_list = mol.GetRingInfo().AtomRings() 
    if len(cycle_list) == 0:
        cycle_length = 0
    else:
        cycle_length = max([ len(j) for j in cycle_list ])
    if cycle_length <= 6:
        cycle_length = 0
    else:
        cycle_length = cycle_length - 6
    return cycle_length


def calculate_pLogP(smiles):  
    mol, smiles_canon, done = sanitize_smiles(smiles)
    logP_scores   = [get_logP(mol)]
    SA_scores     = [get_SA(mol)]
    RingP_scores  = [calc_RingP(mol)]

    #logP_norm  = np.array([((x - 2.4729421499641497) / 1.4157879815362406) for x in logP_scores])
    #SAS_norm   = np.array([((x - 3.0470797085649894) / 0.830643172314514) for x in SA_scores])
    #RingP_norm = [((x - 0.038131530820234766) / 0.2240274735210179) for x in RingP_scores]

    logP_norm  = (np.array(logP_scores) - 2.4729421499641497) / 1.4157879815362406
    SAS_norm   = (np.array(SA_scores)  - 3.0470797085649894) / 0.830643172314514
    RingP_norm = (np.array(RingP_scores)  - 0.038131530820234766) / 0.2240274735210179

    return logP_norm[0] - SAS_norm[0] - RingP_norm[0]


#
#  Copyright (c) 2013, Novartis Institutes for BioMedical Research Inc.
#  All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above
#       copyright notice, this list of conditions and the following
#       disclaimer in the documentation and/or other materials provided
#       with the distribution.
#     * Neither the name of Novartis Institutes for BioMedical Research Inc.
#       nor the names of its contributors may be used to endorse or promote
#       products derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#

;;;

envs/selfies_tokenizer.py
;;;
from typing import Iterable, Optional, Union, List

import numpy as np
import selfies as sf


def selfies_alphabet() -> List[str]:
    try:
        return ["[#N]"] + sf.selfies_alphabet()
    except AttributeError:
        pass
    
    try:
        return list(sf.get_semantic_robust_alphabet())
    except AttributeError:
        pass
    
    raise AttributeError("We highly recommend the selfies package version either 0.2.4 or 2.1.1.")

def split_selfies(selfies: str) -> List[str]:
    return list('[' + character for character in selfies.split('['))[1:]

class SelfiesTokenizer:
    def __init__(self, vocabulary: Optional[Iterable[str]] = None) -> None:
        self._stop_token = "[STOP]"
        if vocabulary is None:
            self._selfies_vocab = selfies_alphabet() + [self._stop_token]
        else:
            # vocabulary = set(vocabulary)
            if len(set(vocabulary)) != len(list(vocabulary)):
                raise ValueError("The vocabulary contains duplicate tokens.")
                
            self._selfies_vocab = list(vocabulary)
            if self._stop_token not in set(vocabulary):
                self._selfies_vocab.append(self._stop_token)
        self._token_to_value_dict = {token: i for i, token in enumerate(self._selfies_vocab)}
        self._padding_value = -1
        
    @property
    def vocabulary(self) -> List[str]:
        return list(self._selfies_vocab)
        
    @property
    def vocab_size(self) -> int:
        return len(self._selfies_vocab)
    
    @property
    def stop_token(self) -> str:
        return self._stop_token
    
    @property
    def stop_token_val(self) -> int:
        return self._token_to_value_dict[self._stop_token]
        
    def encode(self, selfies_list: Union[str, List[str]], seq_len: Optional[int] = None, include_stop_token: bool = False) -> np.ndarray:
        """
        Encode the SELFIES strings to the integer sequences. 
        The integer sequences are padded with the padding value `-1`.

        Args:
            selfies_list (str | List[str]): one SELFIES string or `batch_size` SELFIES strings list.
            seq_len (Optional[int], optional): The length of the sequence. If None, the maximum length of the SELFIES strings is used. Defaults to None.
            include_stop_token (bool, optional): Whether to include the stop token at the end of the sequence. Defaults to False.

        Returns:
            encoded_sequences (ndarray): `(seq_len,)` or `(batch_size, seq_len)`
        """
        if isinstance(selfies_list, str):
            selfies_list = [selfies_list]
            return self._encode_batch(selfies_list, seq_len, include_stop_token)[0]
        return self._encode_batch(selfies_list, seq_len, include_stop_token)
    
    def decode(self, encoded_sequences: np.ndarray, include_stop_token: bool = True) -> Union[str, List[str]]:
        """
        Decode the integer sequences to the SELFIES strings. 
        Padding values are ignored.

        Args:
            encoded_sequences (ndarray): `(seq_len,)` or `(batch_size, seq_len)`
            include_stop_token (bool, optional): Whether to include the stop token if it is present in the sequence. Defaults to True.

        Returns:
            selfies_list (str | List[str]): one SELFIES string or `batch_size` SELFIES strings list.
        """
        if len(encoded_sequences.shape) == 1:
            return self._decode(encoded_sequences, include_stop_token)
        return list(self._decode(encoded, include_stop_token) for encoded in encoded_sequences)
    
    def last_token_value(self, encoded_sequences: np.ndarray) -> np.ndarray:
        """
        Get the value of the last token in the sequence. 
        The last token is the one before the first occurrence of the padding token (-1).
        If the sequence is all padding tokens, the last token value will be -1.

        Args:
            encoded_sequences (ndarray): `(seq_len,)` or `(batch_size, seq_len)`

        Returns:
            last_token (int | ndarray): scalar integer or `(batch_size,)`
        """
        if len(encoded_sequences.shape) == 1:
            encoded_sequences = np.expand_dims(encoded_sequences, axis=0)
            return self._last_token_value_batch(encoded_sequences)[0]
        return self._last_token_value_batch(encoded_sequences)
    
    def to_one_hot(self, encoded_sequences: np.ndarray) -> np.ndarray:
        """
        Convert the encoded sequences to one-hot encoding. 
        Padding tokens are converted to zero vectors.

        Args:
            encoded_sequences (ndarray): `(seq_len,)` or `(batch_size, seq_len)`

        Returns:
            one_hot (ndarray): `(seq_len, n_tokens)` or `(batch_size, seq_len, n_tokens)`
        """
        if len(encoded_sequences.shape) == 1:
            encoded_sequences = np.expand_dims(encoded_sequences, axis=0)
            return self._to_one_hot_batch(encoded_sequences)[0]
        return self._to_one_hot_batch(encoded_sequences)
    
    def from_one_hot(self, one_hot: np.ndarray) -> np.ndarray:
        """
        Convert the one-hot encoded sequences back to the original encoded sequences.
        Zero vectors are converted to padding tokens.

        Args:
            one_hot (ndarray): `(seq_len, n_tokens)` or `(batch_size, seq_len, n_tokens)`

        Returns:
            encoded_sequences (ndarray): `(seq_len,)` or `(batch_size, seq_len)`
        """
        if len(one_hot.shape) == 2:
            one_hot = np.expand_dims(one_hot, axis=0)
            return self._from_one_hot_batch(one_hot)[0]
        return self._from_one_hot_batch(one_hot)
    
    def _encode_batch(self, selfies_list: Iterable[str], seq_len: Optional[int], include_stop_token: bool) -> np.ndarray:
        selfies_tokens_list = [self._split_selfies(selfies) for selfies in selfies_list]
        if include_stop_token:
            for selfies_tokens in selfies_tokens_list:
                selfies_tokens.append(self._stop_token)
        if seq_len is None:
            seq_len = max(len(selfies_tokens) for selfies_tokens in selfies_tokens_list)
        encoded_list = [self._encode_from_tokens(selfies_tokens, seq_len) for selfies_tokens in selfies_tokens_list]
        return np.stack(encoded_list, axis=0)
    
    def _split_selfies(self, selfies: str) -> List[str]:
        return list('[' + character for character in selfies.split('['))[1:]
        
    def _encode_from_tokens(self, selfies_tokens: List[str], seq_len: int) -> np.ndarray:
        sequence = np.full((seq_len,), self._padding_value, dtype=np.int64)
        for i, token in enumerate(selfies_tokens):
            if seq_len <= i:
                break
            sequence[i] = self._token_to_value_dict[token]
        return sequence
    
    def _decode(self, encoded: np.ndarray, include_stop_token: bool) -> str:
        string = ""
        for idx in encoded:
            if idx == self._padding_value:
                break
            if not include_stop_token and idx == self._token_to_value_dict[self._stop_token]:
                break
            string += self._selfies_vocab[idx]
        return string
    
    def _last_token_value_batch(self, encoded_sequences: np.ndarray) -> np.ndarray:
        last_token = np.full((encoded_sequences.shape[0],), self._padding_value, dtype=np.int64)
        for i, encoded in enumerate(encoded_sequences):
            try:
                last_token[i] = encoded[encoded != self._padding_value][-1]
            except IndexError:
                last_token[i] = self._padding_value
        return last_token
    
    def _to_one_hot_batch(self, encoded_sequences: np.ndarray) -> np.ndarray:
        one_hot = np.zeros((encoded_sequences.shape[0], encoded_sequences.shape[1], self.vocab_size), dtype=np.int64)
        for i, encoded in enumerate(encoded_sequences):
            for j, idx in enumerate(encoded):
                if idx != self._padding_value:
                    one_hot[i, j, idx] = 1
        return one_hot
    
    def _from_one_hot_batch(self, one_hot: np.ndarray) -> np.ndarray:
        encoded_sequences = np.full((one_hot.shape[0], one_hot.shape[1]), self._padding_value, dtype=np.int64)
        for i, one_hot_seq in enumerate(one_hot):
            for j, token_one_hot in enumerate(one_hot_seq):
                if np.any(token_one_hot):
                    encoded_sequences[i, j] = np.argmax(token_one_hot)
        return encoded_sequences
;;;

envs/selfies_util.py
;;;
# References: https://github.com/aspuru-guzik-group/curiosity/blob/main/selfies_helper.py

def __selfies_to_smiles_derive(selfies,smiles,N_restrict=True):    
    # Elements of start_alphabet, again, stand for integers (see comments in _smiles_to_selfies function for more details)
    start_alphabet=['[#N]','[epsilon]','[Ring1]','[Ring2]','[Branch1_1]','[Branch1_2]','[Branch1_3]','[Branch2_1]','[Branch2_2]','[Branch2_3]','[F]','[O]','[=O]','[N]','[=N]','[#N]','[C]','[=C]','[#C]','[S]','[=S]'];
            
    tmp_ds=selfies.replace('X','Z!') # X will be used as states of the derivation
    
    next_X=smiles.find('X');
    while next_X>=0:
        before_smiles=smiles[0:next_X] # smiles before the non-terminal
        if smiles[next_X+1]=='9':
            state=int(smiles[next_X+1:next_X+5])  # states after branches are called X999...
            after_smiles=smiles[next_X+6:] # smiles after the non-terminal            
        else:            
            state=int(smiles[next_X+1]) # the state is given by the nonterminal symbol X_n, where n=state        
            after_smiles=smiles[next_X+2:] # smiles after the non-terminal
            
        [current_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds) # the current selfies symbol gives the rule vector, and the current state indentifies the one specific, current rule.
        
        # The semantic informations of this set of rules could be significantly extended and more details could be added. Here, we have semantic rules for the most important molecules in organic chemistry, Carbon, Oxygen, Nitrogen, Flour.
        # Other elements get a generic (very weak) restrictions

        if state==0:
            if current_symbol=='[epsilon]':
                new_smiles_symbol='X0'
            elif current_symbol.find('Ring')>=0 or current_symbol.find('Branch')>=0:
                new_smiles_symbol='X0'
                [_,tmp_ds]=_get_next_selfies_symbol(tmp_ds)  # ignore next symbol  
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]X1'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]X1'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]X1'
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X2'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[O]X2'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X3'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[N]X3'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[N]X3'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X4'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[C]X4'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[C]X4'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X6'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[S]X6'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles

        if state==1:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''
            elif current_symbol.find('Branch')>=0:   
                new_smiles_symbol='X1'
                [_,tmp_ds]=_get_next_selfies_symbol(tmp_ds)  # ignore next symbol               
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'
                
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')
            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]                
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'
                    
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[S]X5'
            else:
                new_smiles_symbol=current_symbol+'X6'                
            smiles=before_smiles+new_smiles_symbol+after_smiles

        if state==2:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''              
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]                     
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'
                    
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X1'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X1'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X1'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]                      
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'
                    
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X1'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X1'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X1'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[Branch1_1]' or current_symbol=='[Branch1_2]' or current_symbol=='[Branch1_3]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'

            elif current_symbol=='[Branch2_1]' or current_symbol=='[Branch2_2]' or current_symbol=='[Branch2_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'
                

            elif current_symbol=='[Branch3_1]' or current_symbol=='[Branch3_2]' or current_symbol=='[Branch3_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol

                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'              
                
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles


        if state==3:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''             
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]                  
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'
                    
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X2'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X2'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X2'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]                    
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'
                    
                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X2'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X2'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X2'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[Branch1_1]' or current_symbol=='[Branch1_2]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                
            elif current_symbol=='[Branch1_3]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'             
                
            elif current_symbol=='[Branch2_1]' or current_symbol=='[Branch2_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                
                
            elif current_symbol=='[Branch2_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'
                
                

            elif current_symbol=='[Branch3_1]' or current_symbol=='[Branch3_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                
                
            elif current_symbol=='[Branch3_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'                 
                
                
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[#N]'
                else:
                    new_smiles_symbol='[#N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[#C]X1'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'

            smiles=before_smiles+new_smiles_symbol+after_smiles


        if state==4:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''            
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'                

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X3'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X3'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X3'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                new_smiles_symbol='X4'
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X3'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X3'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X3'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[Branch1_1]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                
            elif current_symbol=='[Branch1_2]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'              
                
            elif current_symbol=='[Branch1_3]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'
                
            elif current_symbol=='[Branch2_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                

            elif current_symbol=='[Branch2_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'            
                
                
            elif current_symbol=='[Branch2_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)   
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'
                
            elif current_symbol=='[Branch3_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                

            elif current_symbol=='[Branch3_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'         
                
                
            elif current_symbol=='[Branch3_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X1'         



            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[#N]'
                else:
                    new_smiles_symbol='[#N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[#C]X1'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles



        if state==5:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''            
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'                

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X4'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X4'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X4'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                new_smiles_symbol='X5'
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X4'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X4'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X4'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[Branch1_1]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'
                
            elif current_symbol=='[Branch1_2]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1

                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'              

            elif current_symbol=='[Branch1_3]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1

                branch_str=''
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                
            elif current_symbol=='[Branch2_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'
                

            elif current_symbol=='[Branch2_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'            
                
                
            elif current_symbol=='[Branch2_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)   
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'
                                
            elif current_symbol=='[Branch3_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'
                
            elif current_symbol=='[Branch3_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'         
                
            elif current_symbol=='[Branch3_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X2'     

                
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[#N]'
                else:
                    new_smiles_symbol='[#N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[#C]X1'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles




        if state==6:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''            
            elif current_symbol.find('Ring1]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    ring_num=str(start_alphabet.index(next_symbol)+2)
                else:
                    ring_num='5'                

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X5'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X5'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X5'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol.find('Ring2]')>=0:
                pre_symbol=''
                if current_symbol[1:5]=='Expl': # Explicit Bond Information
                    pre_symbol=current_symbol[5]
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                new_smiles_symbol='X6'
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    ring_num_1=(start_alphabet.index(next_symbol1)+1)*20
                    ring_num_2=start_alphabet.index(next_symbol2)                
                    ring_num=str(ring_num_1+ring_num_2)
                else:
                    ring_num='5'

                if len(ring_num)==1:
                    new_smiles_symbol=pre_symbol+'%00'+ring_num+'X5'
                elif len(ring_num)==2:
                    new_smiles_symbol=pre_symbol+'%0'+ring_num+'X5'
                elif len(ring_num)==3:
                    new_smiles_symbol=pre_symbol+'%'+ring_num+'X5'
                else:
                    raise ValueError('__selfies_to_smiles_derive: Problem with deriving very long ring.')

            elif current_symbol=='[Branch1_1]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'
                
            elif current_symbol=='[Branch1_2]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1

                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X5'              

            elif current_symbol=='[Branch1_3]':
                [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if next_symbol in start_alphabet:
                    branch_num=start_alphabet.index(next_symbol)+1
                else:
                    branch_num=1

                branch_str=''
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'
                
            elif current_symbol=='[Branch2_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'
                

            elif current_symbol=='[Branch2_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X5'            
                
                
            elif current_symbol=='[Branch2_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)   
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*20
                    branch_num2=start_alphabet.index(next_symbol2)
                    branch_num=branch_num1+branch_num2
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'
                                
            elif current_symbol=='[Branch3_1]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9992',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X4'
                
            elif current_symbol=='[Branch3_2]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9991',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X5'         
                
            elif current_symbol=='[Branch3_3]':
                [next_symbol1,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol2,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                [next_symbol3,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                if (next_symbol1 in start_alphabet) and (next_symbol2 in start_alphabet) and (next_symbol3 in start_alphabet):
                    branch_num1=(start_alphabet.index(next_symbol1)+1)*400
                    branch_num2=(start_alphabet.index(next_symbol2))*20
                    branch_num3=start_alphabet.index(next_symbol3)
                    branch_num=branch_num1+branch_num2+branch_num3
                else:
                    branch_num=1
                
                branch_str=''                
                for bii in range(branch_num):
                    [next_symbol,tmp_ds]=_get_next_selfies_symbol(tmp_ds)
                    branch_str+=next_symbol
                    
                branch_smiles,_=__selfies_to_smiles_derive(branch_str,'X9993',N_restrict)
                new_smiles_symbol=''
                if len(branch_smiles)>0:
                    new_smiles_symbol='('+branch_smiles+')X3'     

                
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X6'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X6'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[#N]'
                else:
                    new_smiles_symbol='[#N]X6'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[#C]X1'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles



        if state==9991: # states 5-7 occure after branches are derived, because a branch or a ring directly after a branch is syntactically illegal.
                     # state  5 corresponds to state 1, state 6 corresponds to state 2, and state 7 corresponds to state 3, without branches & rings
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''
            elif current_symbol.find('Ring')>=0 or current_symbol.find('Branch')>=0: 
                new_smiles_symbol='X9991'
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X4'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X4'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X4'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[S]X5'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles



        if state==9992:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''          
            elif current_symbol.find('Ring')>=0 or current_symbol.find('Branch')>=0: 
                new_smiles_symbol='X9992' 
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X4'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X4'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X4'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'
            smiles=before_smiles+new_smiles_symbol+after_smiles




        if state==9993:
            if current_symbol=='[epsilon]':
                new_smiles_symbol=''        
            elif current_symbol.find('Ring')>=0 or current_symbol.find('Branch')>=0: 
                new_smiles_symbol='X9993'     
            elif current_symbol=='[F]':
                new_smiles_symbol='[F]'
            elif current_symbol=='[Cl]':
                new_smiles_symbol='[Cl]'
            elif current_symbol=='[Br]':
                new_smiles_symbol='[Br]'                  
            elif current_symbol=='[O]':
                new_smiles_symbol='[O]X1'
            elif current_symbol=='[=O]':
                new_smiles_symbol='[=O]'
            elif current_symbol=='[N]':
                if N_restrict:
                    new_smiles_symbol='[N]X2'
                else:
                    new_smiles_symbol='[N]X4'
            elif current_symbol=='[=N]':
                if N_restrict:
                    new_smiles_symbol='[=N]X1'
                else:
                    new_smiles_symbol='[=N]X4'
            elif current_symbol=='[#N]':
                if N_restrict:
                    new_smiles_symbol='[#N]'
                else:
                    new_smiles_symbol='[#N]X4'
            elif current_symbol=='[C]':
                new_smiles_symbol='[C]X3'
            elif current_symbol=='[=C]':
                new_smiles_symbol='[=C]X2'
            elif current_symbol=='[#C]':
                new_smiles_symbol='[#C]X1'
            elif current_symbol=='[S]':
                new_smiles_symbol='[S]X5'
            elif current_symbol=='[=S]':
                new_smiles_symbol='[=S]X4'
            else:
                new_smiles_symbol=current_symbol+'X6'

            smiles=before_smiles+new_smiles_symbol+after_smiles
    
        if len(tmp_ds)<=2: # if all selfies symbols are derived, the final non-terminals are removed
            while True:
                non_terminal=smiles.find('X')
                if non_terminal>=0:
                    if smiles[non_terminal+1]=='9':
                        smiles=smiles[0:non_terminal]+smiles[non_terminal+5:]
                    else:
                        smiles=smiles[0:non_terminal]+smiles[non_terminal+2:]
                else:
                    break;

        next_X=smiles.find('X')
        
    return smiles.replace('Z!','X'), tmp_ds



def _get_next_selfies_symbol(tmp_ds): # get the next selfies symbol
    next_symbol=''
    tmp_ds_new=tmp_ds
    if len(tmp_ds)<=2:
        return [next_symbol, tmp_ds_new]
    
    if tmp_ds[0]!='[':
        raise ValueError('_get_next_selfies_symbol: Decoding Problem 1: '+tmp_ds)
    
    end_of_symbol=tmp_ds.find(']')
    
    if end_of_symbol==-1:
        raise ValueError('_get_next_selfies_symbol: Decoding Problem 2: '+tmp_ds)
    else:
        next_symbol=tmp_ds[0:end_of_symbol+1]
        tmp_ds_new=tmp_ds_new[end_of_symbol+1:]
        
    return [next_symbol, tmp_ds_new]





def is_finished(selfies):
    selfies += '[Q]'
    _,tmp_ds = __selfies_to_smiles_derive(selfies, 'X0')
    
    if len(tmp_ds) > 0:
        return True
    else:
        return False


#selfies = '[P][STOP]'
#print(is_finished(selfies))

;;;

envs/__init__.py
;;;
from .env import Env, AsyncEnv, EnvMakeFunc
;;;

train/factory.py
;;;
# train/factory.py
from dataclasses import dataclass
from typing import Optional, Tuple

import torch
import torch.optim as optim

import drl
import drl.agent as agent # Import agent module
import train.net as train_net # Import train network implementations
import util
from envs import Env
from envs.chem_env import make_async_chem_env
from train.train import Train
from train.pretrain import Pretrain, SelfiesDataset
from util import instance_from_dict
from train.inference import Inference
# from train.net import SelfiesPretrainedNet # Already imported via train_net
import os

class ConfigParsingError(Exception):
    pass

def yaml_to_config_dict(file_path: str) -> Tuple[str, dict]:
    try:
        config_dict = util.load_yaml(file_path)
    except FileNotFoundError:
        raise ConfigParsingError(f"Config file not found: {file_path}")

    try:
        config_id = tuple(config_dict.keys())[0]
    except IndexError: # Changed from generic Exception
        raise ConfigParsingError("YAML config file must start with the training ID.")
    config = config_dict[config_id]
    return config_id, config

@dataclass(frozen=True)
class CommonConfig:
    num_envs: int = 1
    seed: Optional[int] = None
    device: Optional[str] = None
    # lr: float = 1e-3 # LR is now agent-specific (actor_lr, critic_lr, alpha_lr)
    grad_clip_max_norm: float = 5.0 # Keep for potential use, though less common in SAC
    pretrained_path: Optional[str] = None
    num_inference_envs: int = 0

class MolRLTrainFactory:
    """
    Factory class creates a Train instance from a dictionary config.
    """
    @staticmethod
    def from_yaml(file_path: str) -> "MolRLTrainFactory":
        """ Create a `MolRLTrainFactory` from a YAML file. """
        config_id, config = yaml_to_config_dict(file_path)
        return MolRLTrainFactory(config_id, config)

    def __init__(self, id: str, config: dict):
        self._id = id
        self._config = config
        self._agent_config_dict = self._config.get("Agent", {}) # Store raw dict
        self._env_config = self._config.get("Env", {})
        self._train_config = self._config.get("Train", {})
        self._count_int_reward_config = self._config.get("CountIntReward", {})
        # Use train_config for common settings like seed, device etc.
        self._common_config = instance_from_dict(CommonConfig, self._train_config)
        self._pretrained = None

    def create_train(self) -> Train:
        self._train_setup()

        try:
            env = self._create_env()
            inference_env = self._create_inference_env()
        except TypeError as e:
            raise ConfigParsingError(f"Invalid Env config. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Env config: {e}")

        try:
            agent_instance = self._create_agent(env)
        except TypeError as e:
            raise ConfigParsingError(f"Invalid Agent config. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Agent config: {e}")
        except ValueError as e: # Catch agent type error
             raise ConfigParsingError(str(e))


        try:
            smiles_or_selfies_refset = util.load_smiles_or_selfies(self._train_config["refset_path"]) if "refset_path" in self._train_config else None
            # Pass necessary args from train_config to Train constructor
            train_args = {
                "env": env,
                "agent": agent_instance,
                "id": self._id,
                "inference_env": inference_env,
                "smiles_or_selfies_refset": smiles_or_selfies_refset,
                "total_time_steps": self._train_config.get("total_time_steps"),
                "summary_freq": self._train_config.get("summary_freq"),
                "agent_save_freq": self._train_config.get("agent_save_freq"),
                "n_inference_episodes": self._train_config.get("n_inference_episodes", 1) # Default if missing
            }
            # Filter out None values before passing to instance_from_dict
            train_args_filtered = {k: v for k, v in train_args.items() if v is not None}
            train = instance_from_dict(Train, train_args_filtered)

        except TypeError as e:
            raise ConfigParsingError(f"Invalid Train config. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Train config: {e}")

        return train

    def _train_setup(self):
        util.logger.enable(self._id, enable_log_file=False) # Disable file logging initially
        util.try_create_dir(util.logger.dir())
        config_to_save = {self._id: self._config}
        util.save_yaml(f"{util.logger.dir()}/config.yaml", config_to_save)

        if self._common_config.seed is not None:
            util.seed(self._common_config.seed)

        # --- Pretrained Model Loading ---
        pretrained_load_path = self._common_config.pretrained_path
        if pretrained_load_path is None and os.path.exists(f"{util.logger.dir()}/pretrained_models/best.pt"):
             pretrained_load_path = f"{util.logger.dir()}/pretrained_models/best.pt"

        if pretrained_load_path and os.path.exists(pretrained_load_path):
             try:
                 self._pretrained = torch.load(pretrained_load_path, map_location=self._common_config.device or 'cpu')
                 print(f"[Factory] Loaded pretrained model from: {pretrained_load_path}")
             except Exception as e:
                 print(f"[Factory] Warning: Failed to load pretrained model from {pretrained_load_path}: {e}")
                 self._pretrained = None
        elif pretrained_load_path:
             print(f"[Factory] Warning: Pretrained model path specified but not found: {pretrained_load_path}")


        # --- Vocabulary Loading ---
        vocab_load_path = self._env_config.get("vocab_path")
        if vocab_load_path is None and os.path.exists(f"{util.logger.dir()}/vocab.json"):
             vocab_load_path = f"{util.logger.dir()}/vocab.json"

        if vocab_load_path and os.path.exists(vocab_load_path):
            try:
                vocab, max_str_len = util.load_vocab(vocab_load_path)
                self._env_config["vocabulary"] = vocab # Add to env_config for env creation
                self._env_config["max_str_len"] = max_str_len
                print(f"[Factory] Loaded vocabulary from: {vocab_load_path}")
            except Exception as e:
                raise ConfigParsingError(f"Failed to load vocabulary from {vocab_load_path}: {e}")
        elif "vocabulary" not in self._env_config:
             # If no vocab path and not already in config, it might be an issue depending on the env
             print("[Factory] Warning: Vocabulary not found or specified. Environment might fail if it requires one.")


        util.logger.disable() # Disable logger after setup

    def _create_env(self) -> Env:
        # Ensure necessary keys are present before calling make_async_chem_env
        # 'vocabulary' and 'max_str_len' should be added during _train_setup if loaded
        required_keys = [] # Add keys absolutely required by make_async_chem_env
        for key in required_keys:
             if key not in self._env_config:
                  raise ConfigParsingError(f"Missing required key '{key}' in Env config for make_async_chem_env.")

        # Combine env config and count reward config
        full_env_config = {**self._env_config, **self._count_int_reward_config}

        env = make_async_chem_env(
            num_envs=self._common_config.num_envs,
            seed=self._common_config.seed,
            **full_env_config # Pass combined config
        )
        return env

    def _create_inference_env(self) -> Optional[Env]:
        if self._common_config.num_inference_envs <= 0: # Check for > 0
            return None

        # Use the same env_config as the training env, but without count reward config
        inference_env_config = self._env_config.copy()

        env = make_async_chem_env(
            num_envs=self._common_config.num_inference_envs,
            seed=self._common_config.seed, # Use same base seed, async env handles per-env seeding
            **inference_env_config
        )
        return env

    def _create_agent(self, env: Env) -> agent.Agent:
        agent_type = self._agent_config_dict.get("type", "").lower()
        if not agent_type:
             raise ValueError("Agent 'type' not specified in configuration.")

        obs_shape = env.obs_shape
        # Assuming discrete actions for SELFIES, action_shape is usually (1,)
        action_shape = (1,) # Or derive from env if it provides it
        num_actions = env.num_actions

        if agent_type == "ppo":
            return self._create_ppo_agent(env)
        elif agent_type == "rnd": # PPO + RND
            return self._create_rnd_agent(env)
        elif agent_type == "pretrained":
            return self._create_pretrained_agent(env)
        elif agent_type == "sac":
            return self._create_sac_agent(env, obs_shape, action_shape, num_actions)
        else:
            raise ValueError(f"Unknown agent type: {agent_type}")

    def _create_ppo_agent(self, env: Env) -> agent.RecurrentPPO:
        config = instance_from_dict(agent.RecurrentPPOConfig, self._agent_config_dict)
        network = train_net.SelfiesRecurrentPPONet(
            env.obs_shape[0], # Assuming obs_shape[0] is vocab size for one-hot
            env.num_actions
        )
        if self._pretrained is not None and "model" in self._pretrained:
            network.load_state_dict(self._pretrained["model"], strict=False)
            print("[Factory] Loaded pretrained weights into PPO network.")

        # Use default LR from Train config if not specified in Agent config
        lr = self._train_config.get("lr", 1e-3) # Get default LR

        trainer = drl.Trainer(optim.Adam(
            network.parameters(),
            lr=lr
        )).enable_grad_clip(network.parameters(), max_norm=self._common_config.grad_clip_max_norm)

        return agent.RecurrentPPO(
            config=config,
            network=network,
            trainer=trainer,
            num_envs=self._common_config.num_envs,
            device=self._common_config.device
        )

    def _create_rnd_agent(self, env: Env) -> agent.RecurrentPPORND:
        config = instance_from_dict(agent.RecurrentPPORNDConfig, self._agent_config_dict)
        network = train_net.SelfiesRecurrentPPORNDNet(
            env.obs_shape[0],
            env.num_actions
        )
        if self._pretrained is not None and "model" in self._pretrained:
            network.load_state_dict(self._pretrained["model"], strict=False)
            print("[Factory] Loaded pretrained weights into RND network.")

        lr = self._train_config.get("lr", 1e-3)

        trainer = drl.Trainer(optim.Adam(
            network.parameters(),
            lr=lr
        )).enable_grad_clip(network.parameters(), max_norm=self._common_config.grad_clip_max_norm)

        return agent.RecurrentPPORND(
            config=config,
            network=network,
            trainer=trainer,
            num_envs=self._common_config.num_envs,
            device=self._common_config.device
        )

    def _create_pretrained_agent(self, env: Env) -> agent.PretrainedRecurrentAgent:
        # Pretrained agent typically uses a simpler network structure
        network = train_net.SelfiesPretrainedNet(
            env.num_actions, # Assumes input is vocab size
        )
        if self._pretrained is not None and "model" in self._pretrained:
            network.load_state_dict(self._pretrained["model"], strict=False)
            print("[Factory] Loaded pretrained weights into Pretrained network.")
        else:
             print("[Factory] Warning: Pretrained agent type selected, but no pretrained weights found or loaded.")


        return agent.PretrainedRecurrentAgent(
            network=network,
            num_envs=self._common_config.num_envs,
            device=self._common_config.device
        )

    def _create_sac_agent(self, env: Env, obs_shape, action_shape, num_actions) -> agent.RecurrentSAC:
        # Instantiate SAC config from the agent config dictionary
        config = instance_from_dict(agent.RecurrentSACConfig, self._agent_config_dict)

        # Instantiate the specific SAC network implementation
        network = train_net.SelfiesRecurrentSACNet(
            obs_shape=obs_shape[0], # Pass vocab size
            num_actions=num_actions,
            # hidden_dim=self._agent_config_dict.get("hidden_dim", 64), # Example: Get from config or use default
            # n_recurrent_layers=self._agent_config_dict.get("n_recurrent_layers", 2)
        )

        # Load pretrained weights if available (might need careful handling for actor/critic separation)
        if self._pretrained is not None and "model" in self._pretrained:
             # Loading into SAC requires care as architecture differs from PPO/Pretrained
             # Option 1: Load selectively if possible (e.g., only shared backbone)
             # Option 2: Load fully and accept potential mismatches (strict=False)
             try:
                 network.load_state_dict(self._pretrained["model"], strict=False)
                 print("[Factory] Loaded pretrained weights into SAC network (strict=False).")
             except Exception as e:
                  print(f"[Factory] Warning: Failed to load pretrained weights into SAC network: {e}")


        # SAC manages its own optimizers, so no Trainer is passed directly
        return agent.RecurrentSAC(
            config=config,
            network=network,
            num_envs=self._common_config.num_envs,
            obs_shape=obs_shape,
            action_shape=action_shape,
            num_actions=num_actions,
            device=self._common_config.device
        )


class MolRLInferenceFactory:
    """ Factory class creates an Inference instance from a dictionary config. """
    @staticmethod
    def from_yaml(file_path: str) -> "MolRLInferenceFactory":
        """ Create a `MolRLInferenceFactory` from a YAML file. """
        config_id, config = yaml_to_config_dict(file_path)
        return MolRLInferenceFactory(config_id, config)

    def __init__(self, id: str, config: dict):
        self._id = id
        self._config = config
        self._agent_config_dict = self._config.get("Agent", {})
        self._env_config = self._config.get("Env", {})
        # Train config might contain seed/device defaults if not in Inference section
        self._train_config_defaults = self._config.get("Train", {})
        self._inference_config = self._config.get("Inference", {}) # Specific inference settings
        # Prioritize inference config, fall back to train config, then use defaults
        common_seed = self._inference_config.get("seed", self._train_config_defaults.get("seed"))
        common_device = self._inference_config.get("device", self._train_config_defaults.get("device"))
        # Note: num_envs for inference comes from inference_config
        self._common_config = CommonConfig(
             num_envs=self._inference_config.get("num_envs", 1), # Default to 1 if not specified
             seed=common_seed,
             device=common_device
             # Other CommonConfig fields aren't typically needed for inference factory setup
        )
        self._agent_ckpt = None # To store loaded agent state dict

    def create_inference(self) -> Inference:
        self._inference_setup() # Load agent checkpoint here

        try:
            env = self._create_env() # Create env based on inference num_envs
        except TypeError as e:
            raise ConfigParsingError(f"Invalid Env config for inference. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Env config for inference: {e}")

        try:
            # Create the base agent structure first
            agent_instance = self._create_agent_structure(env)
            # Load the checkpoint state into the agent
            if self._agent_ckpt:
                 agent_instance.load_state_dict(self._agent_ckpt)
                 print(f"[Factory] Loaded agent checkpoint state into {agent_instance.name} agent.")
            else:
                 # This should have been caught in _inference_setup, but double-check
                 raise ConfigParsingError("Agent checkpoint not loaded during setup.")

            # Get the specific inference agent instance
            inference_agent = agent_instance.inference_agent(
                 num_envs=env.num_envs, # Use inference env's num_envs
                 device=self._common_config.device
            )

        except TypeError as e:
            raise ConfigParsingError(f"Invalid Agent config for inference. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Agent config for inference: {e}")
        except ValueError as e: # Catch agent type error
             raise ConfigParsingError(str(e))
        except FileNotFoundError as e: # Catch checkpoint loading error
             raise ConfigParsingError(str(e))


        try:
            # Determine reference set path (priority: inference config -> train config)
            refset_path = self._inference_config.get("refset_path", self._train_config_defaults.get("refset_path"))
            smiles_or_selfies_refset = util.load_smiles_or_selfies(refset_path) if refset_path else None

            inference_args = {
                "env": env,
                "agent": inference_agent, # Use the inference-specific agent
                "id": self._id,
                "smiles_or_selfies_refset": smiles_or_selfies_refset,
                "n_episodes": self._inference_config.get("n_episodes"),
                "n_unique_molecules": self._inference_config.get("n_unique_molecules"),
                # Add other inference-specific args from config if needed by Inference class
            }
            inference_args_filtered = {k: v for k, v in inference_args.items() if v is not None}
            inference_instance = instance_from_dict(Inference, inference_args_filtered)

        except TypeError as e:
            raise ConfigParsingError(f"Invalid Inference config. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Inference config: {e}")

        return inference_instance

    def _inference_setup(self):
        """ Loads vocabulary and agent checkpoint. """
        # Setup logger temporarily for finding paths relative to results dir
        util.logger.enable(self._id, enable_log_file=False)
        results_dir = util.logger.dir() # Get results dir path
        util.logger.disable() # Disable after getting path

        if self._common_config.seed is not None:
            util.seed(self._common_config.seed)

        # --- Vocabulary Loading (same as train setup) ---
        vocab_load_path = self._env_config.get("vocab_path")
        if vocab_load_path is None and os.path.exists(f"{results_dir}/vocab.json"):
             vocab_load_path = f"{results_dir}/vocab.json"

        if vocab_load_path and os.path.exists(vocab_load_path):
            try:
                vocab, max_str_len = util.load_vocab(vocab_load_path)
                self._env_config["vocabulary"] = vocab
                self._env_config["max_str_len"] = max_str_len
                print(f"[Factory] Loaded vocabulary for inference from: {vocab_load_path}")
            except Exception as e:
                raise ConfigParsingError(f"Failed to load vocabulary from {vocab_load_path} for inference: {e}")
        elif "vocabulary" not in self._env_config:
             print("[Factory] Warning: Vocabulary not found or specified for inference.")


        # --- Agent Checkpoint Loading ---
        ckpt_spec = self._inference_config.get("ckpt", "best") # Default to 'best'
        ckpt_path = None
        if ckpt_spec == "best":
            ckpt_path = f"{results_dir}/best_agent.pt"
        elif ckpt_spec == "final":
            ckpt_path = f"{results_dir}/agent.pt"
        elif isinstance(ckpt_spec, int):
            ckpt_path = f"{results_dir}/agent_ckpt/agent_{ckpt_spec}.pt"
        elif os.path.isabs(ckpt_spec) and util.file_exists(ckpt_spec): # Allow absolute path
             ckpt_path = ckpt_spec
        elif util.file_exists(os.path.join(results_dir, ckpt_spec)): # Allow relative path within results dir
             ckpt_path = os.path.join(results_dir, ckpt_spec)


        if ckpt_path and os.path.exists(ckpt_path):
            try:
                # Load the entire state dict, including agent sub-dict
                full_state_dict = torch.load(ckpt_path, map_location=self._common_config.device or 'cpu')
                if "agent" in full_state_dict:
                     self._agent_ckpt = full_state_dict["agent"] # Store only the agent's state dict
                     print(f"[Factory] Loaded agent checkpoint from: {ckpt_path}")
                else:
                     raise ConfigParsingError(f"Agent state dict not found within checkpoint file: {ckpt_path}")

            except FileNotFoundError:
                 raise ConfigParsingError(f"Specified checkpoint file not found: {ckpt_path}")
            except Exception as e:
                 raise ConfigParsingError(f"Error loading checkpoint from {ckpt_path}: {e}")
        else:
            raise ConfigParsingError(f"Could not find or determine checkpoint path from spec: '{ckpt_spec}' in directory {results_dir}")


    def _create_env(self) -> Env:
        """ Creates environment for inference using inference num_envs """
        # Inference env should not use count reward intrinsically
        inference_env_config = self._env_config.copy()

        env = make_async_chem_env(
            num_envs=self._common_config.num_envs, # Use inference num_envs
            seed=self._common_config.seed,
            **inference_env_config
        )
        return env

    def _create_agent_structure(self, env: Env) -> agent.Agent:
        """ Creates the agent object structure *without* loading the state dict. """
        # This logic is similar to _create_agent in MolRLTrainFactory,
        # but it just sets up the object. The state is loaded later.
        agent_type = self._agent_config_dict.get("type", "").lower()
        if not agent_type:
             raise ValueError("Agent 'type' not specified in configuration.")

        obs_shape = env.obs_shape
        action_shape = (1,) # Assume discrete
        num_actions = env.num_actions

        # We don't need trainers or optimizers for inference structure creation
        dummy_trainer = drl.Trainer(optim.Adam([torch.nn.Parameter(torch.empty(1))], lr=1e-4)) # Dummy optimizer

        if agent_type == "ppo":
            config = instance_from_dict(agent.RecurrentPPOConfig, self._agent_config_dict)
            network = train_net.SelfiesRecurrentPPONet(obs_shape[0], num_actions)
            # Pass dummy trainer
            return agent.RecurrentPPO(config, network, dummy_trainer, env.num_envs, self._common_config.device)
        elif agent_type == "rnd":
            config = instance_from_dict(agent.RecurrentPPORNDConfig, self._agent_config_dict)
            network = train_net.SelfiesRecurrentPPORNDNet(obs_shape[0], num_actions)
            # Pass dummy trainer
            return agent.RecurrentPPORND(config, network, dummy_trainer, env.num_envs, self._common_config.device)
        elif agent_type == "pretrained":
            network = train_net.SelfiesPretrainedNet(num_actions)
            return agent.PretrainedRecurrentAgent(network, env.num_envs, self._common_config.device)
        elif agent_type == "sac":
            config = instance_from_dict(agent.RecurrentSACConfig, self._agent_config_dict)
            network = train_net.SelfiesRecurrentSACNet(obs_shape[0], num_actions)
            # SAC agent constructor doesn't need trainer, manages optimizers internally
            return agent.RecurrentSAC(config, network, env.num_envs, obs_shape, action_shape, num_actions, self._common_config.device)
        else:
            raise ValueError(f"Unknown agent type for inference structure: {agent_type}")


class MolRLPretrainFactory:
    @staticmethod
    def from_yaml(file_path: str) -> "MolRLPretrainFactory":
        """ Create a `MolRLPretrainFactory` from a YAML file. """
        config_id, config = yaml_to_config_dict(file_path)
        return MolRLPretrainFactory(config_id, config)

    def __init__(self, id: str, config: dict):
        self._id = id
        self._config = config
        self._pretrain_config = self._config.get("Pretrain", {})
        # Pretrain might have its own seed/device settings
        self._common_config = instance_from_dict(CommonConfig, self._pretrain_config)

    def create_pretrain(self) -> Pretrain:
        self._pretrain_setup()

        try:
            # Dataset path must be in Pretrain config
            dataset_path = self._pretrain_config.get("dataset_path")
            if not dataset_path:
                 raise ConfigParsingError("Missing 'dataset_path' in Pretrain config.")
            dataset = SelfiesDataset.from_txt(dataset_path)
        except FileNotFoundError:
             raise ConfigParsingError(f"Dataset file not found: {dataset_path}")
        except Exception as e:
            raise ConfigParsingError(f"Error creating dataset: {e}")

        try:
            # Network uses vocab size from the loaded dataset
            net = train_net.SelfiesPretrainedNet(vocab_size=dataset.tokenizer.vocab_size)
        except Exception as e:
            raise ConfigParsingError(f"Error creating pretrain network: {e}")

        try:
            # Combine pretrain config with necessary created objects
            pretrain_args = {
                "id": self._id,
                "net": net,
                "dataset": dataset,
                **self._pretrain_config, # Pass all other pretrain settings
            }
            # Filter None args if Pretrain constructor expects specific types
            pretrain_args_filtered = {k: v for k, v in pretrain_args.items() if v is not None}
            pretrain = instance_from_dict(Pretrain, pretrain_args_filtered)

        except TypeError as e:
            raise ConfigParsingError(f"Invalid Pretrain config. Missing arguments or wrong type: {e}")
        except KeyError as e:
             raise ConfigParsingError(f"Missing required key in Pretrain config: {e}")

        return pretrain

    def _pretrain_setup(self):
        util.logger.enable(self._id, enable_log_file=False) # Disable file logging initially
        util.try_create_dir(util.logger.dir())
        config_to_save = {self._id: self._config}
        util.save_yaml(f"{util.logger.dir()}/config.yaml", config_to_save)
        # Store log dir if needed later, e.g., for saving vocab
        self._log_dir = util.logger.dir()
        util.logger.disable() # Disable logger after setup

        if "seed" in self._pretrain_config: # Use seed from Pretrain config if present
            util.seed(self._pretrain_config["seed"])

;;;

train/inference.py
;;;
from collections import defaultdict
from typing import List, Optional

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm

import drl
from drl.agent import Agent
from envs import Env
from metric import MolMetric, canonicalize
from util import draw_molecules, logger, to_smiles, try_create_dir


class Inference:
    def __init__(
        self,
        id: str,
        env: Env,
        agent: Agent,
        n_episodes: int = 1,
        n_unique_molecules: Optional[int] = None,
        smiles_or_selfies_refset: Optional[List[str]] = None,
    ):
        self._id = id
        self._env = env
        self._agent = agent
        self._n_episodes = n_episodes
        self._n_unique_molecules = n_unique_molecules
        self._canonical_smiles_set = set()
        self._device = agent.device
        self._smiles_or_selfies_refset = smiles_or_selfies_refset
        
        self._dtype = torch.float32
        
        self._enabled = True
    
    def inference(self) -> "Inference":
        if not self._enabled:
            raise RuntimeError("Inference is already closed.")        

        logger.enable(self._id, enable_log_file=False)
        logger.print(f"Inference started (ID: {self._id}).")
        
        episodes = np.zeros((self._env.num_envs,), dtype=int)
        metric_list_dict = defaultdict(list)
        
        self._agent.model.eval()
        
        obs = self._env.reset()
        
        pbar = tqdm(total=self._n_episodes, desc="Episode", leave=True)
        if self._n_unique_molecules is not None:
            unique_molecule_pbar = tqdm(total=self._n_unique_molecules, position=1, desc="Unique Molecule", leave=True)
        else:
            unique_molecule_pbar = None
        
        while np.sum(episodes) < self._n_episodes:
            obs = self._numpy_to_tensor(obs)
            with torch.no_grad():
                action = self._agent.select_action(obs)
            next_obs, reward, terminated, real_final_next_obs, env_info = self._env.step(action.detach().cpu().numpy())
            
            # update the agent
            real_next_obs = next_obs.copy()
            real_next_obs[terminated] = real_final_next_obs
            real_next_obs = self._numpy_to_tensor(real_next_obs)
            
            exp = drl.Experience(
                obs,
                action,
                real_next_obs,
                self._numpy_to_tensor(reward[..., np.newaxis]),
                self._numpy_to_tensor(terminated[..., np.newaxis]),
            )
            with torch.no_grad():
                _ = self._agent.update(exp)
                            
            # update metrics
            for key, value in self._inference_metric(env_info).items():
                metric_list_dict[key].extend(value)
                
                if unique_molecule_pbar is not None and key == "smiles":
                    self._update_n_unique_molecules(value)
                        
            # take the next step       
            obs = next_obs
            episodes += terminated.astype(int)
            n_terminated = np.sum(terminated)
            if pbar.n + n_terminated > pbar.total:
                pbar.total = pbar.n + n_terminated
                pbar.refresh()
            pbar.update(n_terminated)
            
            # update unique molecule progress bar
            if unique_molecule_pbar is not None:
                n_unique_molecules = self._update_n_unique_molecules()
                if n_unique_molecules > unique_molecule_pbar.total:
                    unique_molecule_pbar.total = n_unique_molecules
                    unique_molecule_pbar.refresh()
                unique_molecule_pbar.update(n_unique_molecules - unique_molecule_pbar.n)
                if n_unique_molecules >= self._n_unique_molecules: # type: ignore
                    break
        
        elapsed_time = pbar.format_dict["elapsed"]
        
        pbar.close()
        if unique_molecule_pbar is not None:
            unique_molecule_pbar.close()
            
        original_metric_df = pd.DataFrame(metric_list_dict)
        original_metric_df = original_metric_df[:self._n_episodes]
        n_total = len(original_metric_df)
        metric_df = original_metric_df.dropna()
        n_valid = len(metric_df)
        if n_valid == 0:
            logger.print("No valid molecule is generated.")
            return self
        
        avg_scores = metric_df.mean(numeric_only=True)
            
        smiles_list = metric_df["smiles"].tolist()
        if self._smiles_or_selfies_refset is not None:
            logger.print(f"Calculating molecular metrics with the reference set ({len(self._smiles_or_selfies_refset)}) and the generated set ({n_valid})...")
            smiles_refset = to_smiles(self._smiles_or_selfies_refset)
            mol_metric = MolMetric().preprocess(smiles_refset=smiles_refset, smiles_generated=smiles_list)
        else:
            logger.print(f"Calculating molecular metrics with the generated set ({n_valid})...")
            mol_metric = MolMetric().preprocess(smiles_generated=smiles_list)
        try:
            avg_scores["diversity"] = mol_metric.calc_diversity()
            avg_scores["uniqueness"] = mol_metric.calc_uniqueness()
            avg_scores["novelty"] = mol_metric.calc_novelty()
        except ValueError:
            pass
        
        try_create_dir(f"{logger.dir()}/inference")
        original_metric_df.to_csv(f"{logger.dir()}/inference/molecules.csv", index=False)
        
        avg_scores = pd.concat([pd.Series([n_total, n_valid, elapsed_time], index=["n_total", "n_valid", "time"]), avg_scores])
        avg_scores.index.name = "Metric"
        avg_scores.name = "Score"
        avg_scores.to_csv(f"{logger.dir()}/inference/metrics.csv", header=True)
        
        logger.print("===== Inference Result =====")
        logger.print(avg_scores.to_frame().T.to_string(index=False), prefix="")
        
        best_i = metric_df["score"].argmax()
        best_row = metric_df.iloc[best_i:best_i+1]
        logger.print("===== Best Molecule =====")
        best_scores = best_row.mean(numeric_only=True)
        logger.print(best_scores.to_frame().T.to_string(index=False), prefix="")
        
        best_row = best_row.iloc[0]
        best_row.index.name = "Metric"
        best_row.name = "Score"
        best_row.to_csv(f"{logger.dir()}/inference/best_molecule.csv", header=True)
        
        # draw top-50 molecules
        top_50_df = metric_df.drop_duplicates("smiles").sort_values("score", ascending=False).head(50)
        try:
            draw_molecules(top_50_df["smiles"].tolist(), top_50_df["score"].tolist()).save(f"{logger.dir()}/inference/top_50_unique_molecules.png")
        except ImportError as e:
            logger.print(str(e))
        
        return self
    
    def close(self):
        self._enabled = False
        self._env.close()
        
        if logger.enabled():
            logger.disable()
    
    def _inference_metric(self, env_info: dict):
        metric_list_dict = defaultdict(list)
        
        if "metric" not in env_info:
            return metric_list_dict
        
        metric_dicts = env_info["metric"]
        
        for metric_dict in metric_dicts:
            if metric_dict is None:
                continue
        
            if "episode_metric" not in metric_dict:
                continue
            
            for key, value in metric_dict["episode_metric"]["values"].items():
                metric_list_dict[key].append(value)
            
        return metric_list_dict
    
    def _agent_tensor(self, x: torch.Tensor) -> torch.Tensor:
        return x.to(device=self._device, dtype=self._dtype)
    
    def _numpy_to_tensor(self, x: np.ndarray) -> torch.Tensor:
        return torch.from_numpy(x).to(device=self._device, dtype=self._dtype)
    
    def _update_n_unique_molecules(self, new_smiles_list: Optional[List[str]] = None) -> int:
        if new_smiles_list is None:
            return len(self._canonical_smiles_set)
        
        canonical_smiles = canonicalize(new_smiles_list)
        self._canonical_smiles_set.update(canonical_smiles)
        return len(self._canonical_smiles_set)
;;;

train/net.py
;;;
# train/net.py
from typing import Tuple, Iterable, Optional # <<< Added Optional import here

import torch
import torch.nn as nn
import torch.nn.functional as F # <<< Added F import
import torch.nn.init as init

import drl.agent as agent_interfaces # Use interfaces from drl.agent.net
import drl.net as drl_net_base # Base classes like wrap/unwrap LSTM
from drl.policy import CategoricalPolicy
from drl.policy_dist import CategoricalDist

# --- Helper Functions ---
def init_linear_weights(model: nn.Module, gain: float = 2.0**0.5) -> None:
    """Initialize linear layers with orthogonal initialization."""
    for layer in model.modules():
        if isinstance(layer, nn.Linear):
            init.orthogonal_(layer.weight, gain)
            if layer.bias is not None:
                layer.bias.data.zero_()

# --- Shared Recurrent Backbone (Example using LSTM) ---
class SelfiesRecurrentSharedNet(nn.Module):
    """ A shared LSTM backbone for actor and critic networks. """
    def __init__(self, in_features: int, hidden_dim: int = 64, n_recurrent_layers: int = 2):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.n_recurrent_layers = n_recurrent_layers
        # Calculate combined hidden state dim for LSTM (H_cell + H_out)
        # Assuming H_cell = H_out = hidden_dim
        self.lstm_hidden_dim = hidden_dim * 2

        # Input embedding layer (e.g., for one-hot encoded tokens)
        # Maps input features (vocab size) to the LSTM's expected input dimension
        self.input_embedding = nn.Linear(in_features, hidden_dim)

        self.recurrent_layer = nn.LSTM(
            input_size=hidden_dim, # Input to LSTM is embedded features
            hidden_size=hidden_dim, # H_out size
            batch_first=True,       # Input/output tensors are (batch, seq, feature)
            num_layers=n_recurrent_layers
        )
        # Output features after LSTM will match hidden_dim (H_out)
        self.out_features = hidden_dim

    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the shared recurrent backbone.

        Args:
            x (torch.Tensor): Input sequence (batch_size, seq_len, in_features).
            hidden_state (torch.Tensor): LSTM hidden state ((num_layers * D, batch_size, H_cell), (num_layers * D, batch_size, H_out)).
                                         Or wrapped as (num_layers * D, batch_size, H_cell + H_out).

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - Output sequence from LSTM (batch_size, seq_len, hidden_dim).
                - Next LSTM hidden state ((num_layers * D, batch_size, H_cell), (num_layers * D, batch_size, H_out)) wrapped.
        """
        # Apply input embedding
        # Input x shape: (batch_size, seq_len, in_features)
        embedded_seq = F.relu(self.input_embedding(x)) # Shape: (batch_size, seq_len, hidden_dim)

        # Feed forward to the recurrent layers
        # Unwrap the combined hidden state into h_0 and c_0 for LSTM
        h, c = drl_net_base.unwrap_lstm_hidden_state(hidden_state, h_size=self.hidden_dim, c_size=self.hidden_dim)
        # LSTM expects hidden state tuple: (h_0, c_0)
        # h_0 shape: (num_layers * D, batch_size, H_out)
        # c_0 shape: (num_layers * D, batch_size, H_cell)
        lstm_out_seq, (h_n, c_n) = self.recurrent_layer(embedded_seq, (h, c))
        # lstm_out_seq shape: (batch_size, seq_len, hidden_dim)

        # Wrap the next hidden state (h_n, c_n) back into a single tensor
        next_seq_hidden_state = drl_net_base.wrap_lstm_hidden_state(h_n, c_n)
        # next_seq_hidden_state shape: (num_layers * D, batch_size, H_cell + H_out)

        return lstm_out_seq, next_seq_hidden_state

    def hidden_state_shape(self) -> Tuple[int, int]:
         """ Returns (num_layers * D, H_cell + H_out) """
         # Assuming D=1 (unidirectional LSTM)
         return (self.n_recurrent_layers, self.lstm_hidden_dim)

# --- PPO/RND Networks (Keep them) ---
# Renamed internal class to avoid potential conflicts if used elsewhere
class SelfiesRecurrentPPOSharedNetInternal(nn.Module):
    def __init__(self, in_features: int) -> None:
        super().__init__()
        self.hidden_state_dim = 64 * 2
        self.n_recurrent_layers = 2
        self.out_features = 256
        self.recurrent_layers = nn.LSTM(in_features, self.hidden_state_dim // 2, batch_first=True, num_layers=self.n_recurrent_layers)
        self.linear_layers = nn.Sequential(
            nn.Linear(self.hidden_state_dim // 2, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, self.out_features), nn.ReLU()
        )
    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        h, c = drl_net_base.unwrap_lstm_hidden_state(hidden_state)
        embedding_seq, (h_n, c_n) = self.recurrent_layers(x, (h, c))
        next_seq_hidden_state = drl_net_base.wrap_lstm_hidden_state(h_n, c_n)
        embedding_seq = self.linear_layers(embedding_seq)
        return embedding_seq, next_seq_hidden_state
    def hidden_state_shape(self) -> Tuple[int, int]:
        # Assuming D=1
        return (self.n_recurrent_layers, self.hidden_state_dim)


class SelfiesEmbeddedConcatRND(nn.Module):
    def __init__(self, obs_features, hidden_state_shape) -> None:
        super().__init__()
        # hidden_state_shape is expected as (num_layers * D, H)
        hidden_state_features = hidden_state_shape[0] * hidden_state_shape[1]
        self._predictor_obs_embedding = nn.Sequential(nn.Linear(obs_features, 64), nn.ReLU())
        self._predictor = nn.Sequential(
            nn.Linear(64 + hidden_state_features, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, 256)
        )
        self._target_obs_embedding = nn.Sequential(nn.Linear(obs_features, 64), nn.ReLU())
        self._target = nn.Sequential(
            nn.Linear(64 + hidden_state_features, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, 256)
        )
        for param in self._target_obs_embedding.parameters(): param.requires_grad = False
        for param in self._target.parameters(): param.requires_grad = False
    def forward(self, obs: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # hidden_state input shape: (batch_size, num_layers * D * H) - needs flattening before concat
        hidden_state_flat = hidden_state.flatten(1)
        obs_embedding_pred = self._predictor_obs_embedding(obs)
        predicted_features = self._predictor(torch.cat([obs_embedding_pred, hidden_state_flat], dim=1))

        obs_embedding_target = self._target_obs_embedding(obs)
        target_features = self._target(torch.cat([obs_embedding_target, hidden_state_flat], dim=1))
        return predicted_features, target_features


class SelfiesPretrainedNet(nn.Module, agent_interfaces.PretrainedRecurrentNetwork):
     def __init__(self, vocab_size: int) -> None:
        super().__init__()
        # Use the internal PPO shared net structure for pretraining
        self._shared_net = SelfiesRecurrentPPOSharedNetInternal(vocab_size)
        self._actor = CategoricalPolicy(self._shared_net.out_features, vocab_size)
        init_linear_weights(self) # Initialize weights

     def forward(self, x: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[CategoricalDist, torch.Tensor]:
        # Pass through shared backbone
        embedding_seq, next_seq_hidden_state = self._shared_net(x, hidden_state)
        # Pass features through actor head
        policy_dist_seq = self._actor(embedding_seq)
        return policy_dist_seq, next_seq_hidden_state

     def hidden_state_shape(self) -> Tuple[int, int]:
        return self._shared_net.hidden_state_shape()

     def model(self) -> nn.Module:
        return self


class SelfiesRecurrentPPONet(nn.Module, agent_interfaces.RecurrentPPONetwork):
     def __init__(self, in_features: int, num_actions: int) -> None:
        super().__init__()
        # Use the internal PPO shared net structure
        self._actor_critic_shared_net = SelfiesRecurrentPPOSharedNetInternal(in_features)
        self._actor = CategoricalPolicy(self._actor_critic_shared_net.out_features, num_actions)
        self._critic = nn.Linear(self._actor_critic_shared_net.out_features, 1) # Critic head
        init_linear_weights(self) # Initialize weights

     def model(self) -> nn.Module:
        return self

     def hidden_state_shape(self) -> Tuple[int, int]:
        return self._actor_critic_shared_net.hidden_state_shape()

     def forward(self, obs_seq: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[CategoricalDist, torch.Tensor, torch.Tensor]:
        # Pass through shared backbone
        embedding_seq, next_seq_hidden_state = self._actor_critic_shared_net(obs_seq, hidden_state)
        # Pass features through actor and critic heads
        policy_dist_seq = self._actor(embedding_seq)
        state_value_seq = self._critic(embedding_seq)
        return policy_dist_seq, state_value_seq, next_seq_hidden_state


class SelfiesRecurrentPPORNDNet(nn.Module, agent_interfaces.RecurrentPPORNDNetwork):
     def __init__(self, in_features: int, num_actions: int, temperature: float = 1.0) -> None:
        super().__init__()
        # Use the internal PPO shared net structure
        self._actor_critic_shared_net = SelfiesRecurrentPPOSharedNetInternal(in_features)
        self._actor = CategoricalPolicy(self._actor_critic_shared_net.out_features, num_actions, temperature=temperature)
        # Separate critics for extrinsic and intrinsic values
        self._ext_critic = nn.Linear(self._actor_critic_shared_net.out_features, 1)
        self._int_critic = nn.Linear(self._actor_critic_shared_net.out_features, 1)
        # RND predictor/target networks
        self._rnd_net = SelfiesEmbeddedConcatRND(in_features, self.hidden_state_shape())
        init_linear_weights(self) # Initialize weights

     def model(self) -> nn.Module:
        return self

     def hidden_state_shape(self) -> Tuple[int, int]:
        return self._actor_critic_shared_net.hidden_state_shape()

     def forward_actor_critic(self, obs_seq: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[CategoricalDist, torch.Tensor, torch.Tensor, torch.Tensor]:
        # Pass through shared backbone
        embedding_seq, next_seq_hidden_state = self._actor_critic_shared_net(obs_seq, hidden_state)
        # Pass features through actor and critic heads
        policy_dist_seq = self._actor(embedding_seq)
        ext_state_value_seq = self._ext_critic(embedding_seq)
        int_state_value_seq = self._int_critic(embedding_seq)
        return policy_dist_seq, ext_state_value_seq, int_state_value_seq, next_seq_hidden_state

     def forward_rnd(self, obs: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # Note: hidden_state here is expected to be flattened for RND network input
        return self._rnd_net(obs, hidden_state)


# --- New SAC Network Implementation ---
class SelfiesRecurrentSACNet(nn.Module, agent_interfaces.RecurrentSACNetwork):
    """
    Recurrent SAC Network implementation for SELFIES environment.
    Uses a shared LSTM backbone.
    """
    def __init__(self, obs_shape: int, num_actions: int, hidden_dim: int = 64, n_recurrent_layers: int = 2, temperature: float = 1.0):
        """
        Args:
            obs_shape (int): The size of the vocabulary (input features are one-hot encoded).
            num_actions (int): The number of discrete actions (vocabulary size).
            hidden_dim (int): The hidden dimension for LSTM and linear layers.
            n_recurrent_layers (int): The number of LSTM layers.
            temperature (float): Temperature for policy sampling (can be used in CategoricalPolicy).
        """
        super().__init__()
        self._obs_shape_val = obs_shape # Store vocab size for embedding
        self._num_actions = num_actions
        self._hidden_dim = hidden_dim
        self._n_recurrent_layers = n_recurrent_layers

        # Shared recurrent backbone
        self._shared_net = SelfiesRecurrentSharedNet(
            in_features=obs_shape, # Input is one-hot vocab size
            hidden_dim=hidden_dim,
            n_recurrent_layers=n_recurrent_layers
        )

        # --- Actor Head ---
        # Takes features from shared net and outputs action distribution logits
        self._actor_head = CategoricalPolicy(
            in_features=self._shared_net.out_features,
            num_discrete_actions=num_actions,
            temperature=temperature # Pass temperature if needed
        )

        # --- Critic Heads (Two Q-networks) ---
        # Takes features from shared net AND action embedding, outputs Q-values
        # Embed the discrete action to concatenate it with the LSTM output features
        self._action_embedding_dim = 32 # Example dimension for action embedding
        self._action_embed = nn.Embedding(num_embeddings=num_actions, embedding_dim=self._action_embedding_dim)

        # Input dimension for the critic heads
        critic_input_dim = self._shared_net.out_features + self._action_embedding_dim

        # Define the two critic heads
        self._critic1_head = nn.Sequential(
            nn.Linear(critic_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1) # Output single Q-value
        )
        self._critic2_head = nn.Sequential(
            nn.Linear(critic_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1) # Output single Q-value
        )

        # Initialize weights for all layers in this network
        init_linear_weights(self)

    def model(self) -> nn.Module:
        """Returns the entire network module."""
        return self

    def hidden_state_shape(self) -> Tuple[int, int]:
        """ Returns the shape required by the recurrent layer (LSTM). """
        # Shape is (num_layers * D, H_cell + H_out)
        return self._shared_net.hidden_state_shape()

    def forward_actor(
        self,
        obs_seq: torch.Tensor,
        hidden_state: Optional[torch.Tensor] = None # Make hidden_state optional for flexibility
    ) -> Tuple[CategoricalDist, torch.Tensor]:
        """ Actor forward pass. """
        if hidden_state is None:
             # Initialize hidden state if not provided (e.g., for batch size > 1 at step 0)
             batch_size = obs_seq.size(0)
             h_shape = self.hidden_state_shape()
             # Ensure hidden state is created on the correct device
             hidden_state = torch.zeros(h_shape[0], batch_size, h_shape[1], device=obs_seq.device)

        # Pass through shared backbone
        # obs_seq shape: (batch_size, seq_len, obs_shape)
        shared_features_seq, next_hidden_state = self._shared_net(obs_seq, hidden_state)
        # shared_features_seq shape: (batch_size, seq_len, hidden_dim)

        # Pass features through actor head
        policy_dist_seq = self._actor_head(shared_features_seq)

        return policy_dist_seq, next_hidden_state

    def forward_critic(
        self,
        obs_seq: torch.Tensor,
        action_seq: torch.Tensor,
        hidden_state: Optional[torch.Tensor] = None # Make hidden_state optional
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ Critic forward pass. """
        if hidden_state is None:
             # Initialize hidden state if not provided
             batch_size = obs_seq.size(0)
             h_shape = self.hidden_state_shape()
              # Ensure hidden state is created on the correct device
             hidden_state = torch.zeros(h_shape[0], batch_size, h_shape[1], device=obs_seq.device)

        # Pass observations through shared backbone
        shared_features_seq, next_hidden_state = self._shared_net(obs_seq, hidden_state)
        # shared_features_seq shape: (batch_size, seq_len, hidden_dim)

        # Embed actions
        # action_seq shape: (batch_size, seq_len, 1) or (batch_size, seq_len)
        # Ensure action_seq is LongTensor and remove trailing dim if present before embedding
        action_seq_long = action_seq.squeeze(-1).long()
        action_features_seq = self._action_embed(action_seq_long)
        # action_features_seq shape: (batch_size, seq_len, action_embedding_dim)

        # Concatenate shared features and action features along the last dimension
        critic_input_features = torch.cat([shared_features_seq, action_features_seq], dim=-1)
        # critic_input_features shape: (batch_size, seq_len, hidden_dim + action_embedding_dim)

        # Pass through critic heads
        q1_values_seq = self._critic1_head(critic_input_features) # Shape: (batch_size, seq_len, 1)
        q2_values_seq = self._critic2_head(critic_input_features) # Shape: (batch_size, seq_len, 1)

        return q1_values_seq, q2_values_seq, next_hidden_state

    def actor_parameters(self) -> Iterable[torch.nn.Parameter]:
        """ Return parameters of the actor components (shared net + actor head). """
        # Parameters from the shared backbone and the actor-specific head
        return list(self._shared_net.parameters()) + list(self._actor_head.parameters())

    def critic_parameters(self) -> Iterable[torch.nn.Parameter]:
        """ Return parameters of the critic components (shared net + action embed + critic heads). """
        # Note: Shared net params are included here too. Optimizers handle duplicates correctly.
        # Parameters from the shared backbone, action embedding layer, and both critic heads
        return list(self._shared_net.parameters()) + list(self._action_embed.parameters()) + \
               list(self._critic1_head.parameters()) + list(self._critic2_head.parameters())


;;;

train/pretrain.py
;;;
from typing import List, Optional

import selfies as sf
import torch
from torch.nn.functional import cross_entropy
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm

from drl.agent import PretrainedRecurrentAgent, PretrainedRecurrentNetwork
from envs.chem_env import make_async_chem_env
from envs.selfies_tokenizer import SelfiesTokenizer
from train.train import Train
from util import (load_smiles_or_selfies, load_vocab, logger, save_vocab,
                  to_selfies, try_create_dir)


class SelfiesDataset(Dataset):
    def __init__(self, selfies_list: List[str]):
        self.tokenizer = SelfiesTokenizer(vocabulary=sf.get_alphabet_from_selfies(selfies_list))
        self.encoded_sequences = self.tokenizer.encode(selfies_list, include_stop_token=True)
        
    def __len__(self):
        return len(self.encoded_sequences)
    
    def __getitem__(self, idx):
        one_hot = self.tokenizer.to_one_hot(self.encoded_sequences[idx])
        one_hot = torch.from_numpy(one_hot).float()
        return one_hot[:-1], one_hot[1:]
    
    @staticmethod
    def from_txt(file_path: str, auto_convert: bool = True) -> "SelfiesDataset":
        smiles_or_selfies_list = load_smiles_or_selfies(file_path)
        selfies_list = to_selfies(smiles_or_selfies_list) if auto_convert else smiles_or_selfies_list
        return SelfiesDataset(selfies_list)

class Pretrain:
    def __init__(
        self,
        id: str,
        net: PretrainedRecurrentNetwork,
        dataset: SelfiesDataset,
        epoch: int = 50,
        batch_size: int = 256,
        lr: float = 1e-3,
        device: Optional[str] = None,
        save_agent: bool = True
    ) -> None:
        self._id = id
        self._net = net
        self._dataset = dataset
        self._epoch = epoch
        self._batch_size = batch_size
        self._lr = lr
        self._device = torch.device(device) if device is not None else torch.device("cpu")
        self._save_agent = save_agent
        
        self._enabled = True
        
    def pretrain(self) -> "Pretrain":
        if not self._enabled:
            raise RuntimeError("Pretrain is already closed.")       
        
        logger.enable(self._id, enable_log_file=True)

        train_size = int(0.9 * len(self._dataset))
        val_size = len(self._dataset) - train_size
        train_dataset, val_dataset = random_split(self._dataset, [train_size, val_size])
        
        train_dataloader = DataLoader(train_dataset, batch_size=self._batch_size, shuffle=True)
        val_dataloader = DataLoader(val_dataset, batch_size=self._batch_size, shuffle=False)
        save_vocab(self._dataset.tokenizer.vocabulary, self._dataset.encoded_sequences.shape[1] - 1, f"{logger.dir()}/vocab.json")
        
        self._net.model().to(self._device)
        optimizer = torch.optim.Adam(self._net.model().parameters(), lr=self._lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self._epoch)
        
        logger.print("SELFIES Pretraining start!")
        
        best_val_loss = float('inf')
        H_shape = self._net.hidden_state_shape()
        
        for e in range(self._epoch):
            self._net.model().train()
            losses = []
            for X, Y in tqdm(train_dataloader, desc=f"Epoch {e+1}/{self._epoch}"):
                X, Y = X.to(self._device), Y.to(self._device)
                mask = Y.sum(dim=-1) > 0
                dist, _ = self._net.forward(X, torch.zeros(H_shape[0], X.size(0), H_shape[1]).to(self._device))
                logits = dist._dist.logits[mask] # type: ignore
                target = Y[mask].argmax(dim=-1)
                loss = cross_entropy(logits, target)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                losses.append(loss.item())
            
            avg_train_loss = sum(losses) / len(losses)
            avg_val_loss = self._evaluate(val_dataloader)

            logger.print(f"Epoch {e+1}/{self._epoch} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
            logger.log_data("Pretrain/Train Loss", avg_train_loss, e)
            logger.log_data("Pretrain/Val Loss", avg_val_loss, e)

            # Save checkpoint if validation loss improves
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                self._save_pretrain("best.pt")

            # Update learning rate scheduler and record learning rate
            scheduler.step()
            logger.log_data("Pretrain/Learning Rate", optimizer.param_groups[0]['lr'], e)
            
            logger.plot_logs()
            
        self._save_pretrain("final.pt")
        self._save_pretrained_agent()
        
        return self
        
    def close(self) -> None:
        self._enabled = False
        
        if logger.enabled():
            logger.disable()
            
    def _evaluate(self, dataloader):
        self._net.model().eval()
        losses = []
        H_shape = self._net.hidden_state_shape()
        with torch.no_grad():
            for X, Y in tqdm(dataloader, desc="Validation"):
                X, Y = X.to(self._device), Y.to(self._device)
                mask = Y.sum(dim=-1) > 0
                dist, _ = self._net.forward(X, torch.zeros(H_shape[0], X.size(0), H_shape[1]).to(self._device))
                logits = dist._dist.logits[mask] # type: ignore
                target = Y[mask].argmax(dim=-1)
                loss = cross_entropy(logits, target)
                losses.append(loss.item())
        return sum(losses) / len(losses)
    
    def _save_pretrain(self, file_name):
        try_create_dir(f"{logger.dir()}/pretrained_models")
        state_dict = {
            'model': self._net.model().state_dict(),
            # 'vocabulary': self._dataset.tokenizer.vocabulary,
        }
        torch.save(state_dict, f"{logger.dir()}/pretrained_models/{file_name}")
        
    def _save_pretrained_agent(self):
        if not self._save_agent:
            return
        
        state_dict = torch.load(f"{logger.dir()}/pretrained_models/best.pt")
        vocab, max_str_len = load_vocab(f"{logger.dir()}/vocab.json")
        env = make_async_chem_env(
            num_envs=1,
            seed=None,
            vocabulary=vocab,
            max_str_len=max_str_len
        )
        self._net.model().load_state_dict(state_dict['model'])
        agent = PretrainedRecurrentAgent(self._net, num_envs=1)
        # just save the agent
        Train(env, agent, self._id, total_time_steps=0).train().close()
;;;

train/train.py
;;;
# train/train.py
import time
from queue import Queue
from typing import Optional, List, Dict # Added Dict

import numpy as np
import pandas as pd
import torch

import drl
import drl.agent as agent
from metric import MolMetric
from drl.util import IncrementalMean
from envs import Env
from util import CSVSyncWriter, TextInfoBox, logger, to_smiles, try_create_dir
import os
from tqdm import tqdm


class Train:
    _TRACE_ENV: int = 0

    def __init__(
        self,
        env: Env,
        agent: agent.Agent,
        id: str,
        total_time_steps: int,
        summary_freq: Optional[int] = None,
        agent_save_freq: Optional[int] = None,
        inference_env: Optional[Env] = None,
        n_inference_episodes: int = 1,
        smiles_or_selfies_refset: Optional[List[str]] = None,
    ) -> None:
        self._env = env
        self._agent = agent

        self._id = id
        self._total_time_steps = total_time_steps
        self._summary_freq = total_time_steps // 20 if summary_freq is None else summary_freq
        self._agent_save_freq = self._summary_freq * 10 if agent_save_freq is None else agent_save_freq
        self._inference_env = inference_env
        self._n_inference_episodes = n_inference_episodes
        self._smiles_or_selfies_refset = smiles_or_selfies_refset

        self._dtype = torch.float32
        self._device = self._agent.device
        self._best_score = float("-inf")

        self._time_steps = 0
        self._episodes = 0
        self._episode_len = 0
        self._real_start_time = time.time()
        self._real_time = 0.0

        self._cumulative_reward_mean = IncrementalMean()
        self._episode_len_mean = IncrementalMean()
        self._mol_metric = MolMetric()

        # helps to synchronize final molecule results of each episode
        self._metric_csv_sync_writer_dict: Dict[str, CSVSyncWriter] = {} # Type hint added
        # self._episode_molecule_sync_buffer_dict = defaultdict(lambda: SyncFixedBuffer(max_size=self._env.num_envs, callback=self._record_molecule))
        # to save final molecule results periodically
        # self._molecule_queue = Queue() # (episode, env_id, score, selfies) - Consider using CSV writer instead
        # self._best_molecule = None
        # self._best_molecule_queue = Queue() # (episode, best_score, selfies) - Consider using CSV writer instead
        # self._intrinsic_reward_queue = Queue() # (episode, env_id, time_step, intrinsic_rewards...) - Consider using CSV writer instead

        self._enabled = True

    def train(self) -> "Train":
        if not self._enabled:
            raise RuntimeError("Train is already closed.")

        if not logger.enabled():
            logger.enable(self._id, enable_log_file=False)

        if self._time_steps == self._total_time_steps:
            # self._save_train() # Save even if already finished? Maybe not needed.
            print(f"[{self._id}] Training already completed at {self._total_time_steps} steps.")
            return self

        self._load_train() # Load previous state if exists

        if self._time_steps >= self._total_time_steps:
            logger.print(f"Training is already finished at step {self._time_steps}.")
            return self

        logger.disable() # Disable temporary logger used for loading
        logger.enable(self._id, enable_log_file=True) # Enable logger with file output

        try:
            if self._smiles_or_selfies_refset is not None:
                logger.print(f"Preprocessing the SMILES reference set ({len(self._smiles_or_selfies_refset)}) for the molecular metric...")
                smiles_refset = to_smiles(self._smiles_or_selfies_refset)
                self._mol_metric.preprocess(smiles_refset=smiles_refset)

            self._print_train_info()
        except KeyboardInterrupt:
            logger.print(f"Training setup interrupted.")
            self.close() # Ensure resources are closed
            return self

        try:
            obs = self._env.reset()
            cumulative_reward = np.zeros(self._env.num_envs) # Track per environment
            last_agent_save_t = self._time_steps # Track last save step

            # Main training loop
            for current_step in range(self._time_steps, self._total_time_steps):
                # take action and observe
                obs_tensor = self._numpy_to_tensor(obs)
                action_tensor = self._agent.select_action(obs_tensor)
                # Ensure action is on CPU and numpy for env step
                action_np = action_tensor.detach().cpu().numpy()

                next_obs, reward, terminated, real_final_next_obs, env_info = self._env.step(action_np)

                # update the agent
                # Use real_final_next_obs for terminated environments
                real_next_obs_np = next_obs.copy()
                if np.any(terminated): # Avoid indexing if nothing terminated
                     real_next_obs_np[terminated] = real_final_next_obs

                real_next_obs_tensor = self._numpy_to_tensor(real_next_obs_np)

                exp = drl.Experience(
                    obs_tensor,
                    action_tensor, # Pass tensor action to agent update
                    real_next_obs_tensor,
                    self._numpy_to_tensor(reward[..., np.newaxis]),
                    self._numpy_to_tensor(terminated[..., np.newaxis]),
                )
                # Agent update step
                agent_info = self._agent.update(exp)

                # process info dicts from env and agent
                self._process_info_dict(env_info, agent_info)

                # take next step
                obs = next_obs
                cumulative_reward += reward # Update cumulative reward per env

                # Handle terminated environments for logging
                terminated_envs = np.where(terminated)[0]
                for env_idx in terminated_envs:
                     if env_idx == self._TRACE_ENV: # Log only for trace env
                         self._cumulative_reward_mean.update(cumulative_reward[env_idx])
                         # Episode length needs careful tracking if envs reset at different times
                         # self._episode_len_mean.update(self._episode_len) # This needs rework for async envs
                         self._tick_episode() # Increment total episode count
                     cumulative_reward[env_idx] = 0.0 # Reset reward for terminated env

                self._tick_time_steps() # Increment global time step counter

                # summary
                if self._time_steps % self._summary_freq == 0:
                    self._summary_train()

                # save the agent periodically and based on inference score
                if self._time_steps % self._agent_save_freq == 0:
                    score = self._inference(self._n_inference_episodes)
                    self._save_train(score) # Pass score to potentially save best agent
                    last_agent_save_t = self._time_steps

            logger.print(f"Training is finished at step {self._time_steps}.")
            # Save final agent if it wasn't saved on the last step
            if self._time_steps > last_agent_save_t:
                final_score = self._inference(self._n_inference_episodes)
                self._save_train(final_score)

        except KeyboardInterrupt:
            logger.print(f"Training interrupted at the time step {self._time_steps}.")
            self._save_train() # Save progress on interrupt
        finally:
             # Ensure writers are closed if they exist
             for writer in self._metric_csv_sync_writer_dict.values():
                  if writer is not None:
                       # Add a close method to CSVSyncWriter if needed, or ensure files are flushed
                       pass
             self.close() # Close environments and logger

        return self

    def close(self):
        if not self._enabled:
            return # Already closed
        self._enabled = False
        self._env.close()

        if self._inference_env is not None:
            self._inference_env.close()

        if logger.enabled():
            logger.disable()

    def _make_csv_sync_writer(self, metric_name: str, metric_info_dict: dict):
        # Ensure keys and values exist before accessing .keys()
        key_fields = metric_info_dict.get("keys", {}).keys()
        value_fields = metric_info_dict.get("values", {}).keys()

        # Add time_step if not already present (useful for env metrics)
        all_value_fields = list(value_fields)
        if "time_step" not in key_fields and "time_step" not in value_fields:
             all_value_fields.append("time_step")


        return CSVSyncWriter(
            file_path=f"{logger.dir()}/{metric_name}.csv",
            key_fields=key_fields,
            value_fields=tuple(all_value_fields), # Use potentially extended list
        )

    def _write_metric_dicts(self, metric_dicts, include_time_step=False):
        """ Writes metrics from an iterable of structured metric dictionaries. """
        if metric_dicts is None:
             return

        for metric_dict in metric_dicts:
            if metric_dict is None:
                continue
            # Expected structure: {'metric_type': {'keys': {...}, 'values': {...}}}
            for metric_name, metric_info in metric_dict.items():
                 # Validate structure before proceeding
                 if not isinstance(metric_info, dict) or "keys" not in metric_info or "values" not in metric_info:
                      logger.print(f"Warning: Skipping malformed metric entry for '{metric_name}': {metric_info}")
                      continue

                 # Add time step to values if requested
                 if include_time_step:
                     metric_info["values"]["time_step"] = self._time_steps

                 # Initialize writer if needed
                 if metric_name not in self._metric_csv_sync_writer_dict:
                     self._metric_csv_sync_writer_dict[metric_name] = self._make_csv_sync_writer(metric_name, metric_info)

                 current_writer = self._metric_csv_sync_writer_dict[metric_name]

                 # Check if new value fields need to be added to the header (should be rare after init)
                 new_fields = set(metric_info["values"].keys()) - set(current_writer.value_fields)
                 if new_fields:
                      # This indicates an unexpected change in metric structure - log a warning
                      logger.print(f"Warning: New value fields {new_fields} detected for metric '{metric_name}'. CSV header might become inconsistent if file already exists.")
                      # Optionally, could try to rewrite the CSV header, but safer to warn.
                      # current_writer.value_fields += tuple(new_fields) # Update writer fields (use setter if implemented)

                 # Add data row
                 current_writer.add(
                     keys=metric_info["keys"],
                     values=metric_info["values"],
                 )

    def _process_info_dict(self, env_info: dict, agent_info: Optional[dict]):
        """ Processes info dicts from environment and agent. """
        # Process environment metrics (expected to be list/tuple of dicts)
        if "metric" in env_info and env_info["metric"] is not None:
            # env_info['metric'] comes from AsyncEnv merging, might contain None entries
            valid_env_metrics = [m for m in env_info["metric"] if m is not None]
            if valid_env_metrics:
                 self._write_metric_dicts(valid_env_metrics, include_time_step=True)

        # Process agent metrics (expected to be a single dict: {'metric': {'Loss':(v,t), ...}})
        if agent_info is not None and "metric" in agent_info and agent_info["metric"]:
            agent_metrics = agent_info["metric"] # This is the dict like {'Loss':(v,t), ...}
            if isinstance(agent_metrics, dict):
                 for key, (value, step) in agent_metrics.items():
                      # Use logger's direct method for simple scalar logging
                      logger.log_data(key, value, step)
            else:
                 logger.print(f"Warning: Unexpected agent metric format: {agent_metrics}")


    def _tick_time_steps(self):
        # self._episode_len += 1 # Episode length tracking needs rework for async envs
        self._time_steps += 1 # Use internal counter consistent with loop
        self._real_time = time.time() - self._real_start_time

    def _tick_episode(self):
        # self._episode_len = 0 # Reset per-env episode length on termination
        self._episodes += 1 # Increment total episodes completed across all envs

    def _agent_tensor(self, x: torch.Tensor) -> torch.Tensor:
        return x.to(device=self._device, dtype=self._dtype)

    def _numpy_to_tensor(self, x: np.ndarray) -> torch.Tensor:
        return torch.from_numpy(x).to(device=self._device, dtype=self._dtype)

    def _print_train_info(self):
        text_info_box = TextInfoBox() \
            .add_text(f"[{self._id}] Training Start!") \
            .add_line(marker="=") \
            .add_text(f"ID: {self._id}") \
            .add_text(f"Output Path: {logger.dir()}")

        # display environment info
        env_config_dict = self._env.config_dict
        if len(env_config_dict.keys()) > 0:
            text_info_box.add_line() \
                .add_text(f"Environment INFO:")
            for key, value in env_config_dict.items():
                text_info_box.add_text(f"    {key}: {value}")

        # display training info
        text_info_box.add_line() \
            .add_text(f"Training INFO:") \
            .add_text(f"    number of environments: {self._env.num_envs}") \
            .add_text(f"    total time steps: {self._total_time_steps}") \
            .add_text(f"    summary frequency: {self._summary_freq}") \
            .add_text(f"    agent save frequency: {self._agent_save_freq}")

        # display agent info
        agent_config_dict = self._agent.config_dict
        # Add device info if not already present
        if 'device' not in agent_config_dict:
             agent_config_dict["device"] = str(self._device) # Convert device object to string

        text_info_box.add_line() \
            .add_text(f"Agent ({self._agent.name}):") # Add agent name if available
        for key, value in agent_config_dict.items():
            text_info_box.add_text(f"    {key}: {value}")

        logger.print(text_info_box.make(), prefix="")
        logger.print("", prefix="") # Add empty line for spacing

    def _summary_train(self):
        """ Logs summary statistics during training. """
        try:
            # Try reading the main metric file (e.g., episode_metric.csv)
            metric_file = f"{logger.dir()}/episode_metric.csv"
            if not os.path.exists(metric_file):
                 logger.print(f"Summary ({self._time_steps}/{self._total_time_steps}): Metric file not found yet.")
                 return

            metric_df = pd.read_csv(metric_file)
            # Filter metrics within the current summary window
            lower_bound = self._time_steps - self._summary_freq
            current_metric_df = metric_df[metric_df["time_step"] >= lower_bound]
            # Ensure 'score' and 'smiles' columns exist and drop rows with NaNs in them
            if "score" not in current_metric_df.columns or "smiles" not in current_metric_df.columns:
                 logger.print(f"Summary ({self._time_steps}/{self._total_time_steps}): 'score' or 'smiles' not found in metrics.")
                 return

            current_metric_df = current_metric_df.dropna(subset=["score", "smiles"])

            if len(current_metric_df) == 0:
                info = "No valid molecules generated in this summary window."
            else:
                score = current_metric_df["score"].mean()
                # Use the class instance of MolMetric
                self._mol_metric.preprocess(smiles_generated=current_metric_df["smiles"].tolist())
                diversity = self._mol_metric.calc_diversity()
                uniqueness = self._mol_metric.calc_uniqueness()
                info = f"score: {score:.3f}, diversity: {diversity:.3f}, uniqueness: {uniqueness:.3f}"
                logger.log_data("Environment/Score", score, self._time_steps)
                logger.log_data(f"Environment/Diversity", diversity, self._time_steps)
                logger.log_data(f"Environment/Uniqueness", uniqueness, self._time_steps)

                # Calculate novelty only if reference set was provided
                if self._mol_metric.smiles_refset is not None:
                    try:
                        novelty = self._mol_metric.calc_novelty()
                        info += f", novelty: {novelty:.3f}"
                        logger.log_data(f"Environment/Novelty", novelty, self._time_steps)
                    except ValueError as e:
                        logger.print(f"Warning: Could not calculate novelty - {e}")
                else:
                    # Log novelty as NaN or skip if no refset
                    logger.log_data(f"Environment/Novelty", float('nan'), self._time_steps)


            # Log average reward and episode length if available
            if self._cumulative_reward_mean.count > 0:
                 avg_reward = self._cumulative_reward_mean.mean
                 logger.log_data("Environment/Avg Reward (Trace Env)", avg_reward, self._time_steps)
                 info += f", avg_reward: {avg_reward:.3f}"
                 self._cumulative_reward_mean.reset() # Reset after logging

            # if self._episode_len_mean.count > 0:
            #      avg_ep_len = self._episode_len_mean.mean
            #      logger.log_data("Environment/Avg Ep Length (Trace Env)", avg_ep_len, self._time_steps)
            #      info += f", avg_ep_len: {avg_ep_len:.1f}"
            #      self._episode_len_mean.reset() # Reset after logging

            logger.print(f"Summary ({self._time_steps}/{self._total_time_steps}): {info}")

        except FileNotFoundError:
             logger.print(f"Summary ({self._time_steps}/{self._total_time_steps}): Waiting for metric file...")
        except pd.errors.EmptyDataError:
             logger.print(f"Summary ({self._time_steps}/{self._total_time_steps}): Metric file is empty.")
        except Exception as e:
             logger.print(f"Error during summary: {e}") # Log other potential errors

        # Plot logs regardless of summary success
        try:
            logger.plot_logs()
        except Exception as e:
             logger.print(f"Error plotting logs: {e}")


    def _save_train(self, score=None):
        """ Saves the training state and agent models. """
        train_dict = dict(
            time_steps=self._time_steps,
            episodes=self._episodes,
            # episode_len=self._episode_len, # Less meaningful for async
            best_score=self._best_score, # Save best score achieved so far
        )
        state_dict = dict(
            train=train_dict,
            agent=self._agent.state_dict,
        )

        agent_save_path = f"{logger.dir()}/agent.pt"
        torch.save(state_dict, agent_save_path)

        agent_ckpt_dir = f"{logger.dir()}/agent_ckpt"
        try_create_dir(agent_ckpt_dir)
        torch.save(state_dict, f"{agent_ckpt_dir}/agent_{self._time_steps}.pt")

        saved_best = False
        if score is not None and score > self._best_score:
            self._best_score = score
            best_agent_save_path = f"{logger.dir()}/best_agent.pt"
            torch.save(state_dict, best_agent_save_path)
            logger.print(f"Agent saved ({self._time_steps} steps). New best score: {score:.4f}. Saved to: {best_agent_save_path}")
            saved_best = True

        if not saved_best:
             logger.print(f"Agent saved ({self._time_steps} steps): {agent_save_path}")

        # self._env.save_data(logger.dir()) # If env needs saving
        # self._save_molecules(logger.dir()) # If using queue-based saving

    def _inference(self, n_episodes: int):
        """ Runs inference using the current agent policy. """
        if self._inference_env is None or n_episodes <= 0:
            return None # Return None if no inference env or episodes

        logger.print(f"--- Starting inference ({self._time_steps} steps) for {n_episodes} episodes ---")

        episodes_done = np.zeros((self._inference_env.num_envs,), dtype=int)
        total_episodes_target = n_episodes * self._inference_env.num_envs # Aim for roughly n_episodes per env if possible
        score_list = []
        smiles_list = []

        # Get a dedicated inference agent instance
        inference_agent = self._agent.inference_agent(self._inference_env.num_envs)
        inference_agent.model.eval() # Set model to evaluation mode

        obs = self._inference_env.reset()

        # Loop until enough episodes are collected across all inference envs
        pbar = tqdm(total=n_episodes, desc="Inference Episodes", leave=False)
        collected_episodes = 0
        while collected_episodes < n_episodes:
            obs_tensor = self._numpy_to_tensor(obs)
            with torch.no_grad():
                action_tensor = inference_agent.select_action(obs_tensor) # Use inference agent

            action_np = action_tensor.detach().cpu().numpy()
            next_obs, _, terminated, real_final_next_obs, env_info = self._inference_env.step(action_np)

            # Update inference agent's internal state (e.g., hidden state)
            # No learning update needed for inference agent usually
            real_next_obs_np = next_obs.copy()
            if np.any(terminated):
                 real_next_obs_np[terminated] = real_final_next_obs
            real_next_obs_tensor = self._numpy_to_tensor(real_next_obs_np)
            exp = drl.Experience(
                 obs_tensor, action_tensor, real_next_obs_tensor,
                 torch.zeros_like(self._numpy_to_tensor(terminated[..., np.newaxis])), # Dummy reward/term
                 self._numpy_to_tensor(terminated[..., np.newaxis])
            )
            with torch.no_grad():
                 _ = inference_agent.update(exp)

            obs = next_obs
            episodes_done += terminated.astype(int)

            # Process metrics from terminated environments during inference
            if "metric" in env_info and env_info["metric"] is not None:
                inf_scores, inf_smiles = self._inference_metric(env_info) # Extract scores/smiles
                score_list.extend(inf_scores)
                smiles_list.extend(inf_smiles)

            # Update progress bar based on completed episodes
            newly_finished = terminated.sum()
            collected_episodes += newly_finished
            pbar.update(newly_finished)
            if collected_episodes >= n_episodes:
                 break # Stop if target number of episodes is reached

        pbar.close()

        # Ensure model is back in training mode after inference
        self._agent.model.train()

        if not score_list: # Check if any valid scores were collected
            logger.print(f"--- Inference ({self._time_steps} steps) -> No valid molecules generated. ---")
            return None

        avg_score = np.mean(score_list)
        valid_smiles_list = [s for s in smiles_list if s] # Filter out potential None/empty smiles
        if not valid_smiles_list:
             logger.print(f"--- Inference ({self._time_steps} steps) -> Score: {avg_score:.3f}, No valid SMILES found. ---")
             diversity, uniqueness, novelty = 0.0, 0.0, 0.0 # Or NaN
        else:
             self._mol_metric.preprocess(smiles_generated=valid_smiles_list)
             diversity = self._mol_metric.calc_diversity()
             uniqueness = self._mol_metric.calc_uniqueness()

             info = f"Score: {avg_score:.3f}, Diversity: {diversity:.3f}, Uniqueness: {uniqueness:.3f}"
             logger.log_data("Inference/Score", avg_score, self._time_steps)
             logger.log_data("Inference/Diversity", diversity, self._time_steps)
             logger.log_data("Inference/Uniqueness", uniqueness, self._time_steps)

             novelty = float('nan') # Default if no refset
             if self._mol_metric.smiles_refset is not None:
                 try:
                     novelty = self._mol_metric.calc_novelty()
                     info += f", Novelty: {novelty:.3f}"
                 except ValueError as e:
                     logger.print(f"Warning: Could not calculate novelty during inference - {e}")
             logger.log_data("Inference/Novelty", novelty, self._time_steps) # Log NaN if no refset

             logger.print(f"--- Inference ({self._time_steps} steps) -> {info} ({len(valid_smiles_list)} valid mols) ---")

        # Plot logs after inference summary
        try:
            logger.plot_logs()
        except Exception as e:
             logger.print(f"Error plotting logs after inference: {e}")

        return avg_score # Return average score for best model tracking


    def _inference_metric(self, env_info: dict):
        """ Extracts scores and smiles from inference environment info. """
        scores = []
        smiles = []

        if "metric" not in env_info or env_info["metric"] is None:
            return scores, smiles

        # env_info['metric'] should be an iterable (list/tuple) from AsyncEnv
        for metric_dict in env_info["metric"]:
            if metric_dict is None or "episode_metric" not in metric_dict:
                continue

            # Ensure 'values' exists and is a dictionary
            values = metric_dict["episode_metric"].get("values")
            if not isinstance(values, dict):
                 continue

            score = values.get("score") # Use .get for safety
            smile = values.get("smiles")

            # Append only if both score and smiles are valid (not None)
            if score is not None and smile is not None:
                scores.append(score)
                smiles.append(smile)

        return scores, smiles


    def _load_train(self):
        """ Loads training state from checkpoint file. """
        load_path = f"{logger.dir()}/agent.pt"
        try:
            # Ensure loading to the correct device (CPU might be safer for loading)
            state_dict = torch.load(load_path, map_location='cpu')
            logger.print(f"Loading training state from: {load_path}")

            train_dict = state_dict.get("train", {})
            self._time_steps = train_dict.get("time_steps", 0)
            self._episodes = train_dict.get("episodes", 0)
            # self._episode_len = train_dict.get("episode_len", 0) # Less relevant now
            self._best_score = train_dict.get("best_score", float("-inf")) # Load best score

            agent_state = state_dict.get("agent")
            if agent_state:
                 self._agent.load_state_dict(agent_state)
                 logger.print(f"Agent state loaded successfully. Resuming from step {self._time_steps}.")
            else:
                 logger.print("Warning: Agent state not found in checkpoint.")

        except FileNotFoundError:
            logger.print("No checkpoint file found. Starting training from scratch.")
        except Exception as e:
             logger.print(f"Error loading checkpoint from {load_path}: {e}. Starting from scratch.")
             # Reset state variables if loading fails
             self._time_steps = 0
             self._episodes = 0
             self._best_score = float("-inf")


;;;

train/__init__.py
;;;
from .factory import MolRLTrainFactory, MolRLInferenceFactory, ConfigParsingError, MolRLPretrainFactory
;;;

config/plogp/molair.yaml # pLogP
;;;
# config/plogp/molair.yaml
PLogP-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    plogp_coef: 1.0

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly due to longer learning_starts
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0 # Less critical for SAC, but keep if desired
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.01
;;;

config/qed/molair.yaml # QED
;;;
# config/qed/molair.yaml
QED-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    qed_coef: 1.0
    # Add any specific Env settings from your previous QED run if needed
    use_raw_reward: True # Example if you had this
    raw_reward_invalid_penalty: -.1 # Example if you had this

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.01
;;;

config/similarity/molair.yaml # Similarity
;;;
# config/similarity/molair.yaml
Similarity-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    similarity_coef: 1.0
    init_selfies: '[C]' # Keep task-specific init

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.001
;;;

config/gsk3b/molair.yaml # GSK3B
;;;
# config/gsk3b/molair.yaml
GSK3B-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    gsk3b_coef: 1.0
    init_selfies: ['[C][C][C]', '[C][=C][C]', '[C][C][=N]', '[C][N][C]', '[C][O][C]']

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.0065
;;;

config/jnk3/molair.yaml # JNK3
;;;
# config/jnk3/molair.yaml
JNK3-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    jnk3_coef: 1.0
    init_selfies: ['[C][C][C]', '[C][=C][C]', '[C][C][=N]', '[C][N][C]', '[C][O][C]']

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.0005
;;;

config/gsk3b+jnk3/molair.yaml # GSK3B+JNK3
;;;
# config/gsk3b+jnk3/molair.yaml
GSK3B+JNK3-MolAIR: # Experiment ID
  Agent:
    type: SAC # Specify SAC agent type
    # --- SAC Specific Hyperparameters (Suggested Starting Points) ---
    gamma: 0.99 # Discount factor
    tau: 0.005 # Target network update rate
    alpha: 0.2 # Initial entropy coefficient (will be tuned if learn_alpha=True)
    actor_lr: 3.0e-4 # Learning rate for actor
    critic_lr: 3.0e-4 # Learning rate for critic
    alpha_lr: 3.0e-4 # Learning rate for alpha
    target_update_interval: 1 # Steps between target network updates
    learn_alpha: True # Automatically tune alpha
    target_entropy_ratio: 0.98 # Target higher entropy (encourage exploration)
    # --- Buffer & Training Hyperparameters ---
    buffer_size: 200000 # Replay buffer size
    batch_size: 512    # Batch size for sampling from buffer
    learning_starts: 10000 # Increase steps before learning starts
    gradient_steps: 1    # Start with 1 gradient step per env step
    # --- Common Agent Params ---
    n_steps: 1 # SAC typically uses n_steps=1

  Env: # Keep task-specific settings
    gsk3b_coef: 0.5
    jnk3_coef: 0.5
    init_selfies: ['[C][C][C]', '[C][=C][C]', '[C][C][=N]', '[C][N][C]', '[C][O][C]']

  Train:
    num_envs: 64
    seed: 0
    total_time_steps: 120000 # Keep original or increase if needed
    summary_freq: 1000
    agent_save_freq: 10000 # Increase save frequency slightly
    num_inference_envs: 32
    n_inference_episodes: 100
    grad_clip_max_norm: 5.0
    device: cuda

  CountIntReward: # Keep original intrinsic reward setting if used
    crwd_coef: 0.002 # Value from original combined PPO/RND config
;;;

